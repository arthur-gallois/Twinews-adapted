{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commands and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oomstopper --no-tail combinasons ; killbill combinasons ; cd ~/twinews-logs ; jupython -o nohup-combinasons-$HOSTNAME.out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/models/combinasons.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO convertir \"doc2vec\" en \"doc2vec-hash\" --> en cherchant le meilleur ndcg\n",
    "# Pour le parametre qui determine les modèles utilisé pour le merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = isNotebook # isNotebook, True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.basics import *\n",
    "from twinews.utils import *\n",
    "from twinews.models.ranking import *\n",
    "from machinelearning.iterator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twinews.models.genericutils import *\n",
    "from twinews.models.ranking import *\n",
    "from twinews.models.ranking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tictoc starts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/combinasons.log\") if isNotebook else Logger(\"combinasons-\" + getHostname() + \".log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestModelKey(model, splitVersion, maxUsers=None, metric='ndcg'):\n",
    "    twinewsRankings = getTwinewsRankings(verbose=False)\n",
    "    twinewsScores = getTwinewsScores(verbose=False)\n",
    "    keys = twinewsRankings.keys()\n",
    "    rows = []\n",
    "    for key in keys:\n",
    "        meta = twinewsRankings.getMeta(key)\n",
    "        if meta['splitVersion'] == splitVersion \\\n",
    "        and meta['maxUsers'] == None \\\n",
    "        and meta['model'] == model:\n",
    "            scoreRow = twinewsScores.findOne({'id': meta['id'], 'metric': metric})\n",
    "            assert 'score' not in meta\n",
    "            meta['score'] = scoreRow['score']\n",
    "            rows.append(meta)\n",
    "    rows = [(e['id'], e['score']) for e in rows]\n",
    "    rows = sortBy(rows, index=1, desc=True)\n",
    "    best = rows[0][0]\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankingsCache = None\n",
    "def getRankings(model, userIds=None):\n",
    "    global rankingsCache\n",
    "    if rankingsCache is None:\n",
    "        rankingsCache = dict()\n",
    "    twinewsRankings = getTwinewsRankings(verbose=False)\n",
    "    if model in rankingsCache:\n",
    "        rankings = rankingsCache[model]\n",
    "    else:\n",
    "        rankings = twinewsRankings[model]\n",
    "        rankingsCache[model] = rankings\n",
    "    assert rankings is not None\n",
    "    if userIds is not None:\n",
    "        rankings = dictSelect(rankings, userIds)\n",
    "    return rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIdsCache = None\n",
    "def generateRankings(config, logger=None, verbose=True):\n",
    "    # We init user ids cache:\n",
    "    global userIdsCache\n",
    "    if userIdsCache is None:\n",
    "        userIdsCache = dict()\n",
    "    # We get models:\n",
    "    assert \"models\" in config\n",
    "    assert isinstance(config[\"models\"], list) or isinstance(config[\"models\"], set)\n",
    "    assert len(config[\"models\"]) >= 2\n",
    "    if isinstance(config[\"models\"], list) and models != sorted(models):\n",
    "        raise Exception(\"You need to give models by alphabetic order\")\n",
    "    models = sorted(list(config[\"models\"]))\n",
    "    # We set default params:\n",
    "    if not dictContains(config, 'rankAsScore'):\n",
    "        config['rankAsScore'] = [True] * len(models)\n",
    "        logWarning(\"You didn't set rankAsScore\", logger=logger, verbose=verbose)\n",
    "    if not dictContains(config, 'weights'):\n",
    "        config['weights'] = [1.0 / len(models)] * len(models)\n",
    "        logWarning(\"You didn't set weights\", logger=logger, verbose=verbose)\n",
    "    if not dictContains(config, 'alphas'):\n",
    "        config['alphas'] = [0.5] * len(models)\n",
    "        logWarning(\"You didn't set alphas\", logger=logger, verbose=verbose)\n",
    "    if not dictContains(config, 'betas'):\n",
    "        config['betas'] = [NormalizedLawBeta.LOG] * len(models)\n",
    "        logWarning(\"You didn't set betas\", logger=logger, verbose=verbose)\n",
    "    # We get user ids:\n",
    "    userIds = None\n",
    "    if config[\"maxUsers\"] is not None:\n",
    "        if config[\"maxUsers\"] in userIdsCache:\n",
    "            userIds = userIdsCache[config[\"maxUsers\"]]\n",
    "        else:\n",
    "            evalData = getEvalData(config['splitVersion'], maxExtraNews=0, maxUsers=config['maxUsers'],\n",
    "                                   logger=logger, verbose=verbose)\n",
    "            userIds = set(evalData['candidates'].keys())\n",
    "            log(\"Users: \" + b(userIds), logger)\n",
    "            userIdsCache[config[\"maxUsers\"]] = userIds\n",
    "    # We get best models:\n",
    "    models = sorted([getBestModelKey(model, config['splitVersion']) for model in models])\n",
    "    log(\"Setting models as: \" + b(models), logger)\n",
    "    config['models'] = models\n",
    "    # We get rankings:\n",
    "    modelsRankings = []\n",
    "    for model in models:\n",
    "        modelsRankings.append(getRankings(model, userIds=userIds))\n",
    "    # We generate rankings:\n",
    "    mergeRankingsKwargs = dictSelect(config, {'rankAsScore', 'weights', 'alphas', 'betas'})\n",
    "    combRankings = dict()\n",
    "    for userId in modelsRankings[0].keys():\n",
    "        for rkIndex in range(len(modelsRankings[0][userId])):\n",
    "            currentRankings = []\n",
    "            for modelIndex in range(len(modelsRankings)):\n",
    "                currentRankings.append(modelsRankings[modelIndex][userId][rkIndex])\n",
    "            merged = mergeRankings(currentRankings, **mergeRankingsKwargs, returnScores=False, logger=logger)\n",
    "            if userId not in combRankings:\n",
    "                combRankings[userId] = [None] * len(modelsRankings[0][userId])\n",
    "            combRankings[userId][rkIndex] = merged\n",
    "    # And we return it:\n",
    "    return (config, combRankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \\\n",
    "{\n",
    "    'splitVersion': 2,\n",
    "    'maxUsers': None if TEST else None, # Sub-sampling\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'combin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsDomain = {'lda', 'nmf', 'tfidf', 'bm25', 'dbert-ft', 'dbert-base', \n",
    "          'stylo', 'infersent', 'sent2vec', 'word2vec', 'doc2vec', 'usent', 'bert'}\n",
    "rankAsScoreDomain = {True, False}\n",
    "alphasDomain = {0.1, 0.25, 0.5, 0.75, 0.9}\n",
    "betasDomain = {NormalizedLawBeta.LOG, NormalizedLawBeta.EXP}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineModels(modelsDomain, amount):\n",
    "    modelsComb = [set(e) for e in combine(modelsDomain, amount) if len(e) == len(set(e))]\n",
    "    result = []\n",
    "    for e in modelsComb:\n",
    "        if e not in result:\n",
    "            result.append(e)\n",
    "    result = sorted(result, key=lambda x: str(sorted(list(x))))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsComb2 = combineModels(modelsDomain, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    modelsComb2 = modelsComb2[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ { bert, bm25 }, { bert, dbert-base } ]\n"
     ]
    }
   ],
   "source": [
    "bp(modelsComb2, 5, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankAsScore = [True, True]\n",
    "weights = [0.5, 0.5]\n",
    "alphas = [0.5, 0.5]\n",
    "betas = [NormalizedLawBeta.LOG, NormalizedLawBeta.LOG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting models as: [ bert-65e2b, bm25-1eb2a ]\n"
     ]
    }
   ],
   "source": [
    "for models in pb(modelsComb2, logger=logger, printRatio=0.01):\n",
    "    currentConfig = mergeDicts\\\n",
    "    (\n",
    "        config,\n",
    "        {\n",
    "            'models': models,\n",
    "            'rankAsScore': rankAsScore,\n",
    "            'weights': weights,\n",
    "            'alphas': alphas,\n",
    "            'betas': betas,\n",
    "        }\n",
    "    )\n",
    "    (currentConfig, rankings) = generateRankings(currentConfig, logger=logger)\n",
    "    try:\n",
    "        addRanking(modelName, rankings, currentConfig, logger=logger)\n",
    "    except Exception as e:\n",
    "        log(str(e), logger)\n",
    "    warnFreeRAM()\n",
    "    if freeRAM() < 4:\n",
    "        rankingsCache = None\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
