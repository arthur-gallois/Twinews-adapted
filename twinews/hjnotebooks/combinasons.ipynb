{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commands and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oomstopper --no-tail combinasons ; killbill combinasons ; cd ~/twinews-logs ; jupython -o nohup-combinasons-$HOSTNAME.out --venv st-venv ~/notebooks/twinews/hjnotebooks/combinasons.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False # isNotebook, True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.basics import *\n",
    "from twinews.utils import *\n",
    "from twinews.models.ranking import *\n",
    "from machinelearning.iterator import *\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twinews.models.genericutils import *\n",
    "from twinews.models.ranking import *\n",
    "from twinews.models.ranking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/combinasons.log\") if isNotebook else Logger(\"combinasons-\" + getHostname() + \".log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestModelKey(model, splitVersion, maxUsers=None, metric='ndcg'):\n",
    "    twinewsRankings = getTwinewsRankings(verbose=False)\n",
    "    twinewsScores = getTwinewsScores(verbose=False)\n",
    "    keys = twinewsRankings.keys()\n",
    "    rows = []\n",
    "    for key in keys:\n",
    "        meta = twinewsRankings.getMeta(key)\n",
    "        if meta['splitVersion'] == splitVersion \\\n",
    "        and meta['maxUsers'] == None \\\n",
    "        and meta['model'] == model:\n",
    "            scoreRow = twinewsScores.findOne({'id': meta['id'], 'metric': metric})\n",
    "            assert 'score' not in meta\n",
    "            meta['score'] = scoreRow['score']\n",
    "            rows.append(meta)\n",
    "    rows = [(e['id'], e['score']) for e in rows]\n",
    "    rows = sortBy(rows, index=1, desc=True)\n",
    "    best = rows[0][0]\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankingsCache = None\n",
    "def getRankings(model, userIds=None):\n",
    "    global rankingsCache\n",
    "    if rankingsCache is None:\n",
    "        rankingsCache = dict()\n",
    "    twinewsRankings = getTwinewsRankings(verbose=False)\n",
    "    if model in rankingsCache:\n",
    "        rankings = rankingsCache[model]\n",
    "    else:\n",
    "        rankings = twinewsRankings[model]\n",
    "        rankingsCache[model] = rankings\n",
    "    assert rankings is not None\n",
    "    if userIds is not None:\n",
    "        rankings = dictSelect(rankings, userIds)\n",
    "    return rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeTwinewsRankings(modelsRankings, config, *args, logger=None, verbose=True, **kwargs):\n",
    "    # We merge rankings for these users:\n",
    "    mergeRankingsKwargs = dictSelect(config, {'rankAsScore', 'weights', 'alphas', 'betas'})\n",
    "    combRankings = dict()\n",
    "    for userId in modelsRankings[0].keys():\n",
    "        for rkIndex in range(len(modelsRankings[0][userId])):\n",
    "            currentRankings = []\n",
    "            for modelIndex in range(len(modelsRankings)):\n",
    "                currentRankings.append(modelsRankings[modelIndex][userId][rkIndex])\n",
    "            merged = mergeRankings(currentRankings, **mergeRankingsKwargs, returnScores=False, logger=logger)\n",
    "            if userId not in combRankings:\n",
    "                combRankings[userId] = [None] * len(modelsRankings[0][userId])\n",
    "            combRankings[userId][rkIndex] = merged\n",
    "    return combRankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userIdsCache = None\n",
    "def generateRankings(config, logger=None, verbose=True):\n",
    "    # We init user ids cache:\n",
    "    global userIdsCache\n",
    "    if userIdsCache is None:\n",
    "        userIdsCache = dict()\n",
    "    # We get models:\n",
    "    assert \"models\" in config\n",
    "    assert isinstance(config[\"models\"], list) or isinstance(config[\"models\"], set)\n",
    "    assert len(config[\"models\"]) >= 2\n",
    "    if isinstance(config[\"models\"], list) and models != sorted(models):\n",
    "        raise Exception(\"You need to give models by alphabetic order\")\n",
    "    models = sorted(list(config[\"models\"]))\n",
    "    # We set default params:\n",
    "    if not dictContains(config, 'rankAsScore'):\n",
    "        config['rankAsScore'] = [True] * len(models)\n",
    "        logWarning(\"You didn't set rankAsScore\", logger=logger, verbose=verbose)\n",
    "    if not dictContains(config, 'weights'):\n",
    "        config['weights'] = [1.0 / len(models)] * len(models)\n",
    "        logWarning(\"You didn't set weights\", logger=logger, verbose=verbose)\n",
    "    if not dictContains(config, 'alphas'):\n",
    "        config['alphas'] = [0.5] * len(models)\n",
    "        logWarning(\"You didn't set alphas\", logger=logger, verbose=verbose)\n",
    "    if not dictContains(config, 'betas'):\n",
    "        config['betas'] = [NormalizedLawBeta.LOG] * len(models)\n",
    "        logWarning(\"You didn't set betas\", logger=logger, verbose=verbose)\n",
    "    # We get user ids:\n",
    "    userIds = None\n",
    "    if config[\"maxUsers\"] is not None:\n",
    "        if config[\"maxUsers\"] in userIdsCache:\n",
    "            userIds = userIdsCache[config[\"maxUsers\"]]\n",
    "        else:\n",
    "            evalData = getEvalData(config['splitVersion'], maxExtraNews=0, maxUsers=config['maxUsers'],\n",
    "                                   logger=logger, verbose=verbose)\n",
    "            userIds = set(evalData['candidates'].keys())\n",
    "            log(\"Users: \" + b(userIds), logger)\n",
    "            userIdsCache[config[\"maxUsers\"]] = userIds\n",
    "    # Initi tt:\n",
    "    tt = TicToc(logger=logger)\n",
    "    tt.tic(\"Starting \" + b(models))\n",
    "    # We get best models:\n",
    "    models = sorted([getBestModelKey(model, config['splitVersion']) for model in models])\n",
    "    config['models'] = models\n",
    "    tt.tic(\"Got best models\")\n",
    "    # We check if rankings exists:\n",
    "    if rankingExistsAndIsNotNone(modelName, config):\n",
    "        return (None, None)\n",
    "    # We get rankings:\n",
    "    modelsRankings = []\n",
    "    for model in models:\n",
    "        currentRks = getRankings(model, userIds=userIds)\n",
    "        element = currentRks[list(currentRks.keys())[0]][0][0]\n",
    "        if isinstance(element, str):\n",
    "            logWarning(\"No scores with items in rankings of \" + model, logger, verbose=verbose)\n",
    "            return (None, None)\n",
    "        modelsRankings.append(currentRks)\n",
    "    tt.tic(\"Got all rankings\")\n",
    "    # We chunk rankings:\n",
    "    userIdsChunks = split(sorted(list(modelsRankings[0].keys())), cpuCount())\n",
    "    modelsRankingsChunks = []\n",
    "    for userIdsChunk in userIdsChunks:\n",
    "        current = []\n",
    "        for currentRankings in modelsRankings:\n",
    "            current.append(dictSelect(currentRankings, userIdsChunk))\n",
    "        modelsRankingsChunks.append(current)\n",
    "    modelsRankingsChunks = chunks(modelsRankingsChunks, 1)\n",
    "    tt.tic(\"Rankings chunked\")\n",
    "    # We define the gen funct:\n",
    "    def genFunct(containers, *args, **kwargs):\n",
    "        for modelsRankings in containers:\n",
    "            # with warnings.catch_warnings(): # Doesn't work\n",
    "            #     if filterWarnings:\n",
    "            #         warnings.filterwarnings('ignore', r'encountered in double')\n",
    "            yield mergeTwinewsRankings(modelsRankings, *args, **kwargs)\n",
    "    # We define the mli:\n",
    "    mli = MLIterator(modelsRankingsChunks, genFunct=genFunct, genArgs=(config,),\n",
    "                     parallelProcesses=cpuCount(), maxParallelProcesses=cpuCount(),\n",
    "                     logger=logger, verbose=False)\n",
    "    # We get all merges:\n",
    "    allCombRankings = []\n",
    "    for current in mli:\n",
    "        allCombRankings.append(current)\n",
    "    tt.tic(\"Got all merges from sub-processes\")\n",
    "    # We merge all:\n",
    "    combRankings = mergeDicts(allCombRankings)\n",
    "    tt.toc(b(models) + \" DONE.\")\n",
    "    # And we return it:\n",
    "    return (config, combRankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \\\n",
    "{\n",
    "    'splitVersion': 2,\n",
    "    'maxUsers': 2 if TEST else None, # Sub-sampling\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'combin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsDomain = {'nmf', 'tfidf', 'dbert-ft', 'dbert-base', \n",
    "                'stylo', 'infersent', 'sent2vec', 'word2vec',\n",
    "                'doc2vec', 'bm25', 'bert', 'usent', 'lda'}\n",
    "rankAsScoreDomain = {True, False}\n",
    "alphasDomain = {0.1, 0.25, 0.5, 0.75, 0.9}\n",
    "betasDomain = {NormalizedLawBeta.LOG, NormalizedLawBeta.EXP}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineModels(modelsDomain, amount):\n",
    "    modelsComb = [set(e) for e in combine(modelsDomain, amount) if len(e) == len(set(e))]\n",
    "    result = []\n",
    "    for e in modelsComb:\n",
    "        if e not in result:\n",
    "            result.append(e)\n",
    "    result = sorted(result, key=lambda x: str(sorted(list(x))))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsComb = combineModels(modelsDomain, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelsComb = [e for e in modelsComb if 'tfidf' in e]\n",
    "# modelsComb = [e for e in modelsComb if \"word2vec\" in str(e)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    modelsComb = modelsComb[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp(modelsComb, 5, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankAsScore = [True, True] # [False, False], [True, True]\n",
    "weights = [0.5, 0.5]\n",
    "alphas = [0.5, 0.5]\n",
    "betas = [NormalizedLawBeta.LOG, NormalizedLawBeta.LOG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for models in pb(modelsComb, logger=logger, printRatio=0.01):\n",
    "    currentConfig = mergeDicts\\\n",
    "    (\n",
    "        config,\n",
    "        {\n",
    "            'models': models,\n",
    "            'rankAsScore': rankAsScore,\n",
    "            'weights': weights,\n",
    "            'alphas': alphas,\n",
    "            'betas': betas,\n",
    "        }\n",
    "    )\n",
    "    try:\n",
    "        (currentConfig, rankings) = generateRankings(currentConfig, logger=logger)\n",
    "    except Exception as e:\n",
    "        (currentConfig, rankings) = (None, None)\n",
    "        logException(e, logger)\n",
    "    if currentConfig is not None:\n",
    "        addRanking(modelName, rankings, currentConfig, logger=logger)\n",
    "        warnFreeRAM(logger=logger)\n",
    "        if freeRAM() < 4:\n",
    "            rankingsCache = None\n",
    "            log(\"Breaking the loop\", logger)\n",
    "            break\n",
    "        rankings = None\n",
    "    else:\n",
    "        log(b(models) + \" already added\", logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getBestModelKey('bert', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
