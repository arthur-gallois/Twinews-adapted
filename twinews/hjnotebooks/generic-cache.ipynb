{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanv 1:\n",
    "# screen -S generic-cache1\n",
    "# source ~/.bash_profile ; source ~/.bash_aliases ; cd ~/twinews-logs\n",
    "# DOCKER_PORT=9963 nn -o nohup-generic-cache-$HOSTNAME-1.out ~/docker/keras/run-jupython.sh ~/notebooks/twinews/hjnotebooks/generic-cache.ipynb titanv\n",
    "# observe ~/twinews-logs/nohup-generic-cache-$HOSTNAME-1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# titanv 2:\n",
    "# screen -S generic-cache2\n",
    "# source ~/.bash_profile ; source ~/.bash_aliases ; cd ~/twinews-logs\n",
    "# DOCKER_PORT=9964 nn -o nohup-generic-cache-$HOSTNAME-2.out ~/docker/keras/run-jupython.sh ~/notebooks/twinews/hjnotebooks/generic-cache.ipynb titanv\n",
    "# observe ~/twinews-logs/nohup-generic-cache-$HOSTNAME-2.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oomstopper --no-tail generic-cache ; killbill generic-cache ; cd ~/twinews-logs ; jupython -o nohup-generic-cache-$HOSTNAME.out --venv st-venv ~/notebooks/twinews/hjnotebooks/generic-cache.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twinews.models.genericutils import *\n",
    "cacheFields = dictSelect(genericFields,\\\n",
    "{\n",
    "    # 'bdert-ft',\n",
    "    # 'bdert-base',\n",
    "    # 'infersent',\n",
    "    # 'sent2vec',\n",
    "    # 'doc2vec',\n",
    "    'bert',\n",
    "    # 'stylo',\n",
    "    # 'nmf',\n",
    "    # 'tfidf',\n",
    "    # 'word2vec',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = isNotebook # False, True, isNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from systemtools.hayj import *\n",
    "if labia():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\n",
    "elif toCache(cacheFields, 'dbert'):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import callbacks\n",
    "    from transformers import \\\n",
    "    (\n",
    "        DistilBertConfig,\n",
    "        DistilBertTokenizer,\n",
    "        TFDistilBertForSequenceClassification,\n",
    "    )\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from systemtools.basics import *\n",
    "from systemtools.printer import *\n",
    "from systemtools.file import *\n",
    "from systemtools.location import *\n",
    "from datatools.jsonutils import *\n",
    "from machinelearning.encoder import *\n",
    "from newssource.asa2.utils import *\n",
    "from machinelearning import tf2utils\n",
    "from newssource.dbert.utils import *\n",
    "from twinews.utils import *\n",
    "from twinews.models.ranking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger('generic-cache-' + getHostname() + '.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(user, password, host) = getMongoAuth(user='hayj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsCollection = getNewsCollection(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = newsCollection.distinct('_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIdsChunks(n=200, verbose=True, maxIds=None, doShuffle=True):\n",
    "    if TEST:\n",
    "        n = 10\n",
    "    if maxIds is not None:\n",
    "        localIds = ids[:maxIds]\n",
    "    else:\n",
    "        localIds = ids\n",
    "    if doShuffle:\n",
    "        localIds = shuffle(localIds)\n",
    "    if TEST:\n",
    "        ch = chunks(localIds, n)[:4]\n",
    "    else:\n",
    "        ch = chunks(localIds, n)\n",
    "    bp(ch, logger, verbose=verbose)\n",
    "    return ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicGenFunct(containers, *args, field='detokText', yieldCache=True, cacheKey=None, logger=None, verbose=True, **kwargs):\n",
    "    \"\"\"\n",
    "        This function yield the hash with the data (by field).\n",
    "        If you gave a cacheKey, it will check if the row already exists in the cache.\n",
    "        This function also yield the cache. So the yielded tuple is 3-sized.\n",
    "    \"\"\"\n",
    "    if cacheKey is None:\n",
    "        cache = None\n",
    "        logWarning(\"You didn't give a cache key\", logger, verbose=verbose)\n",
    "    else:\n",
    "        cache = getGenericCache(cacheKey, logger=logger, verbose=False)\n",
    "    newsCollection = getNewsCollection(logger=logger, verbose=False)\n",
    "    if not isinstance(containers[0], list):\n",
    "        containers = [containers]\n",
    "    for container in containers:\n",
    "        for currentId in container:\n",
    "            row = newsCollection.findOne({'_id': currentId}, projection={field})\n",
    "            data = row[field]\n",
    "            theHash = objectToHash(data)\n",
    "            if cache is None:\n",
    "                yield (theHash, data)\n",
    "            elif theHash not in cache:\n",
    "                if yieldCache:\n",
    "                    yield (theHash, data, cache)\n",
    "                else:\n",
    "                    yield (theHash, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, 'dbert'):\n",
    "    raise Exception(\"Need to re-implement this part (the cache key is dbert-ft)\")\n",
    "    # The best model is dbert-94bef_ep32-distilbert-mean\n",
    "    modelNames = [] # ['94bef_ep32', None]\n",
    "    if toCache(cacheFields, 'dbert-ft'):\n",
    "        modelNames.append('94bef_ep32')\n",
    "    if toCache(cacheFields, 'dbert-base'):\n",
    "        modelNames.append(None)\n",
    "    bp(modelNames, 5, logger)\n",
    "    maxLength = 512\n",
    "    batchSize = 16\n",
    "    # layers = ['distilbert', 'pre_classifier', 'dropout', 'classifier'] if modelPath is not None else ['distilbert']\n",
    "    layer = 'distilbert'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if toCache(cacheFields, 'dbert'):\n",
    "    raise Exception(\"Need to re-implement this part (the cache key is dbert-ft)\")\n",
    "    for modelName in modelNames:\n",
    "        if modelName is None:\n",
    "            modelPath = None\n",
    "            modelName = 'base'\n",
    "        else:\n",
    "            if lri():\n",
    "                modelPath = sortedGlob(nosaveDir() + \"/dbert-tmp/\" + modelName + \"/epochs/ep*\")[0]\n",
    "            else:\n",
    "                modelPath = sortedGlob(homeDir() + \"/asa/dbert-tmp/\" + modelName + \"/epochs/ep*\")[0]\n",
    "        # We init the model:\n",
    "        if modelPath is None:\n",
    "            dbertConfig = DistilBertConfig.from_pretrained\\\n",
    "            (\n",
    "                \"distilbert-base-uncased\",\n",
    "                num_labels=1200,\n",
    "                max_length=maxLength,\n",
    "            )\n",
    "            model = TFDistilBertForSequenceClassification.from_pretrained\\\n",
    "            (\n",
    "                \"distilbert-base-uncased\",\n",
    "                config=dbertConfig,\n",
    "            )\n",
    "        else:\n",
    "            dbertConfig = DistilBertConfig.from_pretrained(modelPath + '/config.json')\n",
    "            model = TFDistilBertForSequenceClassification.from_pretrained\\\n",
    "            (\n",
    "                modelPath + '/tf_model.h5',\n",
    "                config=dbertConfig,\n",
    "            )\n",
    "        log(modelName + \" dbert model loaded.\", logger)\n",
    "        # We init the cache:\n",
    "        cache = getGenericCache(\"dbert-\" + modelName)\n",
    "        # We find ids:\n",
    "        ids = set()\n",
    "        pbar = ProgressBar(len(newsCollection), printRatio=0.01, logger=logger, message=\"Finding not yet computed texts\")\n",
    "        for row in newsCollection.find({}, projection={'_id': True, 'detokText': True}):\n",
    "            currentHash = objectToHash(row['detokText'])\n",
    "            if currentHash not in cache:\n",
    "                ids.add(row['_id'])\n",
    "            pbar.tic()\n",
    "        if TEST:\n",
    "            ids = ids[:3]\n",
    "        log(str(len(ids)) + \" rows to process...\", logger)\n",
    "        # We init the Progress Bar:\n",
    "        pbar = ProgressBar(len(ids), logger=logger, printRatio=0.0001, message=\"Generating vectors\")\n",
    "        # We infer all embeddings for each row in each file:\n",
    "        for currentId in ids:\n",
    "            row = newsCollection.findOne({'_id': currentId}, projection={'detokText': True})\n",
    "            if dictContains(row, 'detokText'):\n",
    "                text= row['detokText']\n",
    "                currentHash = objectToHash(text)\n",
    "                if currentHash not in cache:\n",
    "                    encodedText = tf2utils.distilBertEncode\\\n",
    "                    (\n",
    "                        text,\n",
    "                        maxLength=maxLength,\n",
    "                        multiSamplage=True,\n",
    "                        preventTokenizerWarnings=True,\n",
    "                    )\n",
    "                    encodedBatches = chunks(encodedText, batchSize)\n",
    "                    embeddings = []\n",
    "                    for encodedBatch in encodedBatches:\n",
    "                        outputs = tf2utils.getDistilBertRepresentations(model, np.array(encodedBatch), layer=layer)\n",
    "                        for output in outputs:\n",
    "                            embeddings.append(np.array(output))\n",
    "                    embeddings = np.mean(embeddings, axis=0)\n",
    "                    cache[currentHash] = embeddings\n",
    "            pbar.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stylo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `subprocessWrite = True` when you have few processing in sub-processes and `subprocessWrite = False` when sub processing is time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'stylo'\n",
    "cacheKey = model\n",
    "if toCache(cacheFields, model):\n",
    "    subprocessWrite = True # False = 57 secs, True = 52 secs (for 50000 rows)\n",
    "    idsChunks = getIdsChunks()\n",
    "    cache = getGenericCache(cacheKey)\n",
    "    field = genericFields[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    def parseFunct(row, *args, **kwargs):\n",
    "        (theHash, detokText, cache) = row\n",
    "        styloVector = stylo(detokText, asNpArray=True)\n",
    "        if subprocessWrite:\n",
    "            cache[theHash] = styloVector\n",
    "            return None\n",
    "        else:\n",
    "            return (theHash, styloVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    mli = MLIterator\\\n",
    "    (\n",
    "        idsChunks,\n",
    "        basicGenFunct, genKwargs={'field': field, 'cacheKey': cacheKey},\n",
    "        parseFunct=parseFunct,\n",
    "        queuesMaxSize=1000, printRatio=0.01, parallelProcesses=cpuCount(),\n",
    "        logger=logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    for row in mli:\n",
    "        if not subprocessWrite:\n",
    "            (theHash, styloVector) = row\n",
    "            cache[theHash] = styloVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InferSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best is `V=1` and `method=\"mean\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'infersent'\n",
    "cacheKey = model\n",
    "if TEST:\n",
    "    cacheKey += \"-test\"\n",
    "if toCache(cacheFields, model):\n",
    "    idsChunks = getIdsChunks()\n",
    "    cache = getGenericCache(cacheKey)\n",
    "    field = genericFields[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    gitClonesDir = tmpDir(\"git-clones\")\n",
    "    mkdir(gitClonesDir)\n",
    "    infersentDir = gitClonesDir + \"/infersent\"\n",
    "    bash(\"git clone https://github.com/facebookresearch/InferSent \" + infersentDir)\n",
    "    bash(\"touch \" + infersentDir + \"/__init__.py\")\n",
    "    sys.path.append(gitClonesDir)\n",
    "    if isDocker():\n",
    "        bash(\"pip install torch\")\n",
    "    from infersent.models import InferSent\n",
    "    import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    log(\"Loading InferSent model...\", logger)\n",
    "    V = 1\n",
    "    MODEL_PATH = nosaveDir() + '/infersent/infersent%s.pkl' % V\n",
    "    params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                    'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "    infersent = InferSent(params_model)\n",
    "    infersent.load_state_dict(torch.load(MODEL_PATH))\n",
    "    W2V_PATH = nosaveDir() + '/infersent/GloVe/glove.840B.300d.txt'\n",
    "    infersent.set_w2v_path(W2V_PATH)\n",
    "    log(\"Loading docs for vocab...\", logger)\n",
    "    detokDocs = [e[\"detokText\"] for e in newsCollection.find({}, limit=200 if TEST else 0)]\n",
    "    log(\"Building vocab...\", logger)\n",
    "    infersent.build_vocab(detokDocs, tokenize=True)\n",
    "    detokDocs = None\n",
    "    tt.tic(\"InferSent initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    mli = MLIterator\\\n",
    "    (\n",
    "        idsChunks,\n",
    "        basicGenFunct, genKwargs={'field': field, 'cacheKey': cacheKey, 'yieldCache': False},\n",
    "        queuesMaxSize=1000, printRatio=0.01, parallelProcesses=cpuCount(),\n",
    "        logger=logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    for row in mli:\n",
    "        (theHash, detokSentences) = row\n",
    "        embedding = np.mean(infersent.encode(detokSentences, tokenize=True), axis=0)\n",
    "        cache[theHash] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sent2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install Cython ; git clone https://github.com/epfml/sent2vec.git ; cd ./sent2vec ; pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'sent2vec'\n",
    "cacheKey = model\n",
    "if TEST:\n",
    "    cacheKey += \"-test\"\n",
    "if toCache(cacheFields, model):\n",
    "    idsChunks = getIdsChunks(10000)\n",
    "    cache = getGenericCache(cacheKey)\n",
    "    field = genericFields[model]\n",
    "    s2vModel = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    installSent2Vec()\n",
    "    import sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    def parseFunct(row, *args, logger=None, verbose=True, **kwargs):\n",
    "        global s2vModel\n",
    "        if s2vModel is None:\n",
    "            log(\"Loading s2v model...\", logger, verbose=verbose)\n",
    "            s2vModel = sent2vec.Sent2vecModel()\n",
    "            s2vModel.load_model(nosaveDir() + '/sent2vec/' + \"wiki_unigrams.bin\")\n",
    "        (theHash, detokSentences) = row\n",
    "        embedding = np.mean(s2vModel.embed_sentences(detokSentences), axis=0)\n",
    "        return (theHash, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    mli = MLIterator\\\n",
    "    (\n",
    "        idsChunks,\n",
    "        basicGenFunct, genKwargs={'field': field, 'cacheKey': cacheKey, 'yieldCache': False},\n",
    "        parseFunct=parseFunct,\n",
    "        queuesMaxSize=1000, printRatio=0.01, parallelProcesses=2 if TEST else cpuCount(),\n",
    "        logger=logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    for row in mli:\n",
    "        (theHash, embedding) = row\n",
    "        cache[theHash] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'word2vec'\n",
    "cacheKey = model\n",
    "if TEST:\n",
    "    cacheKey += \"-test\"\n",
    "if toCache(cacheFields, model):\n",
    "    idsChunks = getIdsChunks(10000)\n",
    "    cache = getGenericCache(cacheKey)\n",
    "    field = genericFields[model]\n",
    "    wordEmbeddings = None\n",
    "    wvPattern = \"word2vec-googlenews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    def parseFunct(row, *args, logger=None, verbose=True, **kwargs):\n",
    "        global wordEmbeddings\n",
    "        if wordEmbeddings is None:\n",
    "            log(\"Loading word2vec...\", logger, verbose=verbose)\n",
    "            dataDir = None\n",
    "            if isHostname('tipi'):\n",
    "                dataDir = \"/special/hayj/Embeddings\"\n",
    "            emb = Embeddings(wvPattern, dataDir=dataDir, logger=logger, verbose=False)\n",
    "            wordEmbeddings = emb.getVectors()\n",
    "            assert len(wordEmbeddings) > 300\n",
    "        (theHash, sentences) = row\n",
    "        doc = flattenLists(sentences)\n",
    "        if \"The\" not in wordEmbeddings:\n",
    "            doc = [e.lower() for e in doc]\n",
    "        embedding = tokensToEmbedding(doc, wordEmbeddings, operation='mean', removeDuplicates=True, verbose=False)\n",
    "        return (theHash, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    mli = MLIterator\\\n",
    "    (\n",
    "        idsChunks,\n",
    "        basicGenFunct, genKwargs={'field': field, 'cacheKey': cacheKey, 'yieldCache': False},\n",
    "        parseFunct=parseFunct,\n",
    "        queuesMaxSize=1000, printRatio=0.01, parallelProcesses=2 if TEST else cpuCount(),\n",
    "        logger=logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    for row in mli:\n",
    "        (theHash, embedding) = row\n",
    "        cache[theHash] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getD2v(path=nosaveDir() + \"/d2v/d2vmodel-t-ds22.02g-s300-w3-n15-e15-lTrue-adFalse-7bb8a\", loadModel=True):\n",
    "    d2vConfig = None\n",
    "    d2vModel = None\n",
    "    if isFile(path + \"/config.json\"):\n",
    "        d2vConfig = fromJsonFile(path + \"/config.json\")\n",
    "    if loadModel:\n",
    "        d2vModel = Doc2Vec.load(sortedGlob(path + \"/*model*.d2v\")[0])\n",
    "    return (d2vModel, d2vConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'doc2vec'\n",
    "cacheKey = model\n",
    "if TEST:\n",
    "    cacheKey += \"-test\"\n",
    "if toCache(cacheFields, model):\n",
    "    from gensim.models.doc2vec import Doc2Vec\n",
    "    idsChunks = getIdsChunks(10000)\n",
    "    cache = getGenericCache(cacheKey)\n",
    "    field = genericFields[model]\n",
    "    d2vModel = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    def parseFunct(row, *args, logger=None, verbose=True, **kwargs):\n",
    "        global d2vModel\n",
    "        if d2vModel is None:\n",
    "            log(\"Loading doc2vec...\", logger)\n",
    "            (d2vModel, d2vConfig) = getD2v()\n",
    "        (theHash, detokSentences) = row\n",
    "        doc = flattenLists(detokSentences)\n",
    "        embedding = d2vTokenssToEmbeddings([doc], d2vModel, doLower=True, verbose=False)\n",
    "        return (theHash, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    mli = MLIterator\\\n",
    "    (\n",
    "        idsChunks,\n",
    "        basicGenFunct, genKwargs={'field': field, 'cacheKey': cacheKey, 'yieldCache': False},\n",
    "        parseFunct=parseFunct,\n",
    "        queuesMaxSize=1000, printRatio=0.01, parallelProcesses=1 if TEST else 4,\n",
    "        logger=logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    for row in mli:\n",
    "        (theHash, embedding) = row\n",
    "        cache[theHash] = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'bert'\n",
    "cacheKey = model\n",
    "if TEST:\n",
    "    cacheKey += \"-test\"\n",
    "if toCache(cacheFields, model):\n",
    "    bash(\"pip install bert-serving-client==1.10.0\")\n",
    "    from bert_serving.client import BertClient\n",
    "    idsChunks = getIdsChunks()\n",
    "    cache = getGenericCache(cacheKey)\n",
    "    field = genericFields[model]\n",
    "    bc = None # BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    def parseFunct(row, *args, logger=None, verbose=True, **kwargs):\n",
    "        global bc\n",
    "        if bc is None:\n",
    "            bc = BertClient(ip='titanv.lri.fr', check_length=False)\n",
    "        (theHash, detokSentences, cache) = row\n",
    "        bertBatch = bc.encode(detokSentences)\n",
    "        bertBatchMean = np.mean(bertBatch, axis=0)\n",
    "        cache[theHash] = bertBatchMean\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    mli = MLIterator\\\n",
    "    (\n",
    "        idsChunks,\n",
    "        basicGenFunct, genKwargs={'field': field, 'cacheKey': cacheKey, 'yieldCache': True},\n",
    "        parseFunct=parseFunct,\n",
    "        queuesMaxSize=1000, printRatio=0.01, parallelProcesses=4,\n",
    "        logger=logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if toCache(cacheFields, model):\n",
    "    for row in mli:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
