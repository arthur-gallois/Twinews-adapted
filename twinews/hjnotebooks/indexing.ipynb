{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â cd ~/twinews-logs ; jupython --venv st-venv ~/notebooks/twinews/indexing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from databasetools.mongo import *\n",
    "from newstools.goodarticle.utils import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.news import parser as newsParser\n",
    "from machinelearning.iterator import *\n",
    "from twinews.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/twinews-indexing.log\") if isNotebook else Logger(\"indexing.log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = False\n",
    "initialDatasetVersion = 1 if isNotebook else 3\n",
    "datasetVersion = \"1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRootPath = dataDir() + \"/Twinews/\" + \"twinews\" + str(initialDatasetVersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsFiles = sortedGlob(dataRootPath + \"/news/*.bz2\")\n",
    "bp(newsFiles, logger)\n",
    "bp(list(NDJson(random.choice(newsFiles)))[0].keys(), 5, logger)\n",
    "bp(list(NDJson(random.choice(newsFiles)))[0]['scrap'].keys(), 5, logger)\n",
    "if TEST:\n",
    "    newsFiles = newsFiles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersFiles = sortedGlob(dataRootPath + \"/users/*.bz2\")\n",
    "bp(usersFiles, logger)\n",
    "bp(list(NDJson(random.choice(usersFiles)))[0].keys(), 5, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hjuser, hjpass, hjhost) = getMongoAuth(user='hayj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsCollection = getNewsCollection(logger=logger, user=hjuser, password=hjpass, host=hjhost)\n",
    "usersCollection = getUsersCollection(logger=logger, user=hjuser, password=hjpass, host=hjhost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!! DELETION OF THE DATABASE !!!!!!\n",
    "if False:\n",
    "    newsCollection.resetCollection(security=False)\n",
    "    usersCollection.resetCollection(security=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(newsCollection) == 0\n",
    "assert len(usersCollection) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Init done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessNews(row, logger=None, verbose=True):\n",
    "    try:\n",
    "        if isinstance(row, str):\n",
    "            text = row\n",
    "        else:\n",
    "            if dictContains(row, \"scrap\"):\n",
    "                row = row['scrap']\n",
    "            if dictContains(row, \"text\"):\n",
    "                text = row['text']\n",
    "            else:\n",
    "                raise Exception(\"No text found in \" + b(row))\n",
    "        text = newsPreclean(text)\n",
    "        isGood = isGoodArticle(text)\n",
    "        rawText = text\n",
    "        (text, tokens) = newsParser.parseNews(rawText, logger=logger, verbose=verbose)\n",
    "        return (rawText, text, tokens, isGood)\n",
    "    except Exception as e:\n",
    "        logException(e, logger, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newsGenFunct(containers, logger=None, verbose=True):\n",
    "    if not isinstance(containers, list):\n",
    "        containers = [containers]\n",
    "    for container in containers:\n",
    "        for row in NDJson(container, logger=logger, verbose=verbose):\n",
    "            \n",
    "            current = dictSelect(row, {'domain', 'lastUrlDomain', 'url', 'title', 'redirected', 'lastUrl'})\n",
    "            scrap = row['scrap']\n",
    "            current[\"title\"] = preprocess(current[\"title\"], doRemoveUrls=True, unescapeHtml=True,\n",
    "                                  removeHtml=True, doQuoteNormalization=True,\n",
    "                                  doReduceBlank=True, keepNewLines=False, logger=logger)\n",
    "            (rawText, text, sentences, isGood) = preprocessNews(scrap, logger=logger)\n",
    "            current[\"rawText\"] = rawText\n",
    "            current[\"text\"] = text\n",
    "            current[\"sentences\"] = sentences\n",
    "            # current[\"isGoodArticle\"] = isGood\n",
    "            if isGood and text is not None and rawText is not None and sentences is not None and len(sentences) > 0:\n",
    "                yield current\n",
    "            else:\n",
    "                yield None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsFilesChunks = chunks(newsFiles, int(len(shuffle(newsFiles)) / (cpuCount() * 16)))\n",
    "bp(newsFilesChunks, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We add all news (22 mins for the v1 dataset):\n",
    "mli = MLIterator(newsFilesChunks, newsGenFunct, logger=logger, parallelProcesses=cpuCount(), printRatio=0.001)\n",
    "notGoodCount = 0\n",
    "totalCount = 0\n",
    "duplicatesCount = 0\n",
    "hashes = set()\n",
    "for row in mli:\n",
    "    if row is None:\n",
    "        notGoodCount += 1\n",
    "    else:\n",
    "        h = objectToHash(row[\"text\"])\n",
    "        if h in hashes:\n",
    "            duplicatesCount += 1\n",
    "        hashes.add(h)\n",
    "        newsCollection.insert(row)\n",
    "    totalCount += 1\n",
    "log(\"We removed \" + str(int(notGoodCount / totalCount * 100)) + \"% of news.\", logger)\n",
    "log(\"Count of duplicates: \" + str(duplicatesCount), logger)\n",
    "log(\"% of duplicates: \" + str(duplicatesCount / totalCount * 100), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We construct a strucutre: hash of the text --> url\n",
    "newsToKeepByHash = dict()\n",
    "for _id, row in newsCollection.items():\n",
    "    h = objectToHash(row[\"text\"])\n",
    "    newsToKeepByHash[h] = row['url']\n",
    "newsToKeep = set(newsToKeepByHash.values())\n",
    "bp(newsToKeepByHash, logger)\n",
    "assert len(newsToKeep) == len(newsToKeepByHash)\n",
    "# We construct a structure: the duplicate --> the reference news to take into account\n",
    "duplicates = dict()\n",
    "for _id, row in newsCollection.items():\n",
    "    h = objectToHash(row[\"text\"])\n",
    "    if newsToKeepByHash[h] != row['url']:\n",
    "        duplicates[row['url']] = newsToKeepByHash[h]\n",
    "bp(duplicates, logger)\n",
    "# We remove duplicates in news:\n",
    "for dup in duplicates:\n",
    "    newsCollection.delete({\"url\": dup})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"News indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usersGenFunct(containers, maxUsersPerContainer=None, logger=None, verbose=True):\n",
    "    if not isinstance(containers, list):\n",
    "        containers = [containers]\n",
    "    for container in containers:\n",
    "        usersCount = 0\n",
    "        for row in NDJson(container, logger=logger, verbose=verbose):\n",
    "            try:\n",
    "                assert dictContains(row, \"tweets\")\n",
    "                # We handle news:\n",
    "                currentNews = dict()\n",
    "                for tweet in row[\"tweets\"]:\n",
    "                    for n in tweet['news']:\n",
    "                        currentNews[n] = tweet['timestamp']\n",
    "                newCurrentNews = dict()\n",
    "                for n, ts in currentNews.items():\n",
    "                    if n in duplicates:\n",
    "                        n = duplicates[n]\n",
    "                    if n in newsToKeep:\n",
    "                        newCurrentNews[n] = ts\n",
    "                newCurrentNews = sortBy(newCurrentNews, index=1)\n",
    "                row[\"news\"] = [e[0] for e in newCurrentNews]\n",
    "                row[\"timestamps\"] = [e[1] for e in newCurrentNews]\n",
    "                # We parse tweets:\n",
    "                for tweet in row[\"tweets\"]:\n",
    "                    if dictContains(tweet, \"text\"):\n",
    "                        tweet[\"text\"] = preprocess\\\n",
    "                        (\n",
    "                            tweet[\"text\"], logger=logger,\n",
    "                            doQuoteNormalization=True,\n",
    "                            doReduceBlank=True,\n",
    "                            keepNewLines=True,\n",
    "                            stripAccents=True,\n",
    "                            doRemoveUrls=True,\n",
    "                            doLower=False,\n",
    "                            doBadlyEncoded=True,\n",
    "                            doReduceCharSequences=True,\n",
    "                            charSequencesMaxLength=3,\n",
    "                            replaceUnknownChars=True,\n",
    "                            unknownReplacer=\" \",\n",
    "                            doSpecialMap=True,\n",
    "                            doNormalizeEmojis=True,\n",
    "                            doTokenizingHelp=True,\n",
    "                        )\n",
    "                yield row\n",
    "                usersCount += 1\n",
    "                if maxUsersPerContainer is not None and usersCount >= maxUsersPerContainer:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                logException(e, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mli = MLIterator\\\n",
    "(\n",
    "    usersFiles, usersGenFunct,\n",
    "    genKwargs={\"maxUsersPerContainer\": 30 if TEST else None},\n",
    "    logger=logger, parallelProcesses=cpuCount(),\n",
    "    printRatio=0.001,\n",
    ")\n",
    "for row in mli:\n",
    "    usersCollection.insert(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Users indexed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding users references in news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We iterate all users to make the newsUsersMapping:\n",
    "newsUsersMapping = dict()\n",
    "ids = usersCollection.distinct(\"user_id\")\n",
    "for userId in pb(ids, logger=logger, message=\"Finding all users by news\"):\n",
    "    data = usersCollection.findOne({\"user_id\": userId})\n",
    "    for i in range(len(data[\"news\"])):\n",
    "        news = data[\"news\"][i]\n",
    "        ts = data[\"timestamps\"][i]\n",
    "        if news not in newsUsersMapping:\n",
    "            newsUsersMapping[news] = dict()\n",
    "        newsUsersMapping[news][userId] = ts\n",
    "bp(newsUsersMapping, logger)\n",
    "tt.tic(\"Users by news collected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "countOfGreaterThan1 = 0\n",
    "total = 0\n",
    "for url, users in newsUsersMapping.items():\n",
    "    if len(users) > 1:\n",
    "        # log(url + \" --> \" + b(users), logger)\n",
    "        countOfGreaterThan1 += 1\n",
    "    total += len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"total: \" + str(total), logger)\n",
    "log(\"countOfGreaterThan1: \" + str(countOfGreaterThan1), logger)\n",
    "log(\"Mean users amount per news: \" + str(total / len(newsUsersMapping)), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We insert the newsUsersMapping:\n",
    "for url, users in pb(newsUsersMapping.items(), logger=logger, message=\"Inserting user list in all news rows\"):\n",
    "    users = sortBy(users, index=1)\n",
    "    newsCollection.updateOne({\"url\": url},\n",
    "    {\n",
    "        \"$set\":\n",
    "        {\n",
    "            \"users\": [e[0] for e in users],\n",
    "            \"timestamps\": [e[1] for e in users]\n",
    "        }\n",
    "    })\n",
    "tt.tic(\"newsUsersMapping inserted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting news having no users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching news that has no users:\n",
    "toDeleteNews = []\n",
    "for _id, row in newsCollection.items():\n",
    "    if not dictContains(row, 'users') or len(row['users']) == 0:\n",
    "        toDeleteNews.append(row['url'])\n",
    "bp(toDeleteNews, logger)\n",
    "log(str(len(toDeleteNews) / len(newsCollection) * 100) + \" % of news to delete because no user shared it.\", logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting news:\n",
    "for url in toDeleteNews:\n",
    "    newsCollection.delete({'url': url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"News having no users deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding minTimestamp and maxTimestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsMinTimestamp = dict()\n",
    "newsMaxTimestamp = dict()\n",
    "for news in newsCollection.find({}, projection={'url': True, 'timestamps': True}):\n",
    "    newsMinTimestamp[news['url']] = news['timestamps'][0]\n",
    "    newsMaxTimestamp[news['url']] = news['timestamps'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in newsMinTimestamp.keys():\n",
    "    newsCollection.updateOne({'url': url},\n",
    "                    {'$set': {'minTimestamp': newsMinTimestamp[url],\n",
    "                              'maxTimestamp': newsMaxTimestamp[url]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersMinTimestamp = dict()\n",
    "usersMaxTimestamp = dict()\n",
    "for user in usersCollection.find({}, projection={'user_id': True, 'timestamps': True}):\n",
    "    if dictContains(user, 'timestamps') and len(user['timestamps']) > 0:\n",
    "        usersMinTimestamp[user['user_id']] = user['timestamps'][0]\n",
    "        usersMaxTimestamp[user['user_id']] = user['timestamps'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bp(usersMinTimestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for userId in pb(list(usersMinTimestamp.keys()), logger=logger):\n",
    "    usersCollection.updateOne({'user_id': userId},\n",
    "                    {'$set': {'minTimestamp': usersMinTimestamp[userId],\n",
    "                              'maxTimestamp': usersMaxTimestamp[userId]}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"minTimestamp and maxTimestamp added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deleting users having no news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toDeleteUsers = []\n",
    "for user in usersCollection.find({}, projection={'user_id': True, 'news': True}):\n",
    "    if not dictContains(user, 'news') or len(user['news']) == 0:\n",
    "        toDeleteUsers.append(user['user_id'])\n",
    "bp(toDeleteUsers, logger)\n",
    "log(str(len(toDeleteUsers) / len(usersCollection) * 100) + \" % of users to delete because no news in.\", logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting news:\n",
    "for userId in toDeleteUsers:\n",
    "    usersCollection.delete({'user_id': userId})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"News having no users deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
