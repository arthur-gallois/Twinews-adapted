{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oomstopper --no-tail nmf-cache ; killbill nmf-cache ; cd ~/twinews-logs ; jupython -o nohup-nmf-cache-$HOSTNAME.out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/models/nmf-cache.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = isNotebook #Â isNotebook, True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.basics import *\n",
    "from twinews.utils import *\n",
    "from twinews.models.ranking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/nmf-cache.log\") if isNotebook else Logger(\"nmf-cache-\" + getHostname() + \".log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \\\n",
    "{\n",
    "    'maxDocuments': 300 if TEST else 300000,\n",
    "    'useExtraNews': False if TEST else True, # None = unlimited, 0 = no extra news\n",
    "    'minDF': 1 / 500 if TEST else 1 / 2000, # Remove words that have a document frequency ratio lower than 1 / 500\n",
    "    'maxDF': 300, # Remove top 300 voc elements\n",
    "    \n",
    "    'nbTopics': 30 if TEST else 100, # 30, 100\n",
    "    'lowercase': True if TEST else True,\n",
    "    'doLemmatization': False if TEST else False,\n",
    "    \n",
    "    'maxIter': 2 if TEST else 200, # 30 for lda, 200 for nmf\n",
    "    \n",
    "    'nmfInit': 'nndsvd', # None, 'nndsvd'\n",
    "    'nmfL1Ratio': 0, # 0.0, 0.5, 1.0\n",
    "    'nmfAlpha': 0.1, # 0.0, 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get urls for the LDA model:\n",
    "newsCollection = getNewsCollection()\n",
    "urlsForModel = shuffle(list(newsCollection.distinct('url')), seed=0)\n",
    "urlsForModel = urlsForModel[:config['maxDocuments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get sentences:\n",
    "sentences = getNewsSentences(urlsForModel, logger=logger)\n",
    "bp(sentences, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We flatten sentences:\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = flattenLists(sentences[i])\n",
    "docs = sentences\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case:\n",
    "if config['lowercase']:\n",
    "    for i in pb(list(range(len(docs))), logger=logger, message=\"Lower casing\"):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = docs[i][u].lower()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization:\n",
    "if config['doLemmatization']:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pbar = ProgressBar(len(docs), logger=logger, message=\"Lemmatization\")\n",
    "    for i in range(len(docs)):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = lemmatizer.lemmatize(docs[i][u])\n",
    "        pbar.tic()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the corpus:\n",
    "docs = filterCorpus(docs, minDF=config['minDF'], maxDF=config['maxDF'],\n",
    "                    removeEmptyDocs=False, allowEmptyDocs=False, logger=logger)\n",
    "for doc in docs: assert len(doc) > 0\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Data preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infering topic vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer\\\n",
    "(\n",
    "    sublinear_tf=True,\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    # lowercase=True, # Doesn't work because we erased preprocessor\n",
    ")\n",
    "vectorsForModel = vectorizer.fit_transform(docs)\n",
    "assert vectorsForModel.shape[0] == config['maxDocuments']\n",
    "model = NMF\\\n",
    "(\n",
    "    n_components=config['nbTopics'],\n",
    "    random_state=0,\n",
    "    alpha=config['nmfAlpha'],\n",
    "    l1_ratio=config['nmfL1Ratio'],\n",
    "    init=config['nmfInit'],\n",
    "    max_iter=config['maxIter'],\n",
    ")\n",
    "model.fit(vectorsForModel)\n",
    "topics = []\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "    wordProb = []\n",
    "    for i in range(len(topic)):\n",
    "        prob = topic[i]\n",
    "        word = feature_names[i]\n",
    "        wordProb.append((word, prob))\n",
    "    wordProb = sortBy(wordProb, desc=True, index=1)[:100]\n",
    "    current = dict()\n",
    "    for word, prob in wordProb:\n",
    "        current[word] = prob\n",
    "    topics.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Model fitted and topic vectors infered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTopics(topics, maxWords=10, logger=None):\n",
    "    for i in range(len(topics)):\n",
    "        log(str(i) + \": \" + str(\" \".join(list(topics[i].keys())[:10])), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTopicsOf(vector, topics, logger=None):\n",
    "    topicsRepr = \"\"\n",
    "    topTopics = sortBy([(i, score) for i, score in enumerate(vector) if score > 0.001], desc=True, index=1)[:3]\n",
    "    log(\"Top topics number are: \" + str(\" \".join([str(e[0]) for e in topTopics])), logger)\n",
    "    currentTopics = [topics[e[0]] for e in topTopics]\n",
    "    printTopics(currentTopics, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printTopics(topics, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infering and caching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twinews.models.genericutils import getGenericCache, genericFields\n",
    "if TEST:\n",
    "    cache = getGenericCache(\"nmf-test\")\n",
    "else:\n",
    "    cache = getGenericCache(\"nmf\")\n",
    "field = genericFields['nmf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(newsCollection.distinct(\"_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    ids = ids[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for currentId in pb(ids, logger=logger, printRatio=0.01, verbose=not TEST):\n",
    "    row = newsCollection.findOne({\"_id\": currentId}, projection={field: True})\n",
    "    sentences = row[field]\n",
    "    theHash = objectToHash(sentences)\n",
    "    # We flatten sentences:\n",
    "    doc = flattenLists(sentences)\n",
    "    # We lowercase:\n",
    "    if config['lowercase']:\n",
    "        for i in range(len(doc)):\n",
    "            doc[i] = doc[i].lower()\n",
    "    # We lemmatize:\n",
    "    if config['doLemmatization']:\n",
    "        for i in range(len(doc)):\n",
    "            doc[i] = lemmatizer.lemmatize(doc[i])\n",
    "    # We vectorize it:\n",
    "    vectors = vectorizer.transform(np.array([doc]))\n",
    "    # We get topics:\n",
    "    topicRepr = model.transform(vectors)[0]\n",
    "    # We print the doc:\n",
    "    if TEST:\n",
    "        bp(doc, logger)\n",
    "        log(theHash, logger)\n",
    "        printTopicsOf(topicRepr, topics, logger=logger)\n",
    "    # We cache it:\n",
    "    cache[theHash] = topicRepr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
