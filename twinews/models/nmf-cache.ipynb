{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oomstopper --no-tail nmf-cache ; killbill nmf-cache ; cd ~/twinews-logs ; jupython -o nohup-nmf-cache-$HOSTNAME.out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/models/nmf-cache.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = isNotebook #Â isNotebook, True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.basics import *\n",
    "from twinews.utils import *\n",
    "from twinews.models.ranking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tictoc starts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/nmf-cache.log\") if isNotebook else Logger(\"nmf-cache-\" + getHostname() + \".log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \\\n",
    "{\n",
    "    'maxDocuments': 300 if TEST else 300000,\n",
    "    'useExtraNews': False if TEST else True, # None = unlimited, 0 = no extra news\n",
    "    'minDF': 1 / 500 if TEST else 1 / 2000, # Remove words that have a document frequency ratio lower than 1 / 500\n",
    "    'maxDF': 300, # Remove top 300 voc elements\n",
    "    \n",
    "    'nbTopics': 30 if TEST else 100, # 30, 100\n",
    "    'lowercase': True if TEST else True,\n",
    "    'doLemmatization': False if TEST else False,\n",
    "    \n",
    "    'maxIter': 2 if TEST else 200, # 30 for lda, 200 for nmf\n",
    "    \n",
    "    'nmfInit': 'nndsvd', # None, 'nndsvd'\n",
    "    'nmfL1Ratio': 0, # 0.0, 0.5, 1.0\n",
    "    'nmfAlpha': 0.1, # 0.0, 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twinews news (version 1.0) initialised.\n"
     ]
    }
   ],
   "source": [
    "# We get urls for the LDA model:\n",
    "newsCollection = getNewsCollection()\n",
    "urlsForModel = shuffle(list(newsCollection.distinct('url')), seed=0)\n",
    "urlsForModel = urlsForModel[:config['maxDocuments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twinews news (version 1.0) initialised.\n",
      "  0% [                    ]\n",
      " 10% [==                  ] (11.97s left)\n",
      " 20% [====                ] (9.96s left)\n",
      " 30% [======              ] (8.586s left)\n",
      " 40% [========            ] (7.23s left)\n",
      " 50% [==========          ] (5.92s left)\n",
      " 60% [============        ] (4.68s left)\n",
      " 70% [==============      ] (3.462s left)\n",
      " 80% [================    ] (2.314s left)\n",
      " 90% [==================  ] (1.143s left)\n",
      "100% [====================] (total duration: 11.4s, mean duration: 0.038s)\n",
      "[\n",
      "  [ [ __int_2__, Things, ..., here, ! ], [ We, hope, ..., recommend, ! ], ..., [ Available, in, ..., queen, . ], [ How, you, ..., everyone, . ] ],\n",
      "  [ [ Everyone, has, ..., stories, . ], [ From, Nov., ..., talent, . ], ..., [ Christ, Church, ..., storytellers, . ], [ The, evening, ..., dancing, . ] ],\n",
      "  ...,\n",
      "  [ [ Reasonable, people, ..., Khadr, . ], [ How, much, ..., worth, ? ], ..., [ The, Harper, ..., then, . ], [ So, is, ..., now, . ] ],\n",
      "  [ [ White, Nationalists, ..., accounts, . ], [ Written, By, ..., accounts, . ], ..., [ Since, Airbnb, ..., discrimination, . ], [ The, company, ..., commitment, . ] ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# We get sentences:\n",
    "sentences = getNewsSentences(urlsForModel, logger=logger)\n",
    "bp(sentences, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ [ __int_2__, Things, ..., everyone, . ], [ Everyone, has, ..., dancing, . ], ..., [ Reasonable, people, ..., now, . ], [ White, Nationalists, ..., commitment, . ] ]\n"
     ]
    }
   ],
   "source": [
    "# We flatten sentences:\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = flattenLists(sentences[i])\n",
    "docs = sentences\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lower casing   0% [                    ]\n",
      "Lower casing  10% [==                  ] (0.09s left)\n",
      "Lower casing  20% [====                ] (0.079s left)\n",
      "Lower casing  30% [======              ] (0.093s left)\n",
      "Lower casing  40% [========            ] (0.075s left)\n",
      "Lower casing  50% [==========          ] (0.07s left)\n",
      "Lower casing  60% [============        ] (0.06s left)\n",
      "Lower casing  70% [==============      ] (0.042s left)\n",
      "Lower casing  80% [================    ] (0.027s left)\n",
      "Lower casing  90% [==================  ] (0.013s left)\n",
      "Lower casing 100% [====================] (total duration: 0.14s, mean duration: 0s)\n",
      "[ [ __int_2__, things, ..., everyone, . ], [ everyone, has, ..., dancing, . ], ..., [ reasonable, people, ..., now, . ], [ white, nationalists, ..., commitment, . ] ]\n"
     ]
    }
   ],
   "source": [
    "# Lower case:\n",
    "if config['lowercase']:\n",
    "    for i in pb(list(range(len(docs))), logger=logger, message=\"Lower casing\"):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = docs[i][u].lower()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ [ __int_2__, things, ..., everyone, . ], [ everyone, has, ..., dancing, . ], ..., [ reasonable, people, ..., now, . ], [ white, nationalists, ..., commitment, . ] ]\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization:\n",
    "if config['doLemmatization']:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pbar = ProgressBar(len(docs), logger=logger, message=\"Lemmatization\")\n",
    "    for i in range(len(docs)):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = lemmatizer.lemmatize(docs[i][u])\n",
    "        pbar.tic()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Voc removed because of maxDF (300 elements):\n",
      "{ \", ', (, ), ,, -, ., ..., :, ;, ..., without, work, working, world, would, year, years, yet, you, your }\n",
      "1.48% of voc will be removed.\n",
      "[ [ room, comfortable, ..., describe, everyone ], [ everyone, unique, ..., storytelling, dancing ], ..., [ reasonable, debate, ..., harper, trudeau ], [ white, nationalists, ..., breach, commitment ] ]\n"
     ]
    }
   ],
   "source": [
    "# Filtering the corpus:\n",
    "docs = filterCorpus(docs, minDF=config['minDF'], maxDF=config['maxDF'],\n",
    "                    removeEmptyDocs=False, allowEmptyDocs=False, logger=logger)\n",
    "for doc in docs: assert len(doc) > 0\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 15m 17.32s | message: Data preprocessed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "917.32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tic(\"Data preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infering topic vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer\\\n",
    "(\n",
    "    sublinear_tf=True,\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    # lowercase=True, # Doesn't work because we erased preprocessor\n",
    ")\n",
    "vectorsForModel = vectorizer.fit_transform(docs)\n",
    "assert vectorsForModel.shape[0] == config['maxDocuments']\n",
    "model = NMF\\\n",
    "(\n",
    "    n_components=config['nbTopics'],\n",
    "    random_state=0,\n",
    "    alpha=config['nmfAlpha'],\n",
    "    l1_ratio=config['nmfL1Ratio'],\n",
    "    init=config['nmfInit'],\n",
    "    max_iter=config['maxIter'],\n",
    ")\n",
    "model.fit(vectorsForModel)\n",
    "topics = []\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "for topic_idx, topic in enumerate(model.components_):\n",
    "    wordProb = []\n",
    "    for i in range(len(topic)):\n",
    "        prob = topic[i]\n",
    "        word = feature_names[i]\n",
    "        wordProb.append((word, prob))\n",
    "    wordProb = sortBy(wordProb, desc=True, index=1)[:100]\n",
    "    current = dict()\n",
    "    for word, prob in wordProb:\n",
    "        current[word] = prob\n",
    "    topics.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 1.36s | message: Model fitted and topic vectors infered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.36"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tic(\"Model fitted and topic vectors infered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTopics(topics, maxWords=10, logger=None):\n",
    "    for i in range(len(topics)):\n",
    "        log(str(i) + \": \" + str(\" \".join(list(topics[i].keys())[:10])), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTopicsOf(vector, topics, logger=None):\n",
    "    topicsRepr = \"\"\n",
    "    topTopics = sortBy([(i, score) for i, score in enumerate(vector) if score > 0.001], desc=True, index=1)[:3]\n",
    "    log(\"Top topics number are: \" + str(\" \".join([str(e[0]) for e in topTopics])), logger)\n",
    "    currentTopics = [topics[e[0]] for e in topTopics]\n",
    "    printTopics(currentTopics, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: ! yes someone kids thought talk church students feel tell\n",
      "1: tax republicans senate bill republican gop democrats vote r sen.\n",
      "2: technology companies management platform businesses services customer strategy cloud industry\n",
      "3: ! blue duke __netloc__ miami game tickets night win front\n",
      "4: turkey turkish syrian military kurdish russia syria erdogan foreign ankara\n",
      "5: court county defendants attorney lawyer lawsuit drug justice legal filed\n",
      "6: game points scored games team coach hockey season player win\n",
      "7: mccarthy intelligence clinton documents email russian nsa kaspersky lauren emails\n",
      "8: medical health study disease researchers activity brain mental hypothesis rhythms\n",
      "9: hospital schools care martin programs medical housing history students development\n",
      "10: water lake gallons dupage communities chicago county michigan systems exports\n",
      "11: franken sexual sorry women al canada broadcaster tweeden experienced abuse\n",
      "12: chamber startup entrepreneurs area commerce incubator bugg fund chattanooga collective\n",
      "13: airbnb faculty nationalists kaler tolar dean discrimination users health determines\n",
      "14: elmhurst dogs nfl handlers hospital jerusalem league requirements israel therapy\n",
      "15: shooting gun las vegas guns paddock violence attack killed danley\n",
      "16: prime amazon students film side south kennedy films chicago student\n",
      "17: jerusalem israel palestinian palestine capital international carmon haj yahia panel\n",
      "18: done needs test money attorneys music move frustrating fear ask\n",
      "19: writing dreamers am letters daca united write writers words citizenship\n",
      "20: music taxpayers art media social treasury regulations article sound rules\n",
      "21: energy fayetteville sir golf richter solar usage gas wairakei prime\n",
      "22: mr. chantal malaise horror science challenging failures santana boudreau politics\n",
      "23: toronto canada cat sphynx hairless cats canadian financial postal benefits\n",
      "24: cadillac schiller white mcdonald atlanta drop firearm cars given program\n",
      "25: valdrighi juan transplant iphone coalition strikes surgery apple update iraq\n",
      "26: police journalism commons shared man hess ostrom gay village south\n",
      "27: wikileaks restaurant assange eat ecuador ecuadorian dining north food krogh\n",
      "28: greenwich barrow ars ma theater ! nova plans food plating\n",
      "29: needs getting network whatsapp video share data pony fake larger\n"
     ]
    }
   ],
   "source": [
    "printTopics(topics, logger=logger)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "0: ! yes someone kids thought talk church students feel tell\n",
    "1: tax republicans senate bill republican gop democrats vote r sen.\n",
    "2: technology companies management platform businesses services customer strategy cloud industry\n",
    "3: ! blue duke __netloc__ miami game tickets night win front\n",
    "4: turkey turkish syrian military kurdish russia syria erdogan foreign ankara\n",
    "5: court county defendants attorney lawyer lawsuit drug justice legal filed\n",
    "6: game points scored games team coach hockey season player win\n",
    "7: mccarthy intelligence clinton documents email russian nsa kaspersky lauren emails\n",
    "8: medical health study disease researchers activity brain mental hypothesis rhythms\n",
    "9: hospital schools care martin programs medical housing history students development\n",
    "10: water lake gallons dupage communities chicago county michigan systems exports\n",
    "11: franken sexual sorry women al canada broadcaster tweeden experienced abuse\n",
    "12: chamber startup entrepreneurs area commerce incubator bugg fund chattanooga collective\n",
    "13: airbnb faculty nationalists kaler tolar dean discrimination users health determines\n",
    "14: elmhurst dogs nfl handlers hospital jerusalem league requirements israel therapy\n",
    "15: shooting gun las vegas guns paddock violence attack killed danley\n",
    "16: prime amazon students film side south kennedy films chicago student\n",
    "17: jerusalem israel palestinian palestine capital international carmon haj yahia panel\n",
    "18: done needs test money attorneys music move frustrating fear ask\n",
    "19: writing dreamers am letters daca united write writers words citizenship\n",
    "20: music taxpayers art media social treasury regulations article sound rules\n",
    "21: energy fayetteville sir golf richter solar usage gas wairakei prime\n",
    "22: mr. chantal malaise horror science challenging failures santana boudreau politics\n",
    "23: toronto canada cat sphynx hairless cats canadian financial postal benefits\n",
    "24: cadillac schiller white mcdonald atlanta drop firearm cars given program\n",
    "25: valdrighi juan transplant iphone coalition strikes surgery apple update iraq\n",
    "26: police journalism commons shared man hess ostrom gay village south\n",
    "27: wikileaks restaurant assange eat ecuador ecuadorian dining north food krogh\n",
    "28: greenwich barrow ars ma theater ! nova plans food plating\n",
    "29: needs getting network whatsapp video share data pony fake larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infering and caching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twinews.models.genericutils import getGenericCache, genericFields\n",
    "if TEST:\n",
    "    cache = getGenericCache(\"nmf-test\")\n",
    "else:\n",
    "    cache = getGenericCache(\"nmf\")\n",
    "field = genericFields['nmf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(newsCollection.distinct(\"_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    ids = ids[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for currentId in pb(ids, logger=logger, printRatio=0.01, verbose=not TEST):\n",
    "    row = newsCollection.findOne({\"_id\": currentId}, projection={field: True})\n",
    "    sentences = row[field]\n",
    "    theHash = objectToHash(sentences)\n",
    "    # We flatten sentences:\n",
    "    doc = flattenLists(sentences)\n",
    "    # We lowercase:\n",
    "    if config['lowercase']:\n",
    "        for i in range(len(doc)):\n",
    "            doc[i] = doc[i].lower()\n",
    "    # We lemmatize:\n",
    "    if config['doLemmatization']:\n",
    "        for i in range(len(doc)):\n",
    "            doc[i] = lemmatizer.lemmatize(doc[i])\n",
    "    # We vectorize it:\n",
    "    vectors = vectorizer.transform(np.array([doc]))\n",
    "    # We get topics:\n",
    "    topicRepr = model.transform(vectors)[0]\n",
    "    # We print the doc:\n",
    "    if TEST:\n",
    "        bp(doc, logger)\n",
    "        log(theHash, logger)\n",
    "        printTopicsOf(topicRepr, topics, logger=logger)\n",
    "    # We cache it:\n",
    "    cache[theHash] = topicRepr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
