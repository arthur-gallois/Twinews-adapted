{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oomstopper --no-tail tfidf ; killbill tfidf ; cd ~/twinews-logs ; jupython -o nohup-tfidf-$HOSTNAME.out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/models/tfidf.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = isNotebook # isNotebook, True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.basics import *\n",
    "from twinews.utils import *\n",
    "from twinews.models.ranking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/tfidf.log\") if isNotebook else Logger(\"tfidf-\" + getHostname() + \".log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \\\n",
    "{\n",
    "    'splitVersion': 2,\n",
    "    \n",
    "    'maxUsers': 2 if TEST else None, # Sub-sampling\n",
    "    'minDF': None if TEST else None, # 1 / 2000 or None\n",
    "    'maxDF': None if TEST else None, # 300 or None\n",
    "    \n",
    "    'lowercase': False if TEST else False,\n",
    "    'doLemmatization': False if TEST else False,\n",
    "    'sublinearTF': True,\n",
    "    \n",
    "    'dimensions': None if TEST else 3000, # 20, 50, 100, 500, 3000, None (means no dimensionality reduction)\n",
    "    'maxIter': 10, # 3, 10, 30, 100 (Iterations for SVD)\n",
    "    \n",
    "    'distance': 'cosine', # 'cosine', 'euclidean', 'kl', 'js'\n",
    "    'historyRef': 0.3, # 1, 1.0, 0.6, 0.3, 3, 10, 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelName = 'tfidf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['dimensions'] is None:\n",
    "    del config['maxIter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check if we already generated ranking for this model with this specific config:\n",
    "if not isNotebook:\n",
    "    if rankingExists(modelName, config, logger=logger):\n",
    "        raise Exception(modelName + \" with this config already exist:\\n\" + b(config, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting users and news\n",
    "evalData = getEvalData(config['splitVersion'], maxExtraNews=0, maxUsers=config['maxUsers'], logger=logger)\n",
    "(trainUsers, testUsers, trainNews, testNews, candidates, extraNews) = \\\n",
    "(evalData['trainUsers'], evalData['testUsers'], evalData['trainNews'],\n",
    " evalData['testNews'], evalData['candidates'], evalData['extraNews'])\n",
    "bp(evalData.keys(), 5, logger)\n",
    "log(b(evalData['meta'], 5), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here it is important to convert urls to lists because we want the same order to retrieve vectors by index...\n",
    "# And we shuffle it so we do not stick urls a a user at the begin...\n",
    "# But we seed the random to always have same order...\n",
    "trainNewsList = shuffle(list(trainNews), seed=0)\n",
    "testNewsList = shuffle(list(testNews), seed=0)\n",
    "newsList = trainNewsList + testNewsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all:\n",
    "log(str(len(trainNewsList)) + \" urls for trainNewsList\", logger=logger)\n",
    "log(str(len(testNewsList)) + \" urls for testNewsList\", logger=logger)\n",
    "log(str(len(newsList)) + \" urls for newsList\", logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get sentences:\n",
    "sentences = getNewsSentences(newsList, logger=logger)\n",
    "bp(sentences, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We flatten sentences:\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = flattenLists(sentences[i])\n",
    "docs = sentences\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case:\n",
    "if config['lowercase']:\n",
    "    for i in pb(list(range(len(docs))), logger=logger, message=\"Lower casing\"):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = docs[i][u].lower()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization:\n",
    "if config['doLemmatization']:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pbar = ProgressBar(len(docs), logger=logger, message=\"Lemmatization\")\n",
    "    for i in range(len(docs)):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = lemmatizer.lemmatize(docs[i][u])\n",
    "        pbar.tic()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering the corpus:\n",
    "if config['minDF'] is not None or config['maxDF'] is not None:\n",
    "    docs = filterCorpus(docs, minDF=config['minDF'], maxDF=config['maxDF'],\n",
    "                        removeEmptyDocs=False, allowEmptyDocs=False, logger=logger)\n",
    "    for doc in docs: assert len(doc) > 0\n",
    "    bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Data preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infering vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer\\\n",
    "(\n",
    "    sublinear_tf=config['sublinearTF'],\n",
    "    tokenizer=lambda x: x,\n",
    "    preprocessor=lambda x: x,\n",
    "    # lowercase=True, # Doesn't work because we erased preprocessor\n",
    ")\n",
    "vectors = vectorizer.fit_transform(docs)\n",
    "assert vectors.shape[0] == len(newsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"TFIDF vectors generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['dimensions'] is not None:\n",
    "    svd = TruncatedSVD(n_components=config['dimensions'], n_iter=config['maxIter'], random_state=42)\n",
    "    vectors = svd.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a dict url --> topic vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert vectors.shape[0] == len(newsList)\n",
    "urlsVectors = dict()\n",
    "for i in range(len(newsList)):\n",
    "    if isinstance(vectors, scipy.sparse.csr.csr_matrix):\n",
    "        row = vectors.getrow(i)\n",
    "    else:\n",
    "        row = vectors[i]\n",
    "    urlsVectors[newsList[i]] = row\n",
    "bp(urlsVectors, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the doc!\n",
    "rankings = usersRankingsByHistoryDistance\\\n",
    "(\n",
    "    trainUsers,\n",
    "    candidates,\n",
    "    config['historyRef'],\n",
    "    urlsVectors,\n",
    "    distanceMetric=config['distance'],\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bp(rankings, logger, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Rankings done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding rankings to the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the doc!\n",
    "addRanking(modelName, rankings, config, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Rankings stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalDuration = tt.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notif(modelName + '-' + objectToHash(config)[:5] + \" done in \" + secondsToHumanReadableDuration(totalDuration) + \" on \" + getHostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
