{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd twinews-logs ; jupython -o nohup-tfidf-$HOSTNAME.out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/models/tfidf.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = isNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from nlptools.preprocessing import *\n",
    "from twinews.utils import *\n",
    "from twinews.models.ranking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptools.topicmodeling import *\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tictoc starts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/tfidf.log\") if isNotebook else Logger(\"tfidf-\" + getHostname() + \".log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \\\n",
    "{\n",
    "    'splitVersion': 2,\n",
    "    \n",
    "    'maxUsers': 2 if TEST else None, # Sub-sampling\n",
    "    'minDF': 1 / 500 if TEST else 1 / 2000, # Remove words that have a document frequency ratio lower than 1 / 500\n",
    "    'maxDF': 300, # Remove top 300 voc elements\n",
    "    \n",
    "    'lowercase': False if TEST else True,\n",
    "    'doLemmatization': False if TEST else False,\n",
    "    'useTFIDF': False,\n",
    "    \n",
    "    'distance': 'cosine', # 'cosine', 'euclidean', 'kl', 'js'\n",
    "    'historyRef': 30, #Â 1, 1.0, 0.6, 0.3, 3, 10, 30\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check if we already generated ranking for this model with this specific config:\n",
    "if not isNotebook:\n",
    "    if rankingExists(modelName, config, logger=logger):\n",
    "        raise Exception(modelName + \" with this config already exist:\\n\" + b(config, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 31.89s | message: Eval data loaded\n",
      "twinews news (version 1.0) initialised.\n",
      "--> tic: 7.11s | message: Extra news downloaded\n",
      "--> toc total duration: 39.01s | message: Got Twinews evaluation data\n",
      "{ candidates, extraNews, meta, testNews, testUsers, trainNews, trainUsers }\n",
      "{ 'created': 2020.03.24-14.28.06, 'endDate': 2018-01-15, 'id': 2, 'ranksLength': 1000, 'splitDate': 2017-12-25, 'startDate': 2017-10-01, 'testMaxNewsPerUser': 97, 'testMeanNewsPerUser': 7.22, 'testMinNewsPerUser': 2, 'testNewsCount': 71781, 'totalNewsAvailable': 570210, 'trainMaxNewsPerUser': 379, 'trainMeanNewsPerUser': 26.48, 'trainMinNewsPerUser': 8, 'trainNewsCount': 237150, 'usersCount': 15905 }\n"
     ]
    }
   ],
   "source": [
    "# Getting users and news\n",
    "evalData = getEvalData(config['splitVersion'], maxExtraNews=config['maxDocuments'],\n",
    "                       maxUsers=config['maxUsers'], logger=logger)\n",
    "(trainUsers, testUsers, trainNews, testNews, candidates, extraNews) = \\\n",
    "(evalData['trainUsers'], evalData['testUsers'], evalData['trainNews'],\n",
    " evalData['testNews'], evalData['candidates'], evalData['extraNews'])\n",
    "bp(evalData.keys(), 5, logger)\n",
    "log(b(evalData['meta'], 5), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here it is important to convert urls to lists because we want the same order to retrieve vectors by index...\n",
    "# And we shuffle it so we do not stick urls a a user at the begin...\n",
    "# But we seed the random to always have same order...\n",
    "extraNewsList = shuffle(list(extraNews), seed=0)\n",
    "trainNewsList = shuffle(list(trainNews), seed=0)\n",
    "testNewsList = shuffle(list(testNews), seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 urls for urlsForModel:\n",
      "[\n",
      "  http://on.si.com/2yDAEx9,\n",
      "  http://www.forbes.com/sites/bobbymcmahon/2017/11/25/messi-and-barcelona-sign-a-new-contract-to-2021-,\n",
      "  ...,\n",
      "  http://thehill.com/media/368673-fox-friends-host-criticized-trump-shithole-comment-moments-before-he,\n",
      "  https://boston.curbed.com/boston-development/2018/1/10/16870266/allston-project-would-add-74-apartme\n",
      "]\n",
      "2017 urls for urlsToVectorize:\n",
      "[\n",
      "  http://on.si.com/2yDAEx9,\n",
      "  http://www.forbes.com/sites/bobbymcmahon/2017/11/25/messi-and-barcelona-sign-a-new-contract-to-2021-,\n",
      "  ...,\n",
      "  http://www.sfchronicle.com/entertainment/article/The-street-vendor-who-makes-peace-his-profession-12,\n",
      "  http://www.foxnews.com/politics/2018/01/04/house-russia-probe-ending-as-it-began-mess.amp.html?__twi\n",
      "]\n",
      "2017 urls for urlsToInfere:\n",
      "[\n",
      "  http://on.si.com/2yDAEx9,\n",
      "  http://www.forbes.com/sites/bobbymcmahon/2017/11/25/messi-and-barcelona-sign-a-new-contract-to-2021-,\n",
      "  ...,\n",
      "  http://www.sfchronicle.com/entertainment/article/The-street-vendor-who-makes-peace-his-profession-12,\n",
      "  http://www.foxnews.com/politics/2018/01/04/house-russia-probe-ending-as-it-began-mess.amp.html?__twi\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# We get urls for the LDA model:\n",
    "if config['useExtraNews']:\n",
    "    urlsForModel = extraNewsList + trainNewsList + testNewsList\n",
    "else:\n",
    "    urlsForModel = trainNewsList + testNewsList + extraNewsList\n",
    "urlsForModel = urlsForModel[:config['maxDocuments']]\n",
    "# We get urls to vectorize for the training and the inference:\n",
    "urlsForModelSet = set(urlsForModel)\n",
    "urlsToVectorize = copy.deepcopy(urlsForModel)\n",
    "for url in trainNewsList + testNewsList:\n",
    "    if url not in urlsForModelSet:\n",
    "        urlsToVectorize.append(url)\n",
    "# We get urls to infere for the scoring:\n",
    "urlsToInfere = trainNewsList + testNewsList\n",
    "# Print all:\n",
    "log(str(len(urlsForModel)) + \" urls for urlsForModel:\\n\" + b(urlsForModel), logger=logger)\n",
    "log(str(len(urlsToVectorize)) + \" urls for urlsToVectorize:\\n\" + b(urlsToVectorize), logger=logger)\n",
    "log(str(len(urlsToInfere)) + \" urls for urlsToInfere:\\n\" + b(urlsToInfere), logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twinews news (version 1.0) initialised.\n",
      "  0% [                    ]\n",
      "  9% [=                   ] (37.223s left)\n",
      " 19% [===                 ] (31.335s left)\n",
      " 29% [=====               ] (27.459s left)\n",
      " 39% [=======             ] (23.777s left)\n",
      " 49% [=========           ] (19.776s left)\n",
      " 59% [===========         ] (15.803s left)\n",
      " 69% [=============       ] (11.835s left)\n",
      " 79% [===============     ] (7.915s left)\n",
      " 89% [=================   ] (4.057s left)\n",
      " 99% [=================== ] (0.136s left)\n",
      "100% [====================] (total duration: 39.24s, mean duration: 0.019s)\n",
      "[\n",
      "  [ [ Juventus, midfielder, ..., media, . ], [ The, -, ..., page, . ], ..., [ Sturaro, has, ..., league, . ], [ Juventus, do, ..., Sunday, . ] ],\n",
      "  [ [ Barcelona, has, ..., until, __int_4__ ], [ The, previous, ..., June, __int_4__ ], ..., [ Messi, has, ..., marksman, . ], [ Collective, honors, ..., Cups, . ] ],\n",
      "  ...,\n",
      "  [ [ Kate, Winslet, ..., vendors, . ], [ On, April, ..., him, . ], ..., [ Murphy, beamed, ..., to, . ], [ Beth, Spotswood, ..., Datebook, . ] ],\n",
      "  [ [ Want, FOX, ..., day, ? ], [ Sign, up, here, . ], ..., [ Brianna, McClelland, ..., report, . ], [ Want, FOX, ..., day, ? ] ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# We get sentences:\n",
    "sentences = getNewsSentences(urlsToVectorize, logger=logger)\n",
    "bp(sentences, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ [ Juventus, midfielder, ..., Sunday, . ], [ Barcelona, has, ..., Cups, . ], ..., [ Kate, Winslet, ..., Datebook, . ], [ Want, FOX, ..., day, ? ] ]\n"
     ]
    }
   ],
   "source": [
    "# We flatten sentences:\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = flattenLists(sentences[i])\n",
    "docs = sentences\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ [ Juventus, midfielder, ..., Sunday, . ], [ Barcelona, has, ..., Cups, . ], ..., [ Kate, Winslet, ..., Datebook, . ], [ Want, FOX, ..., day, ? ] ]\n"
     ]
    }
   ],
   "source": [
    "# Lower case:\n",
    "if config['lowercase']:\n",
    "    for i in pb(list(range(len(docs))), logger=logger, message=\"Lower casing\"):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = docs[i][u].lower()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ [ Juventus, midfielder, ..., Sunday, . ], [ Barcelona, has, ..., Cups, . ], ..., [ Kate, Winslet, ..., Datebook, . ], [ Want, FOX, ..., day, ? ] ]\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization:\n",
    "if config['doLemmatization']:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pbar = ProgressBar(len(docs), logger=logger, message=\"Lemmatization\")\n",
    "    for i in range(len(docs)):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = lemmatizer.lemmatize(docs[i][u])\n",
    "        pbar.tic()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Voc removed because of minDF (44267 elements):\n",
      "{ 1000th, 100s, 100th, 100x, 103rd, 104th, 108th, 10Weather, 10X, 10x, ..., zooms, zoonotic, zoos, ðŸ’—, ðŸ˜, ðŸ˜„, ðŸ˜‡, ðŸ˜’, ðŸ˜¢, ðŸ™Œ }\n",
      "Voc removed because of maxDF (300 elements):\n",
      "{ \", ', (, ), ,, -, ., :, ;, ?, ..., with, without, work, working, world, would, year, years, you, your }\n",
      "73.9% of voc will be removed.\n",
      "[ [ midfielder, issued, ..., Stadium, Sunday ], [ Barcelona, confirmed, ..., Club, World ], ..., [ Kate, gets, ..., column, appears ], [ Want, News, ..., Report, inbox ] ]\n"
     ]
    }
   ],
   "source": [
    "# Filtering the corpus:\n",
    "docs = filterCorpus(docs, minDF=config['minDF'], maxDF=config['maxDF'],\n",
    "                    removeEmptyDocs=False, allowEmptyDocs=False, logger=logger)\n",
    "for doc in docs: assert len(doc) > 0\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 1m 19.37s | message: Data preprocessed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "79.37"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tic(\"Data preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infering topic vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step we generate `inferedVectors` and `topics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['implementation'] == 'gensim-lda':\n",
    "    dictionary = gensim.corpora.Dictionary(docs)\n",
    "    # dictionary.filter_extremes(no_below=config['min_df'])\n",
    "    bow = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    if config['useTFIDF']:\n",
    "        tfidf = gensim.models.TfidfModel(bow)\n",
    "        bow = tfidf[bow]\n",
    "    assert len(bow) == len(urlsToVectorize)\n",
    "    bowForModel = bow[:len(urlsForModel)]\n",
    "    assert len(bowForModel) == config['maxDocuments']\n",
    "    i = 0\n",
    "    for url in urlsToVectorize:\n",
    "        if url == trainNewsList[0]:\n",
    "            break\n",
    "        i += 1\n",
    "    assert i == len(extraNews) or i == 0\n",
    "    bowForInference = bow[i:i + len(trainNews) + len(testNews)]\n",
    "    assert len(bowForInference) == len(trainNews) + len(testNews)\n",
    "    lda_model = gensim.models.LdaMulticore\\\n",
    "    (\n",
    "        bowForModel,\n",
    "        num_topics=config['nbTopics'],\n",
    "        id2word=dictionary,\n",
    "        iterations=config['maxIter'],\n",
    "        decay=config['ldaLearningDecay'],\n",
    "        offset=config['ldaLearningOffset'],\n",
    "        workers=cpuCount(),\n",
    "        passes=3,\n",
    "    )\n",
    "    inferedVectors = []\n",
    "    for current in bowForInference:\n",
    "        topicProbDistrib = lda_model[current]\n",
    "        currentVector = [0.0] * config['nbTopics']\n",
    "        for t, v in topicProbDistrib:\n",
    "            currentVector[t] = v\n",
    "        inferedVectors.append(np.array(currentVector))\n",
    "    assert len(inferedVectors) == len(trainNews) + len(testNews)\n",
    "    assert len(inferedVectors[0]) == config['nbTopics']\n",
    "    topics = []\n",
    "    for i in range(lda_model.num_topics):\n",
    "        current = dict()\n",
    "        for x in lda_model.get_topic_terms(i, topn=100):\n",
    "            current[dictionary[x[0]]] = x[1]\n",
    "        topics.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['implementation'] == 'sklearn-lda' or config['implementation'] == 'sklearn-nmf':\n",
    "    if config['implementation'] == 'sklearn-nmf' or config['useTFIDF']:\n",
    "        vectorizer = TfidfVectorizer\\\n",
    "        (\n",
    "            sublinear_tf=True,\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            # lowercase=True, # Doesn't work because we erased preprocessor\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer\\\n",
    "        (\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            # lowercase=True, # Doesn't work because we erased preprocessor\n",
    "        )\n",
    "    vectors = vectorizer.fit_transform(docs)\n",
    "    assert vectors.shape[0] == len(urlsToVectorize)\n",
    "    vectorsForModel = vectors[:len(urlsForModel)]\n",
    "    assert vectorsForModel.shape[0] == config['maxDocuments']\n",
    "    i = 0\n",
    "    for url in urlsToVectorize:\n",
    "        if url == trainNewsList[0]:\n",
    "            break\n",
    "        i += 1\n",
    "    assert i == len(extraNews) or i == 0\n",
    "    vectorsForInference = vectors[i:i + len(trainNews) + len(testNews)]\n",
    "    assert vectorsForInference.shape[0] == len(trainNews) + len(testNews)\n",
    "    if config['implementation'] == 'sklearn-lda':\n",
    "        model = LatentDirichletAllocation\\\n",
    "        (\n",
    "            n_components=config['nbTopics'],\n",
    "            learning_method=config['ldaLearningMethod'],\n",
    "            learning_offset=config['ldaLearningOffset'],\n",
    "            learning_decay=config['ldaLearningDecay'],\n",
    "            random_state=0,\n",
    "            n_jobs=cpuCount(),\n",
    "            max_iter=config['maxIter'],\n",
    "        )\n",
    "    else:\n",
    "        model = NMF\\\n",
    "        (\n",
    "            n_components=config['nbTopics'],\n",
    "            random_state=0,\n",
    "            alpha=config['nmfAlpha'],\n",
    "            l1_ratio=config['nmfL1Ratio'],\n",
    "            init=config['nmfInit'],\n",
    "            max_iter=config['maxIter'],\n",
    "        )\n",
    "    model.fit(vectorsForModel)\n",
    "    inferedVectors = model.transform(vectorsForInference)\n",
    "    assert inferedVectors.shape[0] == len(trainNews) + len(testNews)\n",
    "    assert inferedVectors[0].shape[0] == config['nbTopics']\n",
    "    topics = []\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        wordProb = []\n",
    "        for i in range(len(topic)):\n",
    "            prob = topic[i]\n",
    "            word = feature_names[i]\n",
    "            wordProb.append((word, prob))\n",
    "        wordProb = sortBy(wordProb, desc=True, index=1)[:100]\n",
    "        current = dict()\n",
    "        for word, prob in wordProb:\n",
    "            current[word] = prob\n",
    "        topics.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 11.54s | message: Model fitted and topic vectors infered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.54"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tic(\"Model fitted and topic vectors infered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a dict url --> topic vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert len(urlsToInfere) == len(inferedVectors)\n",
    "urlsVectors = dict()\n",
    "for i in range(len(urlsToInfere)):\n",
    "    urlsVectors[urlsToInfere[i]] = inferedVectors[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False and isNotebook:\n",
    "    config['historyRef'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyRef = config['historyRef']\n",
    "assert (isinstance(historyRef, int) and historyRef >= 1) or (isinstance(historyRef, float) and historyRef > 0.0 and historyRef <= 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ranking  50% [==========          ] (0.1s left)\n",
      "Generating ranking 100% [====================] (total duration: 0.15s, mean duration: 0.075s)\n"
     ]
    }
   ],
   "source": [
    "# Read the doc!\n",
    "rankings = usersRankingsByHistoryDistance\\\n",
    "(\n",
    "    trainUsers,\n",
    "    candidates,\n",
    "    historyRef,\n",
    "    urlsVectors,\n",
    "    distanceMetric=config['distance'],\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '2617447752': \n",
      "  [\n",
      "    [\n",
      "      http://ontario.ca/b3v8,\n",
      "      http://nyti.ms/2zaO6c6,\n",
      "      http://www.courant.com/opinion/editorials/hc-ed-ct-needs-left-lane-law-20180101-story.html,\n",
      "      http://bit.ly/2qPkanf,\n",
      "      http://wsvn.com/news/local/burglar-returns-bike-he-stole-outside-of-ne-miami-dade-home/,\n",
      "      https://medium.com/reaching-out/when-the-church-kicked-me-out-this-is-where-i-went-2e214adedf92?source=linkShare-35084a546d22-1515599098,\n",
      "      https://fb.me/8AgCIOtvd,\n",
      "      https://trib.al/HSlqzZ8,\n",
      "      http://bit.ly/2CUajSH,\n",
      "      https://uofacesmg.wordpress.com/2018/01/07/wonder-and-kindness/,\n",
      "      ...,\n",
      "      https://medium.com/shanghaiist/chinese-shoe-company-tricks-people-into-swiping-instagram-ad-with-fake-strand-of-hair-54d8a2d8ec1d,\n",
      "      https://buff.ly/2DfcIEr,\n",
      "      http://www.stltoday.com/news/local/govt-and-politics/protesters-volunteers-urge-city-to-do-more-to-help-homeless/article_4c7990fc-6d8d-579b-bb7a-649fbe225926.html?utm_medium=social&utm_source=facebook&utm_campaign=user-share,\n",
      "      http://bit.ly/2COFUAX,\n",
      "      http://www.crainsdetroit.com/article/20180107/news/649416/document-in-detroits-amazon-hq2-bid-reveals-details-of-talent-marshall,\n",
      "      https://buff.ly/2lDuM2C,\n",
      "      http://www.courant.com/news/connecticut/hc-news-towns-prepay-2018-taxes-20171227-story.html,\n",
      "      https://www.seattletimes.com/business/amazon/seattle-artist-gives-amazons-fleet-of-treasure-trucks-their-distinctive-local-looks/?utm_source=twitter&utm_medium=social&utm_campaign=article_left_1.1,\n",
      "      http://ow.ly/L28b30hsjkh,\n",
      "      https://www.washingtonpost.com/local/can-you-prepay-your-real-estate-taxes-before-the-tax-bill-takes-effect-find-out-here/2017/12/26/5539b14c-ea7c-11e7-8a6a-80acf0774e64_story.html?utm_term=.d3b4b6c86a6e\n",
      "    ]\n",
      "  ],\n",
      "  '790664132': \n",
      "  [\n",
      "    [\n",
      "      http://ow.ly/Vlbj30hG1q8,\n",
      "      http://bit.ly/2E3YUM7,\n",
      "      https://usat.ly/2EzTwAM,\n",
      "      http://www.startribune.com/minneapolis-is-using-electric-bills-to-fight-climate-change/464650183/,\n",
      "      http://www.cbc.ca/news/canada/nova-scotia/amazon-bid-new-location-canada-halifax-1.4361723,\n",
      "      https://medium.com/whatever-source-derived/assessing-the-irss-guidance-on-prepaid-property-taxes-c81c24dcd075,\n",
      "      http://www.democratandchronicle.com/story/news/politics/albany/2017/12/26/prepaying-your-property-taxes/981506001/,\n",
      "      http://www.newsobserver.com/news/politics-government/politics-columns-blogs/under-the-dome/article191973099.html,\n",
      "      https://fb.me/KGi77YhO,\n",
      "      http://www.chicagobusiness.com/article/20171228/BLOGS02/171229924/the-new-tax-law-maybe-isnt-playing-the-way-the-gop-planned?X-IgnoreUserAgent=1,\n",
      "      ...,\n",
      "      http://bit.ly/2DlbU0x,\n",
      "      http://www.shreveporttimes.com/story/news/2018/01/03/smoothie-king-no-more-added-sugar-foam-cups/999835001/,\n",
      "      http://www.themedialine.org/top-stories/dont-marry-hes-palestinia/,\n",
      "      http://ow.ly/FSFL30hOeGe,\n",
      "      https://www.dallasnews.com/opinion/editorials/2017/12/28/texan-year-finalists-hurricane-harveys-ragtag-armada-good-ol-boys,\n",
      "      http://www.bbc.co.uk/news/uk-england-london-42534127,\n",
      "      http://ow.ly/Z1br30hBQn0,\n",
      "      http://bit.ly/2D9jaga,\n",
      "      http://www.thewhig.com/2018/01/04/opp-charge-smith-falls-officer/,\n",
      "      https://www.cbssports.com/soccer/news/wynalda-says-u-s-a-soccer-doesnt-have-its-act-together-and-he-wants-to-change-it/\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "bp(rankings, logger, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 0.29s | message: Rankings done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.29"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tic(\"Rankings done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding rankings to the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception type: <class 'gridfs.errors.FileExists'>\n",
      "Exception: file with _id ObjectId('5e7d19898175116238a94a07') already exists\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hayj/.local/share/virtualenvs/st-venv/lib/python3.6/site-packages/gridfs/grid_file.py\", line 296, in __flush\n",
      "    self._file, session=self._session)\n",
      "  File \"/home/hayj/.local/share/virtualenvs/st-venv/lib/python3.6/site-packages/pymongo/collection.py\", line 693, in insert_one\n",
      "    session=session),\n",
      "  File \"/home/hayj/.local/share/virtualenvs/st-venv/lib/python3.6/site-packages/pymongo/collection.py\", line 607, in _insert\n",
      "    bypass_doc_val, session)\n",
      "  File \"/home/hayj/.local/share/virtualenvs/st-venv/lib/python3.6/site-packages/pymongo/collection.py\", line 595, in _insert_one\n",
      "    acknowledged, _insert_command, session)\n",
      "  File \"/home/hayj/.local/share/virtualenvs/st-venv/lib/python3.6/site-packages/pymongo/mongo_client.py\", line 1243, in _retryable_write\n",
      "    return self._retry_with_session(retryable, func, s, None)\n",
      "  File \"/home/hayj/.local/share/virtualenvs/st-venv/lib/python3.6/site-packages/pymongo/mongo_client.py\", line 1196, in _retry_with_session\n",
      "    return func(session, sock_info, retryable)\n",
      "  File \"/home/hayj/.local/share/virtualenvs/st-venv/lib/python3.6/site-packages/pymongo/collection.py\", line 592, in _insert_command\n",
      "    _check_write_command_response(result)\n",
      "  File \"/home/hayj/.local/share/virtualenvs/st-venv/lib/python3.6/site-packages/pymongo/helpers.py\", line 217, in _check_write_command_response\n",
      "    _raise_last_write_error(write_errors)\n",
      "  File \"/home/hayj/.local/share/virtualenvs/st-venv/lib/python3.6/site-packages/pymongo/helpers.py\", line 198, in _raise_last_write_error\n",
      "    raise DuplicateKeyError(error.get(\"errmsg\"), 11000, error)\n",
      "pymongo.errors.DuplicateKeyError: E11000 duplicate key error collection: twinews-rankings.fs.files index: id_1 dup key: { id: \"lda-72dfd\" }\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hayj/Workspace/Python/Datasets/Twinews/twinews/utils.py\", line 321, in addRanking\n",
      "    twinewsRankings.insert(key, ranks, **config)\n",
      "  File \"/home/hayj/Workspace/Python/Utils/DatabaseTools/databasetools/mongo.py\", line 1035, in insert\n",
      "    self.fs.put(sobj, **{'meta': meta, self.primaryKey: key})\n",
      "  File \"/home/hayj/.local/share/virtualenvs/st-venv/lib/python3.6/site-packages/gridfs/__init__.py\", line 129, in put\n",
      "    grid_file.close()\n",
      "  File \"/home/hayj/.local/share/virtualenvs/st-venv/lib/python3.6/site-packages/gridfs/grid_file.py\", line 311, in close\n",
      "    self.__flush()\n",
      "  File \"/home/hayj/.local/share/virtualenvs/st-venv/lib/python3.6/site-packages/gridfs/grid_file.py\", line 298, in __flush\n",
      "    self._raise_file_exists(self._id)\n",
      "  File \"/home/hayj/.local/share/virtualenvs/st-venv/lib/python3.6/site-packages/gridfs/grid_file.py\", line 302, in _raise_file_exists\n",
      "    raise FileExists(\"file with _id %r already exists\" % file_id)\n",
      "gridfs.errors.FileExists: file with _id ObjectId('5e7d19898175116238a94a07') already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the doc!\n",
    "addRanking(modelName, rankings, config, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 0.25s | message: Rankings stored\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tic(\"Rankings stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> toc total duration: 1m 31.489s\n"
     ]
    }
   ],
   "source": [
    "totalDuration = tt.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "notif(modelName + '-' + objectToHash(config)[:5] + \" done in \" + secondsToHumanReadableDuration(totalDuration) + \" on \" + getHostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
