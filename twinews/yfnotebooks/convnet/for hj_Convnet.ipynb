{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from databasetools.mongo import *\n",
    "from newstools.goodarticle.utils import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.news import parser as newsParser\n",
    "from machinelearning.iterator import *\n",
    "from twinews.utils import *\n",
    "from twinews.models.ranking import *\n",
    "from twinews.evaluation.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptools.basics import *\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "import gensim\n",
    "from math import log2\n",
    "from math import sqrt\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yuting/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from twinews.yfnotebooks.convnet.graph import Graph\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from twinews.yfnotebooks.load_data import load_char_data,char_index  # load data tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()\n",
    "TEST = isNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tictoc starts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/Convnet.log\") if isNotebook else Logger(\"Convnet-\" + getHostname() + \".log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "modelName = \"Convnet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = \\\n",
    "{\n",
    "    'splitVersion': 2,\n",
    "    'testVersion':2, # with some parameters changed\n",
    "    \n",
    "    # for input data\n",
    "    'maxUsers': 10 if TEST else None, # Sub-sampling\n",
    "    'maxDocuments': None,\n",
    "    'useExtraNews': 0 if TEST else None, # None = unlimited, 0 = no extra news\n",
    "    'minDF': 1 / 2000, # Remove words that have a document frequency ratio lower than 1 / 2000\n",
    "    'maxDF': 20, # Remove top 20 voc elements\n",
    "    'lowercase': True,\n",
    "    'doLemmatization': True,\n",
    "    \n",
    "   # for model    \n",
    "    'seq_length' : 100,\n",
    "#     'char_embedding_size': 100,\n",
    "    'learning_rate': 0.0005,\n",
    "    'keep_prob': 0.7,\n",
    "    'vocab_size': 3029,\n",
    "    'class_size': 2,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 1000,\n",
    "    \n",
    "    # for ouput\n",
    "    'similarity': 'cosine',\n",
    "\n",
    "#     'historyRef': 0.3, # 1, 1.0, 0.5, 0.3, 3, 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> tic: 43.19s | message: Eval data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{ candidates, extraNews, meta, testNews, testUsers, trainNews, trainUsers }\n",
      "{ 'created': 2020.03.24-14.28.06, 'endDate': 2018-01-15, 'id': 2, 'ranksLength': 1000, 'splitDate': 2017-12-25, 'startDate': 2017-10-01, 'testMaxNewsPerUser': 97, 'testMeanNewsPerUser': 7.22, 'testMinNewsPerUser': 2, 'testNewsCount': 71781, 'totalNewsAvailable': 570210, 'trainMaxNewsPerUser': 379, 'trainMeanNewsPerUser': 26.48, 'trainMinNewsPerUser': 8, 'trainNewsCount': 237150, 'usersCount': 15905 }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> toc total duration: 43.42s | message: Got Twinews evaluation data\n"
     ]
    }
   ],
   "source": [
    "# Getting users and news\n",
    "evalData = getEvalData(config['splitVersion'], maxExtraNews=0,\n",
    "                       maxUsers=config['maxUsers'])\n",
    "(trainUsers, testUsers, trainNews, testNews, candidates, extraNews) = \\\n",
    "(evalData['trainUsers'], evalData['testUsers'], evalData['trainNews'],\n",
    " evalData['testNews'], evalData['candidates'], evalData['extraNews'])\n",
    "bp(evalData.keys(), 5, logger)\n",
    "log(b(evalData['meta'], 5), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNewsList = shuffle(list(trainNews), seed=0)\n",
    "testNewsList = shuffle(list(testNews), seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9690 urls for newsList:\n",
      "[\n",
      "  https://buff.ly/2yxBJKY,\n",
      "  https://www.bloomberg.com/news/articles/2017-12-13/teva-s-reported-job-cuts-prompt-strike-threat-fro,\n",
      "  ...,\n",
      "  http://www.baltimoresun.com/news/opinion/readersrespond/bs-ed-rr-conowingo-letter-20180111-story.htm,\n",
      "  https://www.nytimes.com/2018/01/12/opinion/proud-nation-of-holers.html\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "newsList = trainNewsList + testNewsList\n",
    "log(str(len(newsList)) + \" urls for newsList:\\n\" + b(newsList), logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twinews news (version 1.0) initialised.\n",
      "  100% [====================] (total duration: 5m 24.79s, mean duration: 0.033s)\n"
     ]
    }
   ],
   "source": [
    "# get all the titles of news urls link to form (url: title) dict\n",
    "titles = getNewsTitles(newsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing   0% [                    ]\n",
      "tokenizing  10% [==                  ] (5.579s left)\n",
      "tokenizing  20% [====                ] (3.08s left)\n",
      "tokenizing  30% [======              ] (2.1s left)\n",
      "tokenizing  40% [========            ] (1.544s left)\n",
      "tokenizing  50% [==========          ] (1.16s left)\n",
      "tokenizing  60% [============        ] (0.86s left)\n",
      "tokenizing  70% [==============      ] (0.604s left)\n",
      "tokenizing  80% [================    ] (0.385s left)\n",
      "tokenizing  90% [==================  ] (0.185s left)\n",
      "tokenizing 100% [====================] (total duration: 1.8s, mean duration: 0s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [ 24, Excellent, ..., Eater, SF ], [ Teva, Braces, ..., -, Bloomberg ], ..., [ Exelon, must, ..., Baltimore, Sun ], [ Opinion, |, ..., York, Times ] ]\n"
     ]
    }
   ],
   "source": [
    "# tokenize the tiltles since there was no tokenized title in the database\n",
    "titlesToken = []\n",
    "for i in pb(list(range(len(titles))), logger=logger, message=\"tokenizing\"):\n",
    "    titlesToken.append(wordTokenize(titles[i]))\n",
    "bp(titlesToken,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lower casing   0% [                    ]\n",
      "Lower casing  10% [==                  ] (0.09s left)\n",
      "Lower casing  20% [====                ] (0.12s left)\n",
      "Lower casing  30% [======              ] (0.116s left)\n",
      "Lower casing  40% [========            ] (0.105s left)\n",
      "Lower casing  50% [==========          ] (0.09s left)\n",
      "Lower casing  60% [============        ] (0.066s left)\n",
      "Lower casing  70% [==============      ] (0.051s left)\n",
      "Lower casing  80% [================    ] (0.035s left)\n",
      "Lower casing  90% [==================  ] (0.017s left)\n",
      "Lower casing 100% [====================] (total duration: 0.17s, mean duration: 0s)\n",
      "[ [ 24, excellent, ..., eater, sf ], [ teva, braces, ..., -, bloomberg ], ..., [ exelon, must, ..., baltimore, sun ], [ opinion, |, ..., york, times ] ]\n"
     ]
    }
   ],
   "source": [
    "for i in pb(list(range(len(titlesToken))), logger=logger, message=\"Lower casing\"):\n",
    "    if titlesToken[i] == None:\n",
    "        #print(titlesToken[i])  # just to see how many urls have empty title\n",
    "        continue\n",
    "    else:\n",
    "        for u in range(len(titlesToken[i])):\n",
    "            titlesToken[i][u] = titlesToken[i][u].lower()\n",
    "bp(titlesToken, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatization   0% [                    ]\n",
      "Lemmatization  10% [==                  ] (9.27s left)\n",
      "Lemmatization  20% [====                ] (4.36s left)\n",
      "Lemmatization  30% [======              ] (2.66s left)\n",
      "Lemmatization  40% [========            ] (1.799s left)\n",
      "Lemmatization  50% [==========          ] (1.25s left)\n",
      "Lemmatization  60% [============        ] (0.866s left)\n",
      "Lemmatization  70% [==============      ] (0.582s left)\n",
      "Lemmatization  80% [================    ] (0.352s left)\n",
      "Lemmatization  90% [==================  ] (0.162s left)\n",
      "[ [ 24, excellent, ..., eater, sf ], [ teva, brace, ..., -, bloomberg ], ..., [ exelon, must, ..., baltimore, sun ], [ opinion, |, ..., york, time ] ]\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pbar = ProgressBar(len(titlesToken), logger=logger, message=\"Lemmatization\")\n",
    "for i in range(len(titlesToken)):\n",
    "    if titlesToken[i] == None:\n",
    "        # print(titlesToken[i])  # same as last one\n",
    "        continue\n",
    "    else:\n",
    "        for u in range(len(titlesToken[i])):\n",
    "            titlesToken[i][u] = lemmatizer.lemmatize(titlesToken[i][u])\n",
    "    pbar.tic()\n",
    "bp(titlesToken, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\n",
      "  http://247wallst.com/special-report/2017/12/08/americas-25-dying-industries-2: [ america, 's, ..., wall, st. ],\n",
      "  http://247wallst.com/special-report/2018/01/11/25-best-fitness-tech-gadgets/: [ 25, best, ..., wall, st. ],\n",
      "  http://a.msn.com/00/en-us/AAuyJLc?ocid=st: [ what, living, ..., motley, fool ],\n",
      "  http://a.msn.com/00/en-us/BBHoy0Q?ocid=st: [ the, world, ..., in, 2017 ],\n",
      "  http://a.msn.com/01/en-ie/BBHzIAS?ocid=st: [ rare, supermoon, ..., see, it ],\n",
      "  ...,\n",
      "  https://www.zerohedge.com/news/2018-01-08/credit-card-debt-hits-all-time-high-consumers-unleash-historic-shopping-spree: [ credit, card, ..., zero, hedge ],\n",
      "  https://www.zerohedge.com/news/2018-01-08/grand-jury-empaneled-10-million-fraud-probe-involving-jane-and-bernie-sanders: [ grand, jury, ..., zero, hedge ],\n",
      "  https://www.zerohedge.com/news/2018-01-08/its-not-his-job-illinois-treasurer-plays-activist-college-savers-cash: [ \", it, ..., zero, hedge ],\n",
      "  https://yaledailynews.com/blog/2018/01/06/yale-psychiatrist-met-with-congressmen-about-trumps-mental-health/: [ yale, psychiatrist, ..., mental, health ],\n",
      "  https://zdubbzattmom.wordpress.com/2018/01/03/ex-libris-the-eyes-of-madness-presents-2017s-bakers-dozen-zakks-favorite-reads-of-the-year/r: [ rating, and, ..., of, madness ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# build a dict with url as key, and tokenized title as value\n",
    "urlTitles= dict()\n",
    "for i in range(len(titlesToken)):\n",
    "    urlTitles[newsList[i]] = titlesToken[i]\n",
    "bp(urlTitles, 2, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ !, #, ..., •, … ]\n"
     ]
    }
   ],
   "source": [
    "# You need to change the file path, I put it under ../Twinews/twinews/yfnotebooks/title_vocab.txt\n",
    "# to filter out the words not in the vacab to shrink down the size of the query\n",
    "titleVocab = []\n",
    "with open('/home/yuting/PycharmProjects/data/title_vocab.txt','r') as file:\n",
    "    for line in file.readlines():\n",
    "        titleVocab.append(line.strip('\\n'))\n",
    "\n",
    "bp(titleVocab,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainUsers.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build user history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build user history  10% [==                  ] (0.36s left)\n",
      "build user history  20% [====                ] (0.319s left)\n",
      "build user history  30% [======              ] (0.303s left)\n",
      "build user history  40% [========            ] (0.255s left)\n",
      "build user history  50% [==========          ] (0.21s left)\n",
      "build user history  60% [============        ] (0.166s left)\n",
      "build user history  70% [==============      ] (0.128s left)\n",
      "build user history  80% [================    ] (0.084s left)\n",
      "build user history  90% [==================  ] (0.042s left)\n",
      "build user history 100% [====================] (total duration: 0.42s, mean duration: 0.041s)\n"
     ]
    }
   ],
   "source": [
    "# for multiple users\n",
    "trainUserQuery = dict()\n",
    "pbar = ProgressBar(len(trainUsers.keys()), logger=logger, message=\"build user history\")\n",
    "\n",
    "# for each user\n",
    "for usr in trainUsers.keys():\n",
    "    queryFinal = []\n",
    "    \n",
    "    # choose 15 urls in their history to form the query text\n",
    "    if len(list(trainUsers[usr])) < 15:\n",
    "        query = list(trainUsers[usr])\n",
    "    else:\n",
    "        query = random.sample(list(trainUsers[usr]),15)\n",
    "        \n",
    "    # put 15 titles together, using extend\n",
    "    temp = []\n",
    "    for url in query:\n",
    "        if urlTitles[url] == None:\n",
    "            continue\n",
    "        else:\n",
    "            temp.extend(urlTitles[url])\n",
    "            \n",
    "    # the following two steps are making the query titles more simplified and efficient        \n",
    "    # filter what is not in the vocab\n",
    "    tempCompact = []\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i] in titleVocab:\n",
    "            tempCompact.append(temp[i])\n",
    "    \n",
    "    # remove duplicated & detokenize\n",
    "    queryCompact = detokenize(list(set(tempCompact)))\n",
    "    \n",
    "    queryFinal.append(queryCompact)\n",
    "    trainUserQuery[usr] = queryFinal\n",
    "    \n",
    "    pbar.tic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not sure whether you can use this checkpoint file\n",
    "def computeSimilarity(p,h,y):\n",
    "    \n",
    "    tf.reset_default_graph() \n",
    "    model = Graph()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # restore the trianed model\n",
    "    with tf.Session()as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, '/home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_49.ckpt')\n",
    "        loss, acc, prediction = sess.run([model.loss, model.acc, model.prediction],\n",
    "                             feed_dict={model.p: p,\n",
    "                                        model.h: h,\n",
    "                                        model.y: y,\n",
    "                                        model.keep_prob: 1})\n",
    "\n",
    "        #print('loss: ', loss, ' acc:', acc)\n",
    "        return prediction\n",
    "#logit = computeSimilarity(p,h,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rankings of the candidates with logits(second last uotput) results\n",
    "pbar = ProgressBar(len(trainUsers.keys()), logger=logger, message=\"testing\")\n",
    "rankings = dict()\n",
    "for usr in trainUsers.keys():\n",
    "    usrRankings = []\n",
    "    \n",
    "    # get historical data as query(p)\n",
    "    query = trainUserQuery[usr] * 1000\n",
    "    \n",
    "    for candidates in evalData['candidates'][usr]:\n",
    "        candidates = list(candidates)\n",
    "        # get 1000 h as a list(h)\n",
    "        h_temp = []\n",
    "        for url in candidates:\n",
    "            h_temp.append(detokenize(urlTitles[url]))\n",
    "        # get p,h\n",
    "        p,h = char_index(query,h_temp)\n",
    "        y = np.zeros((1000))                    # whatever initialization\n",
    "        \n",
    "        scoresCandidates = []\n",
    "        logit = computeSimilarity(p,h,y)            # get the 1000 scores list\n",
    "        scoresCandidates = zip(candidates,list(logit[:,1]))\n",
    "        ranking = sortBy(scoresCandidates, index=1, desc=True)\n",
    "        ranking = [e[0] for e in ranking]\n",
    "        usrRankings.append(ranking)\n",
    "    rankings[usr] = usrRankings\n",
    "    \n",
    "    pbar.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/graph.py:38: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /home/yuting/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/graph.py:45: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/graph.py:53: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/graph.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/graph.py:27: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/graph.py:65: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_49.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  10% [==                  ] (5m 16.079s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_49.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  20% [====                ] (4m 47.48s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_49.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  30% [======              ] (4m 19.839s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_49.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  40% [========            ] (3m 44.954s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_49.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  50% [==========          ] (3m 8.659s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_49.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  60% [============        ] (2m 32.3s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_49.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  70% [==============      ] (1m 54.895s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_49.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  80% [================    ] (1m 16.802s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_49.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  90% [==================  ] (38.579s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_49.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing 100% [====================] (total duration: 6m 27.829s, mean duration: 38.783s)\n"
     ]
    }
   ],
   "source": [
    "# rankings of the candidates with 0/1 classification results\n",
    "pbar = ProgressBar(len(trainUsers.keys()), logger=logger, message=\"testing\")\n",
    "rankings = dict()\n",
    "for usr in trainUsers.keys():\n",
    "    usrRankings = []\n",
    "    \n",
    "    # get historical data as query(p)\n",
    "    query = trainUserQuery[usr] * 1000\n",
    "    \n",
    "    for candidates in evalData['candidates'][usr]:\n",
    "        candidates = list(candidates)\n",
    "        # get 1000 h as a list(h)\n",
    "        h_temp = []\n",
    "        for url in candidates:\n",
    "            h_temp.append(detokenize(urlTitles[url]))\n",
    "        # get p,h\n",
    "        p,h = char_index(query,h_temp)\n",
    "        y = np.zeros((1000))                    # whatever initialization\n",
    "        \n",
    "        scoresCandidates = []\n",
    "        prediction = computeSimilarity(p,h,y)            # get the 1000 scores list\n",
    "        scoresCandidates = zip(candidates,prediction)\n",
    "        ranking = sortBy(scoresCandidates, index=1, desc=True)\n",
    "        ranking = [e[0] for e in ranking]\n",
    "        usrRankings.append(ranking)\n",
    "    rankings[usr] = usrRankings\n",
    "    \n",
    "    pbar.tic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add the ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the ranking before adding to the db for evaluation\n",
    "checkRankings(rankings,evalData['candidates'],maxUsers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the ranking into de DB\n",
    "addRanking(modelName, rankings, config, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
