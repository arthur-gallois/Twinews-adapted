{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from databasetools.mongo import *\n",
    "from newstools.goodarticle.utils import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.news import parser as newsParser\n",
    "from machinelearning.iterator import *\n",
    "from twinews.utils import *\n",
    "from twinews.models.ranking import *\n",
    "from twinews.evaluation.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptools.basics import *\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "import gensim\n",
    "from math import log2\n",
    "from math import sqrt\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/yuting/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from twinews.yfnotebooks.load_data import load_char_data,char_index  # load data tools\n",
    "from twinews.yfnotebooks.convnet import args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()\n",
    "TEST = isNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tictoc starts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/Convnet.log\") if isNotebook else Logger(\"Convnet-\" + getHostname() + \".log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "modelName = \"Convnet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = \\\n",
    "{\n",
    "    'splitVersion': 2,\n",
    "    'testVersion':2, # with some parameters changed    \n",
    "    \n",
    "    # for input data\n",
    "    'maxUsers': 5 if TEST else None, # Sub-sampling\n",
    "    'maxDocuments': None,\n",
    "    'useExtraNews': 0 if TEST else None, # None = unlimited, 0 = no extra news\n",
    "    'minDF': 1 / 2000, # Remove words that have a document frequency ratio lower than 1 / 2000\n",
    "    'maxDF': 20, # Remove top 20 voc elements\n",
    "    'lowercase': True,\n",
    "    'doLemmatization': True,\n",
    "    \n",
    "\n",
    "    \n",
    "    # for ouput\n",
    "    'similarity': 'cosine',\n",
    "\n",
    "#     'historyRef': 0.3, #Â 1, 1.0, 0.5, 0.3, 3, 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*-coding:utf-8-*-\n",
    "# defining the training graph\n",
    "\n",
    "class Graph:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.p = tf.placeholder(dtype=tf.int32, shape=(None, args.seq_length), name='p')\n",
    "        self.h = tf.placeholder(dtype=tf.int32, shape=(None, args.seq_length), name='h')\n",
    "        self.y = tf.placeholder(dtype=tf.int32, shape=None, name='y')\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32, name='drop_rate')\n",
    "\n",
    "        self.embedding = tf.get_variable(dtype=tf.float32, shape=(args.vocab_size, args.char_embedding_size),\n",
    "                                         name='embedding')\n",
    "        self.M = tf.Variable(tf.random_normal(shape=(args.n_filter, args.n_filter), mean=0, stddev=1),\n",
    "                             name='M',\n",
    "                             dtype=tf.float32)\n",
    "        self.x_feat = tf.Variable(tf.random_normal(shape=(args.batch_size, args.n_filter), mean=0, stddev=1),\n",
    "                                  name='x_feat',\n",
    "                                  dtype=tf.float32)\n",
    "\n",
    "        self.forward()\n",
    "\n",
    "    def dropout(self, x):\n",
    "        return tf.nn.dropout(x, keep_prob=self.keep_prob)\n",
    "\n",
    "    def forward(self):\n",
    "        p_embedding = tf.nn.embedding_lookup(self.embedding, self.p)\n",
    "        h_embedding = tf.nn.embedding_lookup(self.embedding, self.h)\n",
    "\n",
    "        p_embedding = tf.expand_dims(p_embedding, axis=3)\n",
    "        h_embedding = tf.expand_dims(h_embedding, axis=3)\n",
    "        p = tf.layers.conv2d(p_embedding,\n",
    "                             filters=args.n_filter,\n",
    "                             kernel_size=(args.filter_width, args.filter_height),\n",
    "                             activation='relu')\n",
    "        h = tf.layers.conv2d(h_embedding,\n",
    "                             filters=args.n_filter,\n",
    "                             kernel_size=(args.filter_width, args.filter_height),\n",
    "                             activation='relu')\n",
    "\n",
    "        pool_width = args.seq_length + 1 - args.filter_width\n",
    "        p_max = tf.layers.max_pooling2d(p, pool_size=(pool_width, 1), strides=1)\n",
    "        h_max = tf.layers.max_pooling2d(h, pool_size=(pool_width, 1), strides=1)\n",
    "\n",
    "        p_max = tf.squeeze(tf.squeeze(p_max, axis=1), axis=1)\n",
    "        h_max = tf.squeeze(tf.squeeze(h_max, axis=1), axis=1)\n",
    "\n",
    "        # sim = tf.matmul(tf.matmul(p_max, self.M), tf.transpose(h_max))\n",
    "        sim = tf.matmul(p_max, self.M)\n",
    "        sim = tf.reduce_sum(tf.multiply(sim, h_max), axis=1, keep_dims=True)\n",
    "\n",
    "        x = tf.concat((p_max, h_max, sim), axis=1)\n",
    "\n",
    "        v = tf.layers.dense(x, 256, activation='relu')\n",
    "        v = self.dropout(v)\n",
    "        logits = tf.layers.dense(v, 2, activation='relu')\n",
    "\n",
    "        self.train(logits)\n",
    "\n",
    "    def train(self, logits):\n",
    "        y = tf.one_hot(self.y, args.class_size)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "        self.loss = tf.reduce_mean(loss)\n",
    "        self.train_op = tf.train.AdamOptimizer(args.learning_rate).minimize(self.loss)\n",
    "        # use the logits output(second last) for ranking\n",
    "        self.logits = logits\n",
    "        prediction = tf.argmax(logits, axis=1)\n",
    "        # use the 0/1 prediction for ranking\n",
    "        self.prediction = prediction\n",
    "        correct_prediction = tf.equal(tf.cast(prediction, tf.int32), self.y)\n",
    "        self.acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  step: 0  loss:  0.69322234  acc: 0.4765625\n",
      "epoch: 0  step: 1  loss:  0.6932197  acc: 0.4970703\n",
      "epoch: 0  step: 2  loss:  0.69299364  acc: 0.5126953\n",
      "epoch: 0  step: 3  loss:  0.6931623  acc: 0.4951172\n",
      "epoch: 0  step: 4  loss:  0.6929022  acc: 0.51171875\n",
      "epoch: 0  step: 5  loss:  0.6932014  acc: 0.4736328\n",
      "epoch: 0  step: 6  loss:  0.69260746  acc: 0.5332031\n",
      "epoch: 0  step: 7  loss:  0.6925728  acc: 0.51464844\n",
      "epoch: 0  step: 8  loss:  0.69272065  acc: 0.4873047\n",
      "epoch: 0  step: 9  loss:  0.69246507  acc: 0.49121094\n",
      "epoch: 0  step: 10  loss:  0.6926708  acc: 0.4794922\n",
      "epoch: 0  step: 11  loss:  0.6919373  acc: 0.50683594\n",
      "epoch: 0  step: 12  loss:  0.69185096  acc: 0.5107422\n",
      "epoch: 0  step: 13  loss:  0.69146144  acc: 0.5078125\n",
      "epoch: 0  step: 14  loss:  0.6918249  acc: 0.48535156\n",
      "epoch: 0  step: 15  loss:  0.691381  acc: 0.49804688\n",
      "epoch: 0  step: 16  loss:  0.69045544  acc: 0.5058594\n",
      "epoch: 0  step: 17  loss:  0.6903516  acc: 0.49316406\n",
      "epoch: 0  step: 18  loss:  0.6904422  acc: 0.4892578\n",
      "loss_eval:  0.68888754  acc_eval: 0.4951172\n",
      "\n",
      "\n",
      "epoch: 1  step: 0  loss:  0.68912756  acc: 0.50453174\n",
      "epoch: 1  step: 1  loss:  0.688267  acc: 0.4970703\n",
      "epoch: 1  step: 2  loss:  0.6871364  acc: 0.5029297\n",
      "epoch: 1  step: 3  loss:  0.6860424  acc: 0.515625\n",
      "epoch: 1  step: 4  loss:  0.68586683  acc: 0.4970703\n",
      "epoch: 1  step: 5  loss:  0.68442523  acc: 0.515625\n",
      "epoch: 1  step: 6  loss:  0.68496156  acc: 0.484375\n",
      "epoch: 1  step: 7  loss:  0.680667  acc: 0.54003906\n",
      "epoch: 1  step: 8  loss:  0.67932385  acc: 0.53027344\n",
      "epoch: 1  step: 9  loss:  0.680043  acc: 0.5263672\n",
      "epoch: 1  step: 10  loss:  0.6779652  acc: 0.54296875\n",
      "epoch: 1  step: 11  loss:  0.6768141  acc: 0.56347656\n",
      "epoch: 1  step: 12  loss:  0.6716663  acc: 0.6425781\n",
      "epoch: 1  step: 13  loss:  0.66920733  acc: 0.6689453\n",
      "epoch: 1  step: 14  loss:  0.6648351  acc: 0.7207031\n",
      "epoch: 1  step: 15  loss:  0.6640196  acc: 0.7207031\n",
      "epoch: 1  step: 16  loss:  0.6576532  acc: 0.75390625\n",
      "epoch: 1  step: 17  loss:  0.65268666  acc: 0.7998047\n",
      "epoch: 1  step: 18  loss:  0.6492913  acc: 0.7949219\n",
      "loss_eval:  0.6407641  acc_eval: 0.7988281\n",
      "\n",
      "\n",
      "epoch: 2  step: 0  loss:  0.6430284  acc: 0.796875\n",
      "epoch: 2  step: 1  loss:  0.63658375  acc: 0.8021148\n",
      "epoch: 2  step: 2  loss:  0.6309886  acc: 0.8125\n",
      "epoch: 2  step: 3  loss:  0.622148  acc: 0.7949219\n",
      "epoch: 2  step: 4  loss:  0.613649  acc: 0.8183594\n",
      "epoch: 2  step: 5  loss:  0.60707366  acc: 0.8330078\n",
      "epoch: 2  step: 6  loss:  0.5988146  acc: 0.81640625\n",
      "epoch: 2  step: 7  loss:  0.59260917  acc: 0.81933594\n",
      "epoch: 2  step: 8  loss:  0.57867694  acc: 0.8300781\n",
      "epoch: 2  step: 9  loss:  0.56130266  acc: 0.86035156\n",
      "epoch: 2  step: 10  loss:  0.5600934  acc: 0.84765625\n",
      "epoch: 2  step: 11  loss:  0.5459021  acc: 0.85253906\n",
      "epoch: 2  step: 12  loss:  0.53442705  acc: 0.8457031\n",
      "epoch: 2  step: 13  loss:  0.51757574  acc: 0.8515625\n",
      "epoch: 2  step: 14  loss:  0.5088759  acc: 0.859375\n",
      "epoch: 2  step: 15  loss:  0.49407354  acc: 0.85546875\n",
      "epoch: 2  step: 16  loss:  0.48587915  acc: 0.8388672\n",
      "epoch: 2  step: 17  loss:  0.46360403  acc: 0.86328125\n",
      "epoch: 2  step: 18  loss:  0.4424301  acc: 0.8847656\n",
      "loss_eval:  0.42568523  acc_eval: 0.8652344\n",
      "\n",
      "\n",
      "epoch: 3  step: 0  loss:  0.4375874  acc: 0.86621094\n",
      "epoch: 3  step: 1  loss:  0.4140662  acc: 0.87402344\n",
      "epoch: 3  step: 2  loss:  0.39954686  acc: 0.8761329\n",
      "epoch: 3  step: 3  loss:  0.38597786  acc: 0.8779297\n",
      "epoch: 3  step: 4  loss:  0.3641382  acc: 0.8808594\n",
      "epoch: 3  step: 5  loss:  0.35453016  acc: 0.8701172\n",
      "epoch: 3  step: 6  loss:  0.33920255  acc: 0.8798828\n",
      "epoch: 3  step: 7  loss:  0.33513615  acc: 0.8779297\n",
      "epoch: 3  step: 8  loss:  0.3213001  acc: 0.88183594\n",
      "epoch: 3  step: 9  loss:  0.32363877  acc: 0.87109375\n",
      "epoch: 3  step: 10  loss:  0.2863247  acc: 0.89160156\n",
      "epoch: 3  step: 11  loss:  0.29196292  acc: 0.8808594\n",
      "epoch: 3  step: 12  loss:  0.28674307  acc: 0.8798828\n",
      "epoch: 3  step: 13  loss:  0.27814728  acc: 0.87890625\n",
      "epoch: 3  step: 14  loss:  0.27669844  acc: 0.88671875\n",
      "epoch: 3  step: 15  loss:  0.27658838  acc: 0.8798828\n",
      "epoch: 3  step: 16  loss:  0.28296983  acc: 0.875\n",
      "epoch: 3  step: 17  loss:  0.29664707  acc: 0.8642578\n",
      "epoch: 3  step: 18  loss:  0.27896902  acc: 0.8808594\n",
      "loss_eval:  0.2553039  acc_eval: 0.8935547\n",
      "\n",
      "\n",
      "epoch: 4  step: 0  loss:  0.24581979  acc: 0.90625\n",
      "epoch: 4  step: 1  loss:  0.26889214  acc: 0.89160156\n",
      "epoch: 4  step: 2  loss:  0.23347898  acc: 0.9033203\n",
      "epoch: 4  step: 3  loss:  0.23120832  acc: 0.90483385\n",
      "epoch: 4  step: 4  loss:  0.25348836  acc: 0.9003906\n",
      "epoch: 4  step: 5  loss:  0.22146596  acc: 0.90722656\n",
      "epoch: 4  step: 6  loss:  0.23724768  acc: 0.89160156\n",
      "epoch: 4  step: 7  loss:  0.2282201  acc: 0.91308594\n",
      "epoch: 4  step: 8  loss:  0.23452479  acc: 0.9082031\n",
      "epoch: 4  step: 9  loss:  0.23898554  acc: 0.9042969\n",
      "epoch: 4  step: 10  loss:  0.25020242  acc: 0.89453125\n",
      "epoch: 4  step: 11  loss:  0.204458  acc: 0.92089844\n",
      "epoch: 4  step: 12  loss:  0.22464678  acc: 0.90527344\n",
      "epoch: 4  step: 13  loss:  0.22850402  acc: 0.90722656\n",
      "epoch: 4  step: 14  loss:  0.20415014  acc: 0.9189453\n",
      "epoch: 4  step: 15  loss:  0.22213332  acc: 0.91503906\n",
      "epoch: 4  step: 16  loss:  0.21561292  acc: 0.91308594\n",
      "epoch: 4  step: 17  loss:  0.21440805  acc: 0.9121094\n",
      "epoch: 4  step: 18  loss:  0.24694696  acc: 0.89453125\n",
      "loss_eval:  0.20482992  acc_eval: 0.93066406\n",
      "\n",
      "\n",
      "epoch: 5  step: 0  loss:  0.23648164  acc: 0.9033203\n",
      "epoch: 5  step: 1  loss:  0.19830742  acc: 0.9345703\n",
      "epoch: 5  step: 2  loss:  0.21582288  acc: 0.9169922\n",
      "epoch: 5  step: 3  loss:  0.18970856  acc: 0.92578125\n",
      "epoch: 5  step: 4  loss:  0.17749402  acc: 0.9410876\n",
      "epoch: 5  step: 5  loss:  0.21358685  acc: 0.9189453\n",
      "epoch: 5  step: 6  loss:  0.17759098  acc: 0.93359375\n",
      "epoch: 5  step: 7  loss:  0.18511209  acc: 0.9345703\n",
      "epoch: 5  step: 8  loss:  0.18777215  acc: 0.93066406\n",
      "epoch: 5  step: 9  loss:  0.1938356  acc: 0.9277344\n",
      "epoch: 5  step: 10  loss:  0.19739664  acc: 0.9277344\n",
      "epoch: 5  step: 11  loss:  0.20333156  acc: 0.9199219\n",
      "epoch: 5  step: 12  loss:  0.17126878  acc: 0.93066406\n",
      "epoch: 5  step: 13  loss:  0.18480292  acc: 0.92578125\n",
      "epoch: 5  step: 14  loss:  0.1864018  acc: 0.9355469\n",
      "epoch: 5  step: 15  loss:  0.16631699  acc: 0.9423828\n",
      "epoch: 5  step: 16  loss:  0.1814734  acc: 0.9326172\n",
      "epoch: 5  step: 17  loss:  0.18338662  acc: 0.92871094\n",
      "epoch: 5  step: 18  loss:  0.18030378  acc: 0.9326172\n",
      "loss_eval:  0.1711726  acc_eval: 0.9501953\n",
      "\n",
      "\n",
      "epoch: 6  step: 0  loss:  0.19011241  acc: 0.92871094\n",
      "epoch: 6  step: 1  loss:  0.2027229  acc: 0.92578125\n",
      "epoch: 6  step: 2  loss:  0.17206103  acc: 0.9482422\n",
      "epoch: 6  step: 3  loss:  0.17911722  acc: 0.94140625\n",
      "epoch: 6  step: 4  loss:  0.1628088  acc: 0.93652344\n",
      "epoch: 6  step: 5  loss:  0.14301181  acc: 0.9592145\n",
      "epoch: 6  step: 6  loss:  0.17283206  acc: 0.93847656\n",
      "epoch: 6  step: 7  loss:  0.15698442  acc: 0.9404297\n",
      "epoch: 6  step: 8  loss:  0.15414628  acc: 0.94628906\n",
      "epoch: 6  step: 9  loss:  0.15562846  acc: 0.94433594\n",
      "epoch: 6  step: 10  loss:  0.15016481  acc: 0.953125\n",
      "epoch: 6  step: 11  loss:  0.1574515  acc: 0.95214844\n",
      "epoch: 6  step: 12  loss:  0.1657868  acc: 0.9423828\n",
      "epoch: 6  step: 13  loss:  0.13739808  acc: 0.94628906\n",
      "epoch: 6  step: 14  loss:  0.15648372  acc: 0.9345703\n",
      "epoch: 6  step: 15  loss:  0.152874  acc: 0.9482422\n",
      "epoch: 6  step: 16  loss:  0.13288349  acc: 0.9580078\n",
      "epoch: 6  step: 17  loss:  0.14807317  acc: 0.94628906\n",
      "epoch: 6  step: 18  loss:  0.14869528  acc: 0.94921875\n",
      "loss_eval:  0.14782482  acc_eval: 0.9511719\n",
      "\n",
      "\n",
      "epoch: 7  step: 0  loss:  0.1615236  acc: 0.9472656\n",
      "epoch: 7  step: 1  loss:  0.15276414  acc: 0.9501953\n",
      "epoch: 7  step: 2  loss:  0.1567527  acc: 0.9404297\n",
      "epoch: 7  step: 3  loss:  0.14283815  acc: 0.94921875\n",
      "epoch: 7  step: 4  loss:  0.15173575  acc: 0.9501953\n",
      "epoch: 7  step: 5  loss:  0.13211274  acc: 0.9560547\n",
      "epoch: 7  step: 6  loss:  0.10890219  acc: 0.96827793\n",
      "epoch: 7  step: 7  loss:  0.14393234  acc: 0.9472656\n",
      "epoch: 7  step: 8  loss:  0.13015091  acc: 0.95996094\n",
      "epoch: 7  step: 9  loss:  0.122183785  acc: 0.95703125\n",
      "epoch: 7  step: 10  loss:  0.12807962  acc: 0.9589844\n",
      "epoch: 7  step: 11  loss:  0.12136482  acc: 0.9667969\n",
      "epoch: 7  step: 12  loss:  0.13098992  acc: 0.96191406\n",
      "epoch: 7  step: 13  loss:  0.14835495  acc: 0.9482422\n",
      "epoch: 7  step: 14  loss:  0.113874055  acc: 0.95410156\n",
      "epoch: 7  step: 15  loss:  0.12537007  acc: 0.9511719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7  step: 16  loss:  0.12527652  acc: 0.9609375\n",
      "epoch: 7  step: 17  loss:  0.115872696  acc: 0.95996094\n",
      "epoch: 7  step: 18  loss:  0.11709474  acc: 0.95703125\n",
      "loss_eval:  0.122704856  acc_eval: 0.9658203\n",
      "\n",
      "\n",
      "epoch: 8  step: 0  loss:  0.11258467  acc: 0.96972656\n",
      "epoch: 8  step: 1  loss:  0.1276662  acc: 0.9560547\n",
      "epoch: 8  step: 2  loss:  0.13018908  acc: 0.9638672\n",
      "epoch: 8  step: 3  loss:  0.128878  acc: 0.95703125\n",
      "epoch: 8  step: 4  loss:  0.12023044  acc: 0.9589844\n",
      "epoch: 8  step: 5  loss:  0.1286129  acc: 0.96191406\n",
      "epoch: 8  step: 6  loss:  0.1048397  acc: 0.9658203\n",
      "epoch: 8  step: 7  loss:  0.086310655  acc: 0.9758308\n",
      "epoch: 8  step: 8  loss:  0.121877454  acc: 0.95703125\n",
      "epoch: 8  step: 9  loss:  0.10741159  acc: 0.9658203\n",
      "epoch: 8  step: 10  loss:  0.09866793  acc: 0.9658203\n",
      "epoch: 8  step: 11  loss:  0.109577864  acc: 0.9658203\n",
      "epoch: 8  step: 12  loss:  0.10319106  acc: 0.96972656\n",
      "epoch: 8  step: 13  loss:  0.107962176  acc: 0.9716797\n",
      "epoch: 8  step: 14  loss:  0.12253089  acc: 0.9550781\n",
      "epoch: 8  step: 15  loss:  0.10141243  acc: 0.96484375\n",
      "epoch: 8  step: 16  loss:  0.10225292  acc: 0.9707031\n",
      "epoch: 8  step: 17  loss:  0.101404846  acc: 0.97265625\n",
      "epoch: 8  step: 18  loss:  0.09382424  acc: 0.96972656\n",
      "loss_eval:  0.1045376  acc_eval: 0.9736328\n",
      "\n",
      "\n",
      "epoch: 9  step: 0  loss:  0.09516135  acc: 0.9707031\n",
      "epoch: 9  step: 1  loss:  0.088394865  acc: 0.97558594\n",
      "epoch: 9  step: 2  loss:  0.09749277  acc: 0.9736328\n",
      "epoch: 9  step: 3  loss:  0.10979721  acc: 0.96875\n",
      "epoch: 9  step: 4  loss:  0.10617316  acc: 0.96777344\n",
      "epoch: 9  step: 5  loss:  0.0997743  acc: 0.9707031\n",
      "epoch: 9  step: 6  loss:  0.11443639  acc: 0.9658203\n",
      "epoch: 9  step: 7  loss:  0.08899631  acc: 0.97265625\n",
      "epoch: 9  step: 8  loss:  0.07116139  acc: 0.9848943\n",
      "epoch: 9  step: 9  loss:  0.09785171  acc: 0.9628906\n",
      "epoch: 9  step: 10  loss:  0.08626786  acc: 0.98046875\n",
      "epoch: 9  step: 11  loss:  0.0750708  acc: 0.97753906\n",
      "epoch: 9  step: 12  loss:  0.09755865  acc: 0.9736328\n",
      "epoch: 9  step: 13  loss:  0.08462872  acc: 0.97558594\n",
      "epoch: 9  step: 14  loss:  0.08779977  acc: 0.97753906\n",
      "epoch: 9  step: 15  loss:  0.09566317  acc: 0.97265625\n",
      "epoch: 9  step: 16  loss:  0.080425516  acc: 0.97265625\n",
      "epoch: 9  step: 17  loss:  0.08848181  acc: 0.97558594\n",
      "epoch: 9  step: 18  loss:  0.08692905  acc: 0.9765625\n",
      "loss_eval:  0.09439573  acc_eval: 0.9765625\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "#from twinews.yfnotebooks.convnet import args\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "p, h, y = load_char_data('/home/yuting/PycharmProjects/data/dssm_title_train.csv', data_size=None)\n",
    "p_eval, h_eval, y_eval = load_char_data('/home/yuting/PycharmProjects/data/dssm_title_dev.csv', data_size=args.batch_size)\n",
    "\n",
    "p_holder = tf.placeholder(dtype=tf.int32, shape=(None, args.seq_length), name='p')\n",
    "h_holder = tf.placeholder(dtype=tf.int32, shape=(None, args.seq_length), name='h')\n",
    "y_holder = tf.placeholder(dtype=tf.int32, shape=None, name='y')\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((p_holder, h_holder, y_holder))\n",
    "dataset = dataset.batch(args.batch_size).repeat(args.epochs)\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "model = Graph()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "\n",
    "with tf.Session(config=config)as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(iterator.initializer, feed_dict={p_holder: p, h_holder: h, y_holder: y})\n",
    "    steps = int(len(y) / args.batch_size)\n",
    "    for epoch in range(args.epochs):\n",
    "        for step in range(steps):\n",
    "            p_batch, h_batch, y_batch = sess.run(next_element)\n",
    "            _, loss, acc = sess.run([model.train_op, model.loss, model.acc],\n",
    "                                    feed_dict={model.p: p_batch,\n",
    "                                               model.h: h_batch,\n",
    "                                               model.y: y_batch,\n",
    "                                               model.keep_prob: args.keep_prob})\n",
    "            print('epoch:', epoch, ' step:', step, ' loss: ', loss, ' acc:', acc)\n",
    "\n",
    "        loss_eval, acc_eval = sess.run([model.loss, model.acc],\n",
    "                                       feed_dict={model.p: p_eval,\n",
    "                                                  model.h: h_eval,\n",
    "                                                  model.y: y_eval,\n",
    "                                                  model.keep_prob: 1})\n",
    "        print('loss_eval: ', loss_eval, ' acc_eval:', acc_eval)\n",
    "        print('\\n')\n",
    "        saver.save(sess, f'/home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_{epoch}.ckpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> tic: 19.26s | message: Eval data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{ candidates, extraNews, meta, testNews, testUsers, trainNews, trainUsers }\n",
      "{ 'created': 2020.03.24-14.28.06, 'endDate': 2018-01-15, 'id': 2, 'ranksLength': 1000, 'splitDate': 2017-12-25, 'startDate': 2017-10-01, 'testMaxNewsPerUser': 97, 'testMeanNewsPerUser': 7.22, 'testMinNewsPerUser': 2, 'testNewsCount': 71781, 'totalNewsAvailable': 570210, 'trainMaxNewsPerUser': 379, 'trainMeanNewsPerUser': 26.48, 'trainMinNewsPerUser': 8, 'trainNewsCount': 237150, 'usersCount': 15905 }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> toc total duration: 19.48s | message: Got Twinews evaluation data\n"
     ]
    }
   ],
   "source": [
    "# Getting users and news\n",
    "evalData = getEvalData(config['splitVersion'], maxExtraNews=0,\n",
    "                       maxUsers=config['maxUsers'])\n",
    "(trainUsers, testUsers, trainNews, testNews, candidates, extraNews) = \\\n",
    "(evalData['trainUsers'], evalData['testUsers'], evalData['trainNews'],\n",
    " evalData['testNews'], evalData['candidates'], evalData['extraNews'])\n",
    "bp(evalData.keys(), 5, logger)\n",
    "log(b(evalData['meta'], 5), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNewsList = shuffle(list(trainNews), seed=0)\n",
    "testNewsList = shuffle(list(testNews), seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5006 urls for newsList:\n",
      "[\n",
      "  http://www.dailymail.co.uk/news/article-5186005/Trump-removes-climate-change-national-security-polic,\n",
      "  http://www.edweek.org/ew/articles/2017/11/29/when-it-comes-to-sexual-harassment-schools.html?cmp=soc,\n",
      "  ...,\n",
      "  https://patch.com/new-york/new-york-city/rikers-island-visitors-put-through-hell-see-loved-ones-repo,\n",
      "  http://www.tabletmag.com/scroll/196615/four-lessons-in-virtue-from-martin-luther-king-jr\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "newsList = trainNewsList + testNewsList\n",
    "log(str(len(newsList)) + \" urls for newsList:\\n\" + b(newsList), logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twinews news (version 1.0) initialised.\n",
      "  100% [====================] (total duration: 3m 32.419s, mean duration: 0.042s)\n"
     ]
    }
   ],
   "source": [
    "# get all the titles of news urls link to form (url: title) dict\n",
    "titles = getNewsTitles(newsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing   0% [                    ]\n",
      "tokenizing   9% [=                   ] (4.235s left)\n",
      "tokenizing  19% [===                 ] (2.243s left)\n",
      "tokenizing  29% [=====               ] (1.495s left)\n",
      "tokenizing  39% [=======             ] (1.172s left)\n",
      "tokenizing  49% [=========           ] (0.852s left)\n",
      "tokenizing  59% [===========         ] (0.615s left)\n",
      "tokenizing  69% [=============       ] (0.425s left)\n",
      "tokenizing  79% [===============     ] (0.266s left)\n",
      "tokenizing  89% [=================   ] (0.125s left)\n",
      "tokenizing  99% [=================== ] (0.001s left)\n",
      "tokenizing 100% [====================] (total duration: 1.2s, mean duration: 0s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [ Trump, removes, ..., Mail, Online ], [ When, It, ..., Education, Week ], ..., [ Rikers, Island, ..., NY, Patch ], [ Four, Lessons, ..., Tablet, Magazine ] ]\n"
     ]
    }
   ],
   "source": [
    "# tokenize the tiltles since there was no tokenized title in the database\n",
    "titlesToken = []\n",
    "for i in pb(list(range(len(titles))), logger=logger, message=\"tokenizing\"):\n",
    "    titlesToken.append(wordTokenize(titles[i]))\n",
    "bp(titlesToken,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lower casing   0% [                    ]\n",
      "Lower casing   9% [=                   ] (0s left)\n",
      "Lower casing  19% [===                 ] (0.04s left)\n",
      "Lower casing  29% [=====               ] (0.046s left)\n",
      "Lower casing  39% [=======             ] (0.045s left)\n",
      "Lower casing  49% [=========           ] (0.04s left)\n",
      "Lower casing  59% [===========         ] (0.033s left)\n",
      "Lower casing  69% [=============       ] (0.025s left)\n",
      "Lower casing  79% [===============     ] (0.017s left)\n",
      "Lower casing  89% [=================   ] (0.008s left)\n",
      "Lower casing  99% [=================== ] (0s left)\n",
      "Lower casing 100% [====================] (total duration: 0.09s, mean duration: 0s)\n",
      "[ [ trump, removes, ..., mail, online ], [ when, it, ..., education, week ], ..., [ rikers, island, ..., ny, patch ], [ four, lessons, ..., tablet, magazine ] ]\n"
     ]
    }
   ],
   "source": [
    "for i in pb(list(range(len(titlesToken))), logger=logger, message=\"Lower casing\"):\n",
    "    if titlesToken[i] == None:\n",
    "        #print(titlesToken[i])  # just to see how many urls have empty title\n",
    "        continue\n",
    "    else:\n",
    "        for u in range(len(titlesToken[i])):\n",
    "            titlesToken[i][u] = titlesToken[i][u].lower()\n",
    "bp(titlesToken, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatization   0% [                    ]\n",
      "Lemmatization   9% [=                   ] (8.291s left)\n",
      "Lemmatization  19% [===                 ] (3.805s left)\n",
      "Lemmatization  29% [=====               ] (2.29s left)\n",
      "Lemmatization  39% [=======             ] (1.518s left)\n",
      "Lemmatization  49% [=========           ] (1.042s left)\n",
      "Lemmatization  59% [===========         ] (0.715s left)\n",
      "Lemmatization  69% [=============       ] (0.473s left)\n",
      "Lemmatization  79% [===============     ] (0.284s left)\n",
      "Lemmatization  89% [=================   ] (0.13s left)\n",
      "Lemmatization  99% [=================== ] (0.001s left)\n",
      "[ [ trump, remove, ..., mail, online ], [ when, it, ..., education, week ], ..., [ rikers, island, ..., ny, patch ], [ four, lesson, ..., tablet, magazine ] ]\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pbar = ProgressBar(len(titlesToken), logger=logger, message=\"Lemmatization\")\n",
    "for i in range(len(titlesToken)):\n",
    "    if titlesToken[i] == None:\n",
    "        # print(titlesToken[i])  # same as last one\n",
    "        continue\n",
    "    else:\n",
    "        for u in range(len(titlesToken[i])):\n",
    "            titlesToken[i][u] = lemmatizer.lemmatize(titlesToken[i][u])\n",
    "    pbar.tic()\n",
    "bp(titlesToken, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\n",
      "  http://a.msn.com/01/en-ie/BBHzIAS?ocid=st: [ rare, supermoon, ..., see, it ],\n",
      "  http://a.msn.com/01/en-us/AAuFzwx?ocid=st: [ gop, senator, ..., 's, remark ],\n",
      "  http://a.msn.com/01/en-us/BBHs8ye?ocid=st: [ eric, garner, ..., :, report ],\n",
      "  http://a.msn.com/02/en-us/BBI8v4T?ocid=st: [ angry, fan, ..., title, game ],\n",
      "  http://a.msn.com/06/en-us/BBH7EiO?ocid=st: [ this, is, ..., any, restaurant ],\n",
      "  ...,\n",
      "  https://www.zerohedge.com/news/2018-01-04/trump-admin-bans-all-personal-devices-white-house: [ trump, admin, ..., zero, hedge ],\n",
      "  https://www.zerohedge.com/news/2018-01-05/fbi-chief-foia-officer-every-single-memo-comey-leaked-was-classified: [ fbi, chief, ..., zero, hedge ],\n",
      "  https://www.zerohedge.com/news/2018-01-05/new-video-emerges-isis-convoys-leaving-raqqa-under-us-coalition-watch: [ new, video, ..., zero, hedge ],\n",
      "  https://www.zerohedge.com/news/2018-01-08/credit-card-debt-hits-all-time-high-consumers-unleash-historic-shopping-spree: [ credit, card, ..., zero, hedge ],\n",
      "  https://yaledailynews.com/blog/2018/01/06/yale-psychiatrist-met-with-congressmen-about-trumps-mental-health/: [ yale, psychiatrist, ..., mental, health ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# build a dict with url as key, and tokenized title as value\n",
    "urlTitles= dict()\n",
    "for i in range(len(titlesToken)):\n",
    "    urlTitles[newsList[i]] = titlesToken[i]\n",
    "bp(urlTitles, 2, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ !, #, ..., â¢, â¦ ]\n"
     ]
    }
   ],
   "source": [
    "# You need to change the file path, I put it under ../Twinews/twinews/yfnotebooks/title_vocab.txt\n",
    "# to filter out the words not in the vacab to shrink down the size of the query\n",
    "titleVocab = []\n",
    "with open('/home/yuting/PycharmProjects/data/title_vocab.txt','r') as file:\n",
    "    for line in file.readlines():\n",
    "        titleVocab.append(line.strip('\\n'))\n",
    "\n",
    "bp(titleVocab,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainUsers.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build user history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build user history  20% [====                ] (0.159s left)\n",
      "build user history  40% [========            ] (0.134s left)\n",
      "build user history  60% [============        ] (0.086s left)\n",
      "build user history  80% [================    ] (0.042s left)\n",
      "build user history 100% [====================] (total duration: 0.22s, mean duration: 0.044s)\n"
     ]
    }
   ],
   "source": [
    "# for multiple users\n",
    "trainUserQuery = dict()\n",
    "pbar = ProgressBar(len(trainUsers.keys()), logger=logger, message=\"build user history\")\n",
    "\n",
    "# for each user\n",
    "for usr in trainUsers.keys():\n",
    "    queryFinal = []\n",
    "    \n",
    "    # choose 15 urls in their history to form the query text\n",
    "    if len(list(trainUsers[usr])) < 15:\n",
    "        query = list(trainUsers[usr])\n",
    "    else:\n",
    "        query = random.sample(list(trainUsers[usr]),15)\n",
    "        \n",
    "    # put 15 titles together, using extend\n",
    "    temp = []\n",
    "    for url in query:\n",
    "        if urlTitles[url] == None:\n",
    "            continue\n",
    "        else:\n",
    "            temp.extend(urlTitles[url])\n",
    "            \n",
    "    # the following two steps are making the query titles more simplified and efficient        \n",
    "    # filter what is not in the vocab\n",
    "    tempCompact = []\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i] in titleVocab:\n",
    "            tempCompact.append(temp[i])\n",
    "    \n",
    "    # remove duplicated & detokenize\n",
    "    queryCompact = detokenize(list(set(tempCompact)))\n",
    "    \n",
    "    queryFinal.append(queryCompact)\n",
    "    trainUserQuery[usr] = queryFinal\n",
    "    \n",
    "    pbar.tic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not sure whether you can use this checkpoint file\n",
    "def computeSimilarity(p,h,y):\n",
    "    \n",
    "    tf.reset_default_graph() \n",
    "    model = Graph()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # restore the trianed model\n",
    "    with tf.Session()as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, '/home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_9.ckpt')\n",
    "        loss, acc, prediction = sess.run([model.loss, model.acc, model.prediction],\n",
    "                             feed_dict={model.p: p,\n",
    "                                        model.h: h,\n",
    "                                        model.y: y,\n",
    "                                        model.keep_prob: 1})\n",
    "\n",
    "        #print('loss: ', loss, ' acc:', acc)\n",
    "        return prediction\n",
    "#logit = computeSimilarity(p,h,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_9.ckpt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-58abde818f56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mscoresCandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputeSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# get the 1000 scores list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mscoresCandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msortBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoresCandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranking\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# rankings of the candidates with logits(second last uotput) results\n",
    "pbar = ProgressBar(len(trainUsers.keys()), logger=logger, message=\"testing\")\n",
    "rankings = dict()\n",
    "for usr in trainUsers.keys():\n",
    "    usrRankings = []\n",
    "    \n",
    "    # get historical data as query(p)\n",
    "    query = trainUserQuery[usr] * 1000\n",
    "    \n",
    "    for candidates in evalData['candidates'][usr]:\n",
    "        candidates = list(candidates)\n",
    "        # get 1000 h as a list(h)\n",
    "        h_temp = []\n",
    "        for url in candidates:\n",
    "            h_temp.append(detokenize(urlTitles[url]))\n",
    "        # get p,h\n",
    "        p,h = char_index(query,h_temp)\n",
    "        y = np.zeros((1000))                    # whatever initialization\n",
    "        \n",
    "        scoresCandidates = []\n",
    "        logit = computeSimilarity(p,h,y)            # get the 1000 scores list\n",
    "        scoresCandidates = zip(candidates,list(logit[:,1]))\n",
    "        ranking = sortBy(scoresCandidates, index=1, desc=True)\n",
    "        ranking = [e[0] for e in ranking]\n",
    "        usrRankings.append(ranking)\n",
    "    rankings[usr] = usrRankings\n",
    "    \n",
    "    pbar.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  20% [====                ] (2m 26.159s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  40% [========            ] (1m 49.919s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  60% [============        ] (1m 13.98s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  80% [================    ] (36.875s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/convnet/output/convnet_9.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing 100% [====================] (total duration: 3m 3.979s, mean duration: 36.796s)\n"
     ]
    }
   ],
   "source": [
    "# rankings of the candidates with 0/1 classification results\n",
    "pbar = ProgressBar(len(trainUsers.keys()), logger=logger, message=\"testing\")\n",
    "rankings = dict()\n",
    "for usr in trainUsers.keys():\n",
    "    usrRankings = []\n",
    "    \n",
    "    # get historical data as query(p)\n",
    "    query = trainUserQuery[usr] * 1000\n",
    "    \n",
    "    for candidates in evalData['candidates'][usr]:\n",
    "        candidates = list(candidates)\n",
    "        # get 1000 h as a list(h)\n",
    "        h_temp = []\n",
    "        for url in candidates:\n",
    "            h_temp.append(detokenize(urlTitles[url]))\n",
    "        # get p,h\n",
    "        p,h = char_index(query,h_temp)\n",
    "        y = np.zeros((1000))                    # whatever initialization\n",
    "        \n",
    "        scoresCandidates = []\n",
    "        prediction = computeSimilarity(p,h,y)            # get the 1000 scores list\n",
    "        scoresCandidates = zip(candidates,prediction)\n",
    "        ranking = sortBy(scoresCandidates, index=1, desc=True)\n",
    "        ranking = [e[0] for e in ranking]\n",
    "        usrRankings.append(ranking)\n",
    "    rankings[usr] = usrRankings\n",
    "    \n",
    "    pbar.tic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add the ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the ranking before adding to the db for evaluation\n",
    "checkRankings(rankings,evalData['candidates'],maxUsers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the ranking into de DB\n",
    "addRanking(modelName, rankings, config, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
