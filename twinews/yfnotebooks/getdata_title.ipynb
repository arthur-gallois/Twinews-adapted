{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from databasetools.mongo import *\n",
    "from newstools.goodarticle.utils import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.news import parser as newsParser\n",
    "from nlptools.tokenizer import *\n",
    "from machinelearning.iterator import *\n",
    "from twinews.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pymongo\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twinews users (version 1.0) initialised.\n"
     ]
    }
   ],
   "source": [
    "ConnUsers = getUsersCollection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twinews news (version 1.0) initialised.\n"
     ]
    }
   ],
   "source": [
    "ConnNews = getNewsCollection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tictoc starts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(tmpDir('logs') + \"title_corpus.log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# title corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  Carolina Panthers surprises Triad couple who wedded in hospice chapel on Valentine's Day,\n",
      "  wtsp.com | You know those flashing crosswalks? Here's why cities can no longer install them,\n",
      "  ...,\n",
      "  WV MetroNews – Bill that would alter public assistance benefits heads to House floor,\n",
      "  wtsp.com | 'It's working' | Baby A.J. out of surgery after surprise kidney transplant\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# get title list\n",
    "titleList = []\n",
    "for row in ConnNews.find().limit(100):\n",
    "    titleList.append(row[\"title\"])\n",
    "bp(titleList,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleList = list(filter(None, titleList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing   1% [                    ]\n",
      "tokenizing  10% [==                  ] (0s left)\n",
      "tokenizing  20% [====                ] (0s left)\n",
      "tokenizing  30% [======              ] (0s left)\n",
      "tokenizing  40% [========            ] (0s left)\n",
      "tokenizing  50% [==========          ] (0s left)\n",
      "tokenizing  60% [============        ] (0.006s left)\n",
      "tokenizing  70% [==============      ] (0.004s left)\n",
      "tokenizing  80% [================    ] (0.002s left)\n",
      "tokenizing  90% [==================  ] (0.001s left)\n",
      "tokenizing 100% [====================] (total duration: 0.01s, mean duration: 0s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [ [ Carolina, Panthers, ..., 's, Day ] ],\n",
      "  [ [ wtsp.com, |, ..., crosswalks, ? ], [ Here, 's, ..., install, them ] ],\n",
      "  ...,\n",
      "  [ [ WV, MetroNews, ..., House, floor ] ],\n",
      "  [ [ wtsp.com, |, ..., Baby, A.J. ], [ out, of, ..., kidney, transplant ] ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# title tokenize\n",
    "titleListToken = []\n",
    "for i in pb(list(range(len(titleList))), logger=logger, message=\"tokenizing\"):\n",
    "    titleListToken.append(sentenceTokenize(titleList[i]))\n",
    "bp(titleListToken,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lower casing   0% [                    ]\n",
      "Lower casing   9% [=                   ] (1.35s left)\n",
      "Lower casing  19% [===                 ] (1.24s left)\n",
      "Lower casing  29% [=====               ] (1.073s left)\n",
      "Lower casing  39% [=======             ] (0.915s left)\n",
      "Lower casing  49% [=========           ] (0.77s left)\n",
      "Lower casing  59% [===========         ] (0.613s left)\n",
      "Lower casing  69% [=============       ] (0.458s left)\n",
      "Lower casing  79% [===============     ] (0.307s left)\n",
      "Lower casing  89% [=================   ] (0.153s left)\n",
      "Lower casing  99% [=================== ] (0s left)\n",
      "Lower casing 100% [====================] (total duration: 1.53s, mean duration: 0s)\n",
      "[ [ carolina, panthers, ..., 's, day ], [ wtsp.com, |, ..., install, them ], ..., [ harvey, was, ..., the, guardian ], [ d.c., and, ..., washington, post ] ]\n"
     ]
    }
   ],
   "source": [
    "# lower case\n",
    "for i in pb(list(range(len(titleListToken))), logger=logger, message=\"Lower casing\"):\n",
    "    for u in range(len(titleListToken[i])):\n",
    "        titleListToken[i][u] = titleListToken[i][u].lower()\n",
    "bp(titleListToken, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatization   0% [                    ]\n",
      "Lemmatization   9% [=                   ] (4.41s left)\n",
      "Lemmatization  19% [===                 ] (3.88s left)\n",
      "Lemmatization  29% [=====               ] (3.383s left)\n",
      "Lemmatization  39% [=======             ] (2.91s left)\n",
      "Lemmatization  49% [=========           ] (2.43s left)\n",
      "Lemmatization  59% [===========         ] (1.946s left)\n",
      "Lemmatization  69% [=============       ] (1.461s left)\n",
      "Lemmatization  79% [===============     ] (0.982s left)\n",
      "Lemmatization  89% [=================   ] (0.496s left)\n",
      "Lemmatization  99% [=================== ] (0s left)\n",
      "Lemmatization 100% [====================] (total duration: 4.98s, mean duration: 0s)\n",
      "[ [ carolina, panther, ..., 's, day ], [ wtsp.com, |, ..., install, them ], ..., [ harvey, wa, ..., the, guardian ], [ d.c., and, ..., washington, post ] ]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "pbar = ProgressBar(len(titleListToken), logger=logger, message=\"Lemmatization\")\n",
    "for i in range(len(titleListToken)):\n",
    "    for u in range(len(titleListToken[i])):\n",
    "        titleListToken[i][u] = lemmatizer.lemmatize(titleListToken[i][u])\n",
    "    pbar.tic()\n",
    "bp(titleListToken, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Voc removed because of minDF (53043 elements):\n",
      "{ '-, '.22summers, '021, '06, '10, '100, '1031, '11, '12, '12th, ..., ☆lil, ❤, ❤️, ツ, 杜博思, ﻿, ﻿chaplin, ﻿opioid, ﻿travel, ﻿united }\n",
      "Voc removed because of maxDF (20 elements):\n",
      "{ ', 's, ,, -, :, a, and, at, for, in, is, new, news, of, on, the, to, with, |, – }\n",
      "94.59% of voc will be removed.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "A document doesn't have words anymore after filtering.\n",
      "[ [ carolina, panther, ..., valentine, day ], [ wtsp.com, you, ..., longer, them ], ..., [ harvey, wa, ..., u, guardian ], [ d.c., maryland, ..., washington, post ] ]\n"
     ]
    }
   ],
   "source": [
    "corpus = filterCorpus(titleListToken, minDF=1/2000, maxDF=20,\n",
    "                    removeEmptyDocs=True, allowEmptyDocs=True, logger=logger)\n",
    "for title in titleListToken: assert len(title) > 0\n",
    "bp(corpus, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "writing vocab   0% [                    ]\n",
      "writing vocab   9% [=                   ] (8.64s left)\n",
      "writing vocab  19% [===                 ] (7.96s left)\n",
      "writing vocab  29% [=====               ] (7.024s left)\n",
      "writing vocab  39% [=======             ] (6.18s left)\n",
      "writing vocab  49% [=========           ] (5.12s left)\n",
      "writing vocab  59% [===========         ] (4.08s left)\n",
      "writing vocab  69% [=============       ] (3.056s left)\n",
      "writing vocab  79% [===============     ] (2.048s left)\n",
      "writing vocab  89% [=================   ] (1.032s left)\n",
      "writing vocab  99% [=================== ] (0s left)\n",
      "writing vocab 100% [====================] (total duration: 10.43s, mean duration: 0s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3029\n"
     ]
    }
   ],
   "source": [
    "# write to txt as vocab\n",
    "titleVocab = []\n",
    "for i in pb(list(range(len(corpus))), logger=logger, message=\"writing vocab\"):\n",
    "    for u in range(len(corpus[i])):\n",
    "        if corpus[i][u] not in titleVocab:\n",
    "            titleVocab.append(corpus[i][u])\n",
    "titleVocab.sort()\n",
    "bp(len(titleVocab))\n",
    "with open('/home/yuting/PycharmProjects/data/title_vocab.txt', 'w',encoding=\"UTF-8\") as f:\n",
    "    for word in titleVocab:\n",
    "        title_word = str(word)\n",
    "        f.write(title_word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ !, #, $, %, &, '', 'black, 'd, 'it, 'll, 'm, 'no, 'not, 're, 'star, 'the, 'this, 've, 'we, 'you, (, ), --, ., ..., 1, 1,000, 10, 100, 100,000, 105.9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 1st, 2, 20, 200, 2016, 2017, 2018, 2019, 2020, 21, 22, 23, 24, 25, 26, 27, 2nd, 3, 30, 307, 4, 40, 5, 50, 500, 6, 60, 630—where, 7, 70, 720, 74, 7news, 8, 80, 9, 90, 93.1, ;, ?, @, [, ], ``, able, abortion, about, abuse, academic, academy, access, accident, accidentally, according, account, accusation, accused, accuser, accuses, acquires, across, act, action, active, activist, activity, actor, actually, ad, adam, add, addiction, address, administration, admits, adoption, adult, advance, adventure, advice, adviser, advisory, advocate, affair, affect, affordable, afghan, afghanistan, afraid, africa, african, after, ag, again, against, age, agency, agenda, agent, aging, ago, agree, agreement, ahead, ai, aid, aide, aim, air, airbnb, airline, airport, al, alabama, albany, album, alcohol, alert, alex, alexa, alive, all, allegation, alleged, allegedly, allen, alliance, allow, allowed, allows, almost, alone, along, already, also, alternative, alumnus, always, alzheimer, am, amazing, amazon, ambassador, amendment, america, american, amid, among, amtrak, an, analysis, anchor, ancient, anderson, andrew, android, angel, angeles, angry, animal, anniversary, announce, announced, announcement, announces, annual, another, answer, anthem, anthony, anxiety, any, anyone, apart, apartment, apologizes, apology, aposs, app, appeal, apple, application, approach, approval, approved, approves, apps, ar, arabia, archive, arctic, are, area, argument, argus, arizona, arlington, arm, armed, army, around, arrest, arrested, art, artery, article, artificial, artist, ash, asia, asian, ask, asked, asking, asks, assault, assistance, assistant, association, athlete, atlanta, atlantic, attack, attempt, attend, attention, attorney, austin, australia, author, authority, autism, auto, autonomous, available, avoid, award, awarded, awareness, away, b, b.c, baby, back, backlash, bad, bag, bail, baker, bakersfield, balance, ball, ballot, baltimore, ban, band, bank, banned, bannon, bar, barrier, base, baseball, based, basic, basketball, baton, battery, battle, battling, bay, be, beach, bear, beast, beat, beautiful, beauty, beaver, became, because, become, becomes, becoming, been, beer, before, begin, behavior, behind, being, belief, believe, bell, ben, benefit, bernie, best, bet, better, between, beyond, bias, bid, big, bigger, biggest, bike, bill, billion, billionaire, bipartisan, bird, birth, birthday, bitcoin, bite, bizarre, black, blade, blame, blast, blind, block, blockchain, blog, blood, blow, blue, board, boat, bob, body, bomb, bond, bonus, book, boom, boost, border, born, bos, boston, bot, both, bowl, box, boy, brace, brain, brand, breach, break, breaking, breast, brexit, brian, bridge, brief, bring, bringing, brings, britain, british, broadband, broadly, broke, broken, brother, brought, broward, brown, bruce, budget, buffalo, bug, build, building, built, bull, bullying, burlington, burn, bus, bush, business, but, button, buy, buying, by, c, ca, cable, cafe, california, call, called, calling, came, camera, camp, campaign, campus, can, canada, canadian, cancel, canceled, cancer, candidate, candy, cannabis, capital, capitol, capture, car, carbon, card, care, career, carol, carolina, carry, carson, carter, case, cash, casino, cast, cat, catch, catholic, caught, cause, caused, cbs, cdc, ce, celebrate, celebrates, celebrating, celebration, celebrity, cell, center, central, centre, century, ceo, ceremony, chain, chair, chairman, challenge, champion, championship, chance, chancellor, change, changed, changing, channel, chaos, character, charge, charged, charging, charity, charles, charlie, charlotte, charlottesville, chart, charter, chase, chattanooga, check, chef, chemical, chicago, chicken, chief, child, china, chinese, chip, choice, chris, christian, christmas, chronicle, church, cia, cincinnati, cio, citizen, city, civil, civilian, claim, class, classic, classroom, clean, clear, cleveland, climate, clinic, clinton, close, closed, closer, closing, closure, cloud, club, cnn, co., coach, coal, coalition, coast, code, coffee, cognoscenti, cold, colin, collapse, collection, college, color, colorado, columbia, columbus, column, columnist, combat, come, comeback, comedian, comedy, comic, coming, comment, commentary, commercial, commission, commissioner, committee, common, commonhealth, communication, community, company, compete, competition, complaint, complete, complex, compliance, computer, concern, concerned, concert, condition, condo, conduct, confederate, conference, confidential, confirms, conflict, congress, congressional, congressman, connect, connecticut, connection, conservation, conservative, consider, considering, considers, conspiracy, construction, consumer, contact, content, contest, continue, continues, contract, control, controversial, controversy, convention, conversation, convicted, conviction, cook, cool, cooper, cop, core, corner, corporate, corruption, cost, costume, could, council, count, country, county, couple, course, court, cover, coverage, cowboy, crack, crackdown, craft, crash, crazy, create, created, creates, creating, creative, creator, credit, creek, crew, crime, criminal, crimson, crisis, critic, critical, criticism, cross, crowd, cryptocurrency, cultural, culture, cuomo, cup, cure, current, custody, customer, cut, cutting, cyber, cybersecurity, cyclist, d.c., da, daca, dad, daily, dairy, dallas, dalton, damage, dan, dance, danger, dangerous, daniel, dark, data, date, dating, daughter, dave, david, davis, davos, day, dc, de, dead, deadline, deadly, deal, death, debate, debt, debut, dec., decade, december, decide, decision, declares, decline, deep, defeat, defend, defends, defense, degree, delaware, delay, deliver, delivers, delivery, demand, dementia, democracy, democrat, democratic, dems, denied, denies, denver, denver7, department, deportation, depression, dept, deputy, desert, deserve, design, designer, despite, detail, detective, detention, detroit, develop, developer, developing, development, device, devos, diabetes, did, die, died, diego, diet, difference, different, digital, dinner, director, disability, disabled, disaster, discovered, discovery, discrimination, discus, discussion, disease, disney, display, dispute, district, diversity, divide, division, dna, do, doc, doctor, document, documentary, doe, dog, doing, doj, dollar, domestic, donald, donate, donates, donation, done, donor, door, dot, double, doug, down, downtown, dozen, dr., draft, drama, draw, dream, dreamer, drilling, drink, drinking, drive, driver, driverless, driving, drone, drop, drug, due, duke, duo, during, duty, dy, dying, each, eagle, early, earth, earthquake, easier, east, eastern, easy, eat, eating, echo, eclipse, economic, economics, economy, ed, edge, edition, editor, editorial, education, educator, edward, effect, effective, effort, egypt, eight, el, elderly, elected, election, electric, elementary, elizabeth, elon, else, email, embrace, emergency, emission, emotional, empire, employee, employer, end, ending, energy, enforcement, engagement, engineer, engineering, england, english, english.news.cn, enough, enrollment, enterprise, entertainment, entire, entrepreneur, environment, environmental, epa, epidemic, episode, epoch, equal, equality, equifax, equity, era, eric, escape, espn, estate, ethic, ethical, eu, europe, european, eve, even, event, ever, every, everyone, everything, evidence, evolution, exceptional, exchange, exclusive, exec, executive, exercise, exhibit, exit, expand, expands, expansion, expect, expected, experience, experiment, expert, explain, explained, explains, expose, exposed, express, extension, extra, extreme, eye, face, facebook, facility, facing, fact, factory, faculty, fail, failed, failing, fails, failure, fair, faith, fake, falcon, fall, fallen, falling, false, fame, family, fan, far, farm, farmer, fashion, fast, faster, fatal, father, favor, favorite, fbi, fcc, fda, fear, feature, february, fed, federal, fee, feed, feel, feeling, fellow, fellowship, female, festival, few, fewer, fiction, field, fight, fighting, figure, file, fill, film, final, finalist, finally, finance, financial, find, finding, fine, finish, fire, fired, firefighter, firing, firm, first, fish, fit, five, fix, fl, flag, flaw, flight, flood, florida, flu, fly, flying, flynn, fm, focus, follow, following, food, foot, football, force, forced, ford, forecast, foreign, forest, forever, forget, forgotten, form, former, fort, forum, forward, fossil, foster, found, foundation, founder, four, fox, france, francisco, frank, franken, fraternity, fraud, fred, free, freedom, freeze, french, fresh, friday, friend, from, front, fsu, fuel, full, fun, fund, funding, fundraiser, further, future, futurelawyer, g, gain, gallery, game, gamesbeat, gang, gap, garage, garden, gary, gas, gate, gave, gay, gear, gender, gene, general, generation, george, georgia, german, germany, get, getting, ghost, giant, gift, girl, give, given, giving, glass, global, globe, go, goal, god, going, gold, golden, golf, gone, gonzales, good, google, gop, got, gov, government, governor, grad, grade, graduate, graduation, graham, grammys, grand, grant, great, greater, greatest, green, grid, grocery, ground, group, grow, growing, grows, growth, guard, guardian, guest, guide, guilty, gun, gunman, guy, ha, hack, hacker, had, hair, half, hall, halloween, halt, hamilton, hampshire, hand, handle, hanford, happen, happened, happening, happens, happy, harassment, hard, harry, harvard, harvey, hate, have, haven, having, hawaii, he, head, headed, headline, headquarters, health, healthcare, healthy, hear, heard, hearing, hears, heart, heat, heavy, hedge, height, held, hell, help, helped, helping, her, herald, here, heritage, hero, heroin, hidden, high, higher, highest, highlight, highway, hike, hill, hillary, him, himself, hip-hop, hire, hiring, his, historic, history, hit, hiv, hockey, hold, holding, hole, holiday, hollywood, holocaust, home, homeless, homelessness, homepagelatest, homicide, honor, honored, hope, horror, hospital, host, hot, hotel, hour, house, housing, houston, how, hq2, hub, huge, human, humanity, hundred, hunt, hunter, hunting, hurricane, hurt, husband, i, i-d, ice, iconic, id, idea, identified, identify, identity, if, ii, ill, illegal, illinois, illness, image, immigrant, immigration, impact, important, improv, improve, improvement, improving, incident, include, including, income, increase, independent, india, indian, indiana, indianapolis, indigenous, individual, industry, inequality, infant, influence, information, infrastructure, initiative, injection, injured, injury, ink, inmate, innovation, inside, insider, instagram, instead, institute, insurance, insurer, intel, intelligence, interest, international, internet, interview, into, introduces, invest, investigate, investigating, investigation, investing, investment, investor, involved, io, iowa, iphone, iran, ireland, irma, isi, island, israel, israeli, issue, it, itself, j., jack, jackson, jail, james, jan., jane, january, japan, japanese, jason, jay, jazz, jedi, jeff, jerry, jersey, jerusalem, jet, jewish, jim, jimmy, job, joe, john, johnson, join, joke, jones, jordan, journal, journalism, journalist, journey, joy, jr., judge, judicial, jump, jury, just, justice, justin, juvenile, k, kaepernick, kansa, kansan, keep, keeping, kelly, kennedy, kentucky, kevin, key, kick, kid, kill, killed, killer, killing, kim, kind, king, kit, knew, know, knowledge, korea, korean, l.a., la, lab, labor, lack, lady, lake, lakeland, land, lane, language, lantern, lapd, large, largest, larry, last, late, later, latest, lauderdale, lauer, launch, launched, law, lawmaker, lawsuit, lawyer, ldi, le, lead, leader, leadership, leading, leaf, league, leak, learn, learned, learning, least, leave, leaving, lecture, led, ledger, lee, left, legacy, legal, legend, legislation, legislative, legislator, legislature, lesbian, lesson, let, letter, level, lewis, lgbt, lgbtq, liberal, liberty, library, license, lie, life, lift, light, like, likely, limit, lincoln, line, link, linked, list, listen, listening, literacy, little, live, living, loan, local, location, log, london, long, longer, longform, lonzo, look, looking, loom, los, lose, loses, losing, loss, lost, lot, louis, louisiana, louisville, love, loved, low, lower, lsa, lunch, luther, lyft, m, ma, mac, machine, made, madison, magazine, magic, mail, main, major, majority, make, maker, making, male, mall, malware, man, manafort, management, manager, mandate, maneater, manufacturing, many, map, marathon, march, maria, marijuana, marine, mark, market, marketing, markle, marriage, marshall, martin, mary, maryland, mass, massachusetts, massacre, massive, master, match, math, matt, matter, may, mayor, mayoral, mcdonald, mcmaster, md, me, meal, mean, measure, medicaid, medical, medicine, medium, meet, meeting, meghan, member, memo, memorial, memory, men, mental, mess, message, met, metal, metoo, metro, metronews, mexican, mexico, miami, michael, michigan, microsoft, middle, might, migrant, migration, mike, mile, military, mill, millennials, miller, million, milwaukee, mind, mine, minimum, mining, minister, minnesota, minor, minority, minute, misconduct, miss, missile, missing, mission, mississippi, missouri, mistake, mixed, mlk, mn, mobile, mobility, model, modern, mom, moment, monday, money, montgomery, month, monument, moon, moore, more, morgan, morning, most, mostly, mother, motherboard, mount, mountain, move, movement, movie, moving, mp, much, mueller, multiple, munchies, murder, museum, music, musical, musician, musk, muslim, must, my, mystery, myth, n, n't, n., nafta, naked, name, named, nasa, nashville, nassar, nation, national, nationalist, native, natural, nature, navy, nazi, nba, nbc, nc, near, nearly, need, needed, neighbor, neighborhood, net, netflix, network, neutrality, never, newest, newspaper, newton, next, nfl, nh, nick, night, nine, no, noisey, nomination, nominee, nonprofit, north, northern, not, note, nothing, nov., novel, november, now, nuclear, number, nunes, nurse, nursing, ny, nyc, obama, obamacare, oc, ocean, october, odds, off, offender, offer, offering, office, officer, official, officially, offshore, often, oh, ohio, oil, ok, oklahoma, old, older, olympian, olympic, olympics, once, one, online, only, ontario, open, opening, opera, operation, opinion, opioid, opioids, opportunity, opposition, oprah, option, or, order, ordered, oregon, organization, original, orleans, oscar, other, others, ottawa, our, out, outbreak, outline, outside, over, overdose, overhaul, overnight, own, owner, p, pa, package, page, paid, pain, pakistan, palestinian, palm, panel, panther, paper, parade, parent, paris, park, parking, part, partner, partnership, party, pas, pass, passenger, past, pastor, patch, path, patient, patrick, patriot, patrol, paul, pay, paying, payment, pc, peace, pedestrian, penalty, penn, pennsylvania, penny, pension, pentagon, people, per, percent, perfect, perform, performance, permit, perry, person, personal, perspective, pet, peter, philadelphia, phoenix, phone, photo, photographer, pick, picture, piece, pilot, pioneer, pipeline, pitch, pittsburgh, pizza, place, plan, plane, planet, planned, planning, plant, plastic, platform, play, player, playing, playoff, plea, pleads, please, pledge, plot, podcast, point, police, policy, political, politician, politics, poll, pollution, poor, pop, pope, popular, population, porn, port, portland, portrait, position, positive, possible, post, pot, potential, poverty, power, powerful, practice, praise, prediction, pregnant, premiere, prep, prepare, prepares, prescription, present, presidency, president, presidential, press, pressure, prevent, prevention, preview, price, primary, prime, prince, principal, priority, prison, prisoner, privacy, private, prize, pro, probably, probe, problem, process, producer, product, production, prof, professional, professor, profile, profit, program, progress, progressive, project, promise, promote, prompt, propaganda, property, proposal, proposed, proposes, prosecutor, protect, protected, protecting, protection, protest, protester, prove, provide, providence, provides, province, pruitt, public, puerto, pull, push, pushing, put, putin, putting, q, quality, queen, question, quietly, quit, race, racial, racism, racist, radical, radio, rage, rail, rain, raise, raised, raising, rally, rank, ranked, rap, rape, rapper, rare, rate, rating, reach, read, reader, reading, ready, real, reality, really, reason, rebel, recall, receive, receives, recent, recipe, recipient, recognition, record, recovery, recruit, red, reduce, reflects, reform, refugee, refuse, region, regional, register, regulation, regulator, reject, relationship, release, released, relief, religious, remain, remains, remark, remember, remembering, remix, remove, removed, renewable, rent, rental, rep., repair, repeal, replace, report, reported, reportedly, reporter, reporting, republican, request, require, requirement, rescue, research, researcher, reserve, resident, resign, resigns, resistance, resolution, resort, resource, respond, responder, responds, response, rest, restaurant, restore, result, retail, retailer, retire, retirement, return, returning, reveal, revealed, reveals, revenue, review, revolution, ri, rich, richard, richmond, rick, rico, ride, rider, right, ring, riot, rip, rise, rising, risk, river, road, rob, robbery, robert, robot, rock, rocket, rohingya, role, roll, rolling, room, root, rose, rouge, round, roundup, route, row, roy, royal, rule, ruling, run, running, rural, rush, russia, russian, ryan, s., safe, safety, said, saint, salary, sale, sam, same, samsung, san, sanction, sanctuary, sander, santa, sarah, saturday, saudi, save, saved, saving, saw, say, saying, sc, scam, scana, scandal, scare, scene, scheme, scholar, scholarship, school, science, scientist, score, scott, scout, screen, screening, sea, seahawks, search, searching, season, seat, seattle, second, secret, secretary, secretary-general, sector, security, see, seeing, seek, seeking, seen, self-driving, sell, selling, sen., senate, senator, send, sends, senior, sense, sent, sentence, sentenced, sentinel, serial, series, serious, seriously, serve, service, session, set, seth, setting, settlement, seven, several, sex, sexual, sexually, shadow, shape, share, sharing, shark, she, shelter, sheriff, shift, shine, ship, shoot, shooter, shooting, shop, shopper, shopping, shore, short, shortage, shot, should, show, shut, shutdown, shuts, sick, side, sight, sign, signal, silence, silicon, simple, since, singer, single, sister, site, six, skill, sky, slam, slave, slavery, sleep, slow, small, smart, smartphone, smith, smoking, snap, snapchat, snow, so, soar, soccer, social, socialist, society, software, solar, sold, soldier, solution, solve, some, someone, something, son, song, soon, sought, soul, sound, source, south, southeastern, southern, space, spacex, spark, speak, speaker, speaks, special, specie, speech, speed, spend, spending, spent, spike, spirit, sport, spot, spotlight, spread, spring, spy, square, st., stadium, staff, staffer, stage, stake, stand, standard, star, start, started, starting, startup, state, statement, station, statue, status, stay, steal, stem, step, stephen, steve, steven, still, stock, stolen, stone, stop, store, storm, story, straight, strange, stranger, strategy, stream, streaming, street, streetsblog, stress, strike, strong, struck, struggle, struggling, student, studio, study, stunning, style, subway, success, successful, sudbury, sue, sued, suggests, suicide, suit, sulphurdailynews.com, summer, summit, sun, sun-times, sundance, sunday, sunshine, super, superintendent, supply, support, supporter, supremacist, supreme, sure, surge, surgery, surprise, surveillance, survey, survive, survivor, suspect, suspected, suspended, sustainability, sustainable, sustainist, sweep, switch, syndrome, syracuse, syracuse.com, syria, syrian, system, t, tab, table, tablet, tackle, tacoma, tahlequahdailypress.com, tahoe, tahoedailytribune.com, take, takeaway, taken, taking, tale, talent, talk, talking, tampa, tank, tap, target, targeted, targeting, task, taste, tax, taxpayer, taxprof, taylor, tbo.com, teach, teacher, teaching, team, tech, techcrunch, technology, ted, teen, teenager, telegram.com, telegraph, television, tell, temperature, ten, tennessee, term, terrible, terror, terrorism, terrorist, tesla, test, testing, texas, text, than, thank, thanks, thanksgiving, that, theater, theatre, thebaynet.com, theblaze, thedailytimes.com, thedenverchannel.com, theeagle.com, theft, theindychannel.com, their, them, themonitor.com, themountainmail.com, themselves, then, theory, therapy, there, therecord.com, thescore.com, these, theshorthorn.com, thesouthern.com, thespec.com, thestreet, they, thing, think, thinkprogress, third, this, thomas, those, thought, thousand, threat, threatens, threatpost, three, through, throw, thursday, ticket, tide, tie, tiger, tim, time, time.com, tiny, tip, title, tmz.com, tobacco, today, today.com, together, told, toledo, toll, tom, tonic, tonight, too, took, tool, top, topic, toronto, total, tough, tour, toward, towards, tower, town, toxic, toy, track, tracking, trade, tradition, traffic, trafficking, tragedy, trail, trailer, train, training, trans, transcript, transfer, transgender, transit, transplant, transport, transportation, trash, trauma, travel, treat, treatment, tree, treehugger, trek, trend, tri-city, trial, trib.com, tribune, tribute, trick, tried, trip, tristatehomepage, troll, troop, trooper, trouble, truck, trudeau, true, trump, trust, truth, truthdig, try, trying, tuesday, tuition, tulsa, tulsaworld.com, turkey, turn, turned, turning, turnto23.com, tuscaloosa, tv, tva, tvline, tvtechnology, tweet, twice, twin, twitter, two, tyee, type, u, u-m, u.s, u.s., uber, uc, ucla, uk, un, under, underground, understand, understanding, undocumented, unicorn, union, unique, unit, united, universal, university, until, unveils, up, update, updated, upi.com, upset, upstream, urban, urge, us, usa, use, used, user, using, ut, uta, utica, uticaod, utility, uw, uw-madison, uw–madison, v, va, va., vacation, vaccine, vaildaily.com, valentine, valley, value, van, vancouver, vanity, variety, vc, vccircle, vega, vehicle, venezuela, venture, venturebeat, verge, vermont, very, vet, veteran, via, vice, victim, victorville, victory, video, view, village, violation, violence, violent, viral, virginia, virtual, visa, vision, visit, voice, volunteer, vote, voter, voting, vow, vox, vr, vs., vtdigger, vulnerable, vvdailypress.com, wa, wafb, wage, wahpetondailynews.com, wait, waiting, wake, wale, walk, walker, walking, wall, walmart, want, wanted, war, warm, warming, warn, warning, warns, warren, washington, washingtonian, waste, watch, watchdog, watching, water, wausau, wave, way, waypoint, wbez, wbir.com, wbur, wcfcourier.com, wcpo, wcyb, we, weapon, wear, weather, web, website, wedding, wednesday, weed, week, weekend, weekly, weight, weinstein, welcome, welfare, well, went, were, west, western, westword, wfaa.com, wfae, wfla, wfmz, wfsb, wftv, wgn, wgn-tv, wgno, wgrz.com, wharton, whas11.com, what, whatsapp, whdh, when, where, whether, which, while, white, whnt.com, who, whole, whotv.com, why, wibc, wicked, wife, wild, wildfire, wildlife, will, willamette, william, williams, wilson, win, wind, window, windsor, wine, wing, winner, winning, winnipeg, winter, wire, wired, wisconsin, wisconsin-madison, wish, wistv.com, witf.org, within, without, witness, wivb, wjar, wjla, wkbw.com, wkrn, wksu, wkyc.com, wltx.com, wmal-af, wmar2news, wnyc, wo, wolf, woman, won, wonder, wonkette, wood, woodtv, worcester, word, work, worker, workforce, working, workplace, workshop, world, worldwide, worried, worry, worse, worst, worth, would, wpbn, wpri, wptv.com, wpxi, wral.com, wrcbtv.com, wreg.com, write, writer, writing, wrong, wsav, wsb-tv, wsfa.com, wsj, wsmv, wsoc-tv, wsvn, wthr, wtkr.com, wtop, wtsp.com, wtvr.com, wusa9.com, wuwm, wv, wwd, wwlp, wwltv.com, wxyz.com, wyoming, wzzm13.com, x, xconomy, xinhua, xxl, year, yellowhammer, yemen, yes, yet, york, yorkregion.com, yorkshire, you, young, your, yourself, yourtango, youth, youtube, zdnet, zero, zone, zoo, —, •, … ]\n"
     ]
    }
   ],
   "source": [
    "titleVocab = []\n",
    "with open('/home/yuting/PycharmProjects/data/title_vocab.txt','r') as file:\n",
    "    for line in file.readlines():\n",
    "        titleVocab.append(line.strip('\\n'))\n",
    "\n",
    "bp(titleVocab,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get training data urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get user & news list\n",
    "userHistory = {}\n",
    "\n",
    "# read user with more than 30 news\n",
    "for row in ConnUsers.find().limit(10000):\n",
    "    if len(row[\"news\"]) >= 30:\n",
    "        userHistory[row[\"user_id\"]] = row[\"news\"]\n",
    "# bp(userHistory,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "negNews = {}\n",
    "for row in ConnNews.find().limit(10000):\n",
    "    negNews[row['url']] = row['title']\n",
    "# bp(negNews,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "negNewsList = list(negNews.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50295\n",
      "10059\n",
      "10059\n"
     ]
    }
   ],
   "source": [
    "# user_id, query\n",
    "queryTitleUrls = []\n",
    "posTitleUrls = []\n",
    "negTitleUrls = []\n",
    "for key in userHistory.keys():\n",
    "    # query\n",
    "    query = random.sample(userHistory[key],15) # get 15 history news\n",
    "    queryTitleUrls.extend(query)\n",
    "    # pos\n",
    "    pos = random.sample(list(set(userHistory[key]).difference(set(query))),3)\n",
    "    posTitleUrls.extend(pos)\n",
    "    # neg\n",
    "    neg = random.sample(list(set(negNewsList).difference(set(query))),3)\n",
    "    negTitleUrls.extend(neg)\n",
    "    \n",
    "            \n",
    "# queryTitles = getNewsTitles(queryTitleUrls)\n",
    "# posTitles = getNewsTitles(posTitleUrls)\n",
    "bp(len(queryTitleUrls),2)\n",
    "bp(len(posTitleUrls),2)\n",
    "bp(len(negTitleUrls),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100% [====================] (total duration: 30m 48.91s, mean duration: 0.036s)\n",
      "  100% [====================] (total duration: 6m 6.399s, mean duration: 0.036s)\n",
      "  100% [====================] (total duration: 4m 54.86s, mean duration: 0.029s)\n"
     ]
    }
   ],
   "source": [
    "queryTitles = getNewsTitles(queryTitleUrls)\n",
    "posTitles = getNewsTitles(posTitleUrls)\n",
    "negTitles = getNewsTitles(negTitleUrls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing   0% [                    ]\n",
      "tokenizing   9% [=                   ] (3.6s left)\n",
      "tokenizing  19% [===                 ] (3.96s left)\n",
      "tokenizing  29% [=====               ] (3.687s left)\n",
      "tokenizing  39% [=======             ] (3.225s left)\n",
      "tokenizing  49% [=========           ] (2.74s left)\n",
      "tokenizing  59% [===========         ] (2.2s left)\n",
      "tokenizing  69% [=============       ] (1.667s left)\n",
      "tokenizing  79% [===============     ] (1.115s left)\n",
      "tokenizing  89% [=================   ] (0.56s left)\n",
      "tokenizing  99% [=================== ] (0s left)\n",
      "tokenizing 100% [====================] (total duration: 5.63s, mean duration: 0s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [ Ed, Lee, ..., York, Times ], [ Santa, Barbara, ..., -, KEYT ], ..., [ An, American, ..., InsideClimate, News ], [ She, Left, ..., York, Times ] ]\n"
     ]
    }
   ],
   "source": [
    "queryTitlesToken = []\n",
    "for i in pb(list(range(len(queryTitles))), logger=logger, message=\"tokenizing\"):\n",
    "    queryTitlesToken.append(wordTokenize(queryTitles[i]))\n",
    "bp(queryTitlesToken,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get every 15 for the user\n",
    "queryTitlesToken15 = []\n",
    "for i in range(int(len(queryTitlesToken)/15)):\n",
    "    tempTitle15 = []\n",
    "    for j in range(15):\n",
    "        if queryTitlesToken[i*15+j] == None:\n",
    "            continue\n",
    "        else:\n",
    "            tempTitle15.extend(queryTitlesToken[i*15+j])\n",
    "    queryTitlesToken15.append(tempTitle15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [ ed, lee, ..., |, deadline ], [ colombian, rebel, ..., |, host.madison.com ], ..., [ disney, to, ..., in, retail ], [ trump, roll, ..., york, time ] ]\n"
     ]
    }
   ],
   "source": [
    "bp(queryTitlesToken15,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lower casing   0% [                    ]\n",
      "Lower casing   9% [=                   ] (0.18s left)\n",
      "Lower casing  19% [===                 ] (0.2s left)\n",
      "Lower casing  29% [=====               ] (0.186s left)\n",
      "Lower casing  39% [=======             ] (0.15s left)\n",
      "Lower casing  49% [=========           ] (0.12s left)\n",
      "Lower casing  59% [===========         ] (0.093s left)\n",
      "Lower casing  69% [=============       ] (0.068s left)\n",
      "Lower casing  79% [===============     ] (0.045s left)\n",
      "Lower casing  89% [=================   ] (0.022s left)\n",
      "Lower casing  99% [=================== ] (0s left)\n",
      "Lower casing 100% [====================] (total duration: 0.22s, mean duration: 0s)\n",
      "[ [ ed, lee, ..., |, deadline ], [ colombian, rebels, ..., |, host.madison.com ], ..., [ disney, to, ..., in, retail ], [ trump, rolls, ..., york, times ] ]\n"
     ]
    }
   ],
   "source": [
    "for i in pb(list(range(len(queryTitlesToken15))), logger=logger, message=\"Lower casing\"):\n",
    "    for u in range(len(queryTitlesToken15[i])):\n",
    "        queryTitlesToken15[i][u] = queryTitlesToken15[i][u].lower()\n",
    "bp(queryTitlesToken15, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatization   0% [                    ]\n",
      "Lemmatization   9% [=                   ] (12.612s left)\n",
      "Lemmatization  19% [===                 ] (6.327s left)\n",
      "Lemmatization  29% [=====               ] (4.135s left)\n",
      "Lemmatization  39% [=======             ] (2.944s left)\n",
      "Lemmatization  49% [=========           ] (2.153s left)\n",
      "Lemmatization  59% [===========         ] (1.563s left)\n",
      "Lemmatization  69% [=============       ] (1.096s left)\n",
      "Lemmatization  79% [===============     ] (0.688s left)\n",
      "Lemmatization  89% [=================   ] (0.328s left)\n",
      "Lemmatization  99% [=================== ] (0.002s left)\n",
      "Lemmatization 100% [====================] (total duration: 3.12s, mean duration: 0s)\n",
      "[ [ ed, lee, ..., |, deadline ], [ colombian, rebel, ..., |, host.madison.com ], ..., [ disney, to, ..., in, retail ], [ trump, roll, ..., york, time ] ]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "pbar = ProgressBar(len(queryTitlesToken15), logger=logger, message=\"Lemmatization\")\n",
    "for i in range(len(queryTitlesToken15)):\n",
    "    for u in range(len(queryTitlesToken15[i])):\n",
    "        queryTitlesToken15[i][u] = lemmatizer.lemmatize(queryTitlesToken15[i][u])\n",
    "    pbar.tic()\n",
    "bp(queryTitlesToken15, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete the elements which are not in the vocab\n",
    "def deleteUseless(obj):\n",
    "    compactSample = []\n",
    "    assert isinstance(obj,list)\n",
    "    for i in range(len(obj)):\n",
    "        if isinstance(obj[i],list):\n",
    "            for j in range(len(obj[i][j])):\n",
    "                if obj[i][j] in titleVocab:\n",
    "                    compactSample.append(obj[i][j])\n",
    "        else:\n",
    "            if obj[i] in titleVocab:\n",
    "                compactSample.append(obj[i][j])\n",
    "    return compactSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queryCompact[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "compact query   0% [                    ]\n",
      "compact query   9% [=                   ] (14.954s left)\n",
      "compact query  19% [===                 ] (13.415s left)\n",
      "compact query  29% [=====               ] (11.681s left)\n",
      "compact query  39% [=======             ] (10.034s left)\n",
      "compact query  49% [=========           ] (8.364s left)\n",
      "compact query  59% [===========         ] (6.688s left)\n",
      "compact query  69% [=============       ] (5.012s left)\n",
      "compact query  79% [===============     ] (3.332s left)\n",
      "compact query  89% [=================   ] (1.672s left)\n",
      "compact query  99% [=================== ] (0.014s left)\n",
      "compact query 100% [====================] (total duration: 16.56s, mean duration: 0.004s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [ ed, lee, ..., award, deadline ], [ colombian, rebel, ..., |, host.madison.com ], ..., [ disney, to, ..., in, retail ], [ trump, roll, ..., york, time ] ]\n"
     ]
    }
   ],
   "source": [
    "# delete what was not in the vocab\n",
    "pbar = ProgressBar(len(queryTitlesToken15), logger=logger, message=\"compact query\")\n",
    "queryCompact = []\n",
    "for i in range(len(queryTitlesToken15)):\n",
    "    queryCompactTemp = []\n",
    "    for j in range(len(queryTitlesToken15[i])):\n",
    "        if queryTitlesToken15[i][j] in titleVocab:\n",
    "            queryCompactTemp.append(queryTitlesToken15[i][j])\n",
    "    queryCompact.append(queryCompactTemp)\n",
    "    pbar.tic()\n",
    "bp(queryTitlesToken15,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remove repeated   0% [                    ]\n",
      "remove repeated   9% [=                   ] (0s left)\n",
      "remove repeated  19% [===                 ] (0.04s left)\n",
      "remove repeated  29% [=====               ] (0.046s left)\n",
      "remove repeated  39% [=======             ] (0.045s left)\n",
      "remove repeated  49% [=========           ] (0.04s left)\n",
      "remove repeated  59% [===========         ] (0.033s left)\n",
      "remove repeated  69% [=============       ] (0.025s left)\n",
      "remove repeated  79% [===============     ] (0.017s left)\n",
      "remove repeated  89% [=================   ] (0.007s left)\n",
      "remove repeated  99% [=================== ] (0s left)\n",
      "remove repeated 100% [====================] (total duration: 0.09s, mean duration: 0s)\n",
      "[ [ american, florida, ..., complaint, tax ], [ central, wisconsin, ..., despite, cut ], ..., [ enough, could, ..., brand, espn ], [ could, education, ..., rising, hurricane ] ]\n"
     ]
    }
   ],
   "source": [
    "# delete repeated element\n",
    "pbar = ProgressBar(len(queryCompact), logger=logger, message=\"remove repeated\")\n",
    "queryCompact2 = []\n",
    "for i in range(len(queryCompact)):\n",
    "    queryCompactTemp2 = list(set(queryCompact[i]))\n",
    "    queryCompact2.append(queryCompactTemp2)\n",
    "    pbar.tic()\n",
    "bp(queryCompact2,logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3353"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(queryCompact2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/yuting/PycharmProjects/data/dssm_test', 'w', newline='') as csvfile:\n",
    "    writer  = csv.writer(csvfile)\n",
    "    for row in dataUrls:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sampling and labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detokenize   0% [                    ]\n",
      "detokenize   9% [=                   ] (1m 26.125s left)\n",
      "detokenize  19% [===                 ] (1m 16.565s left)\n",
      "detokenize  29% [=====               ] (1m 6.655s left)\n",
      "detokenize  39% [=======             ] (57.145s left)\n",
      "detokenize  49% [=========           ] (47.705s left)\n",
      "detokenize  59% [===========         ] (38.198s left)\n",
      "detokenize  69% [=============       ] (28.632s left)\n",
      "detokenize  79% [===============     ] (19.1s left)\n",
      "detokenize  89% [=================   ] (9.594s left)\n",
      "detokenize  99% [=================== ] (0.085s left)\n",
      "detokenize 100% [====================] (total duration: 1m 35.209s, mean duration: 0.028s)\n",
      "[\n",
      "  american florida seek harassment down hollywood next. inside about beautiful claim taking post why w,\n",
      "  central wisconsin education ban vote offer all rolen't tie continues committee into seek congression,\n",
      "  ...,\n",
      "  enough could blue digital trail go netflix google seek plan top hollywood streaming back developing ,\n",
      "  could education american kaepernick good chronicle hand plan america. when return back post lead cas\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# detokenize\n",
    "pbar = ProgressBar(len(queryCompact2), logger=logger, message=\"detokenize\")\n",
    "querySample = []\n",
    "for i in range(len(queryCompact2)):\n",
    "    s = detokenize(queryCompact2[i])\n",
    "    querySample.append(s)\n",
    "    pbar.tic()\n",
    "bp(querySample,logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write query sample to txt\n",
    "with open('/home/yuting/PycharmProjects/data/title_query3353.txt', 'w',encoding=\"UTF-8\") as f:\n",
    "    for query in querySample:\n",
    "        write_query = str(query)\n",
    "        f.write(write_query + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10059"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posTitles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 1]\n"
     ]
    }
   ],
   "source": [
    "b = ['a','b','c'] \n",
    "b.append(1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [\"sentence1\",\"sentence2\",\"label\"] \n",
    "with open('/home/yuting/PycharmProjects/data/dssm_title_train.csv', 'w', encoding=\"UTF-8\") as csvfile:\n",
    "    writer  = csv.writer(csvfile, delimiter = ',')\n",
    "    writer.writerow(h)\n",
    "    for i in range(len(querySample)):\n",
    "        for j in range(3):\n",
    "            sampleTempPos = []\n",
    "            sampleTempNeg = []\n",
    "            sampleTempPos.append(querySample[i])\n",
    "            sampleTempPos.append(posTitles[i*3+j])\n",
    "            sampleTempPos.append(1)\n",
    "            sampleTempNeg.append(querySample[i])\n",
    "            sampleTempNeg.append(negTitles[i*3+j])\n",
    "            sampleTempNeg.append(0) \n",
    "            writer.writerow(sampleTempPos)\n",
    "            writer.writerow(sampleTempNeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = []\n",
    "with open('/home/yuting/PycharmProjects/data/dssm_title_train.csv','r', encoding=\"UTF-8\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for line in reader:\n",
    "        allData.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = random.sample(allData, 2000)\n",
    "h = [\"sentence1\",\"sentence2\",\"label\"] \n",
    "with open('/home/yuting/PycharmProjects/data/dssm_title_dev.csv', 'w', encoding=\"UTF-8\") as csvfile:\n",
    "    writer  = csv.writer(csvfile, delimiter = ',')\n",
    "    writer.writerow(h)\n",
    "    for i in range(len(dev)):\n",
    "        writer.writerow(dev[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 4, 5]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(b).difference(set(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 sentence2 label\n",
      "challenge team unveiling innovation tackle glass ceiling along apps wearable tech necklace team created eye catching art installation happened road tested idea silicon valley bbc influential inspirational challenging tackle biggest facing glass ceiling female illiteracy harassment space sexism sport coming real solution involved idea facebook instagram twitter 100women student facing hate crime charge admitting smearing bodily fluid black roommate possession apparent attempt force room brianna brochu charged misdemeanour criminal mischief breach peace police west hartford connecticut judge add felony bigotry charge greg woodward m brochu longer student campaign roommate chennel rowe apparently light via instagram bragged action published local described m rowe facebook video reportedly wrote finally yo girl rid roommate ! spitting coconut oil putting moldy clam dip lotion ... putting toothbrush sun shine finally goodbye jamaican barbie addressed student mr woodward m brochu action reprehensible incident deeply disturbing morning brianna brochu longer student hartford returning institution m brochu appeared hartford court wednesday morning comment brief appearance facebook video m rowe felt unwanted disrespected m brochu moving room described becoming sick suffered extreme throat pain alleged connected m brochu action looking paragraph paragraph stuff using toothbrush toothbrush sun shine 1\n"
     ]
    }
   ],
   "source": [
    "filename = open('/home/yuting/PycharmProjects/data/dssm_test_dev.csv')\n",
    "reader = csv.reader(filename)\n",
    "i = 0\n",
    "for row in reader:\n",
    "    corpus = ' '.join(row)\n",
    "    print(corpus)\n",
    "    i += 1\n",
    "    if i >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [['sdf','dfwer','dfs'],['derw','ewrc5']]\n",
    "a[1] = ' '.join(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [\"sentence1\",\"sentence2\",\"label\"]\n",
    "with open('/home/yuting/PycharmProjects/data/dssm_test_train.csv','rb') as reader, open('/home/yuting/PycharmProjects/data/dssm_test_train_sub.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(h)\n",
    "    for index, line in enumerate(reader):\n",
    "        if (index-1) % 10 == 0:\n",
    "            writer.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = \"sentence1\"+','+\"sentence2\"+','+\"label\"+'\\n'\n",
    "with open('/home/yuting/PycharmProjects/data/dssm_test_dev.csv','r') as reader:\n",
    "    with open('/home/yuting/PycharmProjects/data/dssm_test_dev_sub.txt','w') as writer:\n",
    "        writer.write(h)\n",
    "        for index, line in enumerate(reader):\n",
    "            if (index-1)%10==0:\n",
    "                writer.write(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
