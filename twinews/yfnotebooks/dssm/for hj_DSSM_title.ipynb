{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from databasetools.mongo import *\n",
    "from newstools.goodarticle.utils import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.news import parser as newsParser\n",
    "from machinelearning.iterator import *\n",
    "from twinews.utils import *\n",
    "from twinews.models.ranking import *\n",
    "from twinews.evaluation.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptools.basics import *\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "import gensim\n",
    "from math import log2\n",
    "from math import sqrt\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twinews.yfnotebooks.dssm.graph import Graph\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from twinews.yfnotebooks.load_data import hashIndex,load_hashed_data,char_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()\n",
    "TEST = isNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tictoc starts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/dssm_title.log\") if isNotebook else Logger(\"dssm_title-\" + getHostname() + \".log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "modelName = \"DSSM_title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL’\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = \\\n",
    "{\n",
    "    'splitVersion': 2,\n",
    "    'testVersion':2, # with some parameters changed\n",
    "    \n",
    "    # for input data\n",
    "    'maxUsers': 30 if TEST else None, # Sub-sampling\n",
    "    'maxDocuments': None,\n",
    "    'useExtraNews': 0 if TEST else None, # None = unlimited, 0 = no extra news\n",
    "    'minDF': 1 / 2000, # Remove words that have a document frequency ratio lower than 1 / 2000\n",
    "    'maxDF': 20, # Remove top 20 voc elements\n",
    "    'lowercase': True,\n",
    "    'doLemmatization': True,\n",
    "\n",
    "    \n",
    "   # for model    \n",
    "    'seq_length' : 100,\n",
    "#     'char_embedding_size': 100,\n",
    "    'learning_rate': 0.0005,\n",
    "    'keep_prob': 0.7,\n",
    "    'vocab_size': 3029,\n",
    "    'class_size': 2,\n",
    "    'epochs': 100,\n",
    "    'batch_size': 1000,\n",
    "    \n",
    "    # for ouput\n",
    "    'similarity': 'cosine',\n",
    "\n",
    "#     'historyRef': 0.3, # 1, 1.0, 0.5, 0.3, 3, 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> tic: 29.58s | message: Eval data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{ candidates, extraNews, meta, testNews, testUsers, trainNews, trainUsers }\n",
      "{ 'created': 2020.03.24-14.28.06, 'endDate': 2018-01-15, 'id': 2, 'ranksLength': 1000, 'splitDate': 2017-12-25, 'startDate': 2017-10-01, 'testMaxNewsPerUser': 97, 'testMeanNewsPerUser': 7.22, 'testMinNewsPerUser': 2, 'testNewsCount': 71781, 'totalNewsAvailable': 570210, 'trainMaxNewsPerUser': 379, 'trainMeanNewsPerUser': 26.48, 'trainMinNewsPerUser': 8, 'trainNewsCount': 237150, 'usersCount': 15905 }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> toc total duration: 29.81s | message: Got Twinews evaluation data\n"
     ]
    }
   ],
   "source": [
    "# Getting users and news\n",
    "evalData = getEvalData(config['splitVersion'], maxExtraNews=0,\n",
    "                       maxUsers=config['maxUsers'])\n",
    "(trainUsers, testUsers, trainNews, testNews, candidates, extraNews) = \\\n",
    "(evalData['trainUsers'], evalData['testUsers'], evalData['trainNews'],\n",
    " evalData['testNews'], evalData['candidates'], evalData['extraNews'])\n",
    "bp(evalData.keys(), 5, logger)\n",
    "log(b(evalData['meta'], 5), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainNewsList = shuffle(list(trainNews), seed=0)\n",
    "testNewsList = shuffle(list(testNews), seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25254 urls for newsList:\n",
      "[\n",
      "  http://www.wcpo.com/news/local-news/kenton-county/independence/five-critically-injured-in-kenton-cou,\n",
      "  https://www.sevendaysvt.com/vermont/how-drug-treatment-policies-in-vermont-prisons-contribute-to-the,\n",
      "  ...,\n",
      "  http://ew.com/movies/2018/01/09/jacob-tremblay-wonder-prosthetic-transformation-photos/,\n",
      "  https://jezebel.com/elon-musk-confirms-he-was-at-a-sex-party-and-didnt-even-1822009703\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "newsList = trainNewsList + testNewsList\n",
    "log(str(len(newsList)) + \" urls for newsList:\\n\" + b(newsList), logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100% [====================] (total duration: 14m 4.009s, mean duration: 0.033s)\n"
     ]
    }
   ],
   "source": [
    "# get all the title to form the (url: title) dict \n",
    "titles = getNewsTitles(newsList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing   0% [                    ]\n",
      "tokenizing   9% [=                   ] (2.34s left)\n",
      "tokenizing  19% [===                 ] (2.28s left)\n",
      "tokenizing  29% [=====               ] (2.077s left)\n",
      "tokenizing  39% [=======             ] (1.815s left)\n",
      "tokenizing  49% [=========           ] (1.53s left)\n",
      "tokenizing  59% [===========         ] (1.233s left)\n",
      "tokenizing  69% [=============       ] (0.926s left)\n",
      "tokenizing  79% [===============     ] (0.62s left)\n",
      "tokenizing  89% [=================   ] (0.31s left)\n",
      "tokenizing  99% [=================== ] (0s left)\n",
      "tokenizing 100% [====================] (total duration: 3.11s, mean duration: 0s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [ Police, :, ..., ,, OH ], [ How, Drug, ..., Independent, Voice ], ..., [ See, Jacob, ..., |, EW.com ], [ Elon, Musk, ..., Know, It ] ]\n"
     ]
    }
   ],
   "source": [
    "# get tokenized title\n",
    "titlesToken = []\n",
    "for i in pb(list(range(len(titles))), logger=logger, message=\"tokenizing\"):\n",
    "    titlesToken.append(wordTokenize(titles[i]))\n",
    "bp(titlesToken,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lower casing   0% [                    ]\n",
      "Lower casing   9% [=                   ] (0.36s left)\n",
      "Lower casing  19% [===                 ] (0.32s left)\n",
      "Lower casing  29% [=====               ] (0.28s left)\n",
      "Lower casing  39% [=======             ] (0.24s left)\n",
      "Lower casing  49% [=========           ] (0.2s left)\n",
      "Lower casing  59% [===========         ] (0.16s left)\n",
      "Lower casing  69% [=============       ] (0.124s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lower casing  79% [===============     ] (0.082s left)\n",
      "Lower casing  89% [=================   ] (0.041s left)\n",
      "Lower casing  99% [=================== ] (0s left)\n",
      "Lower casing 100% [====================] (total duration: 0.41s, mean duration: 0s)\n",
      "[ [ police, :, ..., ,, oh ], [ how, drug, ..., independent, voice ], ..., [ see, jacob, ..., |, ew.com ], [ elon, musk, ..., know, it ] ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# lowercase\n",
    "for i in pb(list(range(len(titlesToken))), logger=logger, message=\"Lower casing\"):\n",
    "    if titlesToken[i] == None:\n",
    "        # print(titlesToken[i])\n",
    "        continue\n",
    "    else:\n",
    "        for u in range(len(titlesToken[i])):\n",
    "            titlesToken[i][u] = titlesToken[i][u].lower()\n",
    "bp(titlesToken, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatization   0% [                    ]\n",
      "Lemmatization   9% [=                   ] (1.17s left)\n",
      "Lemmatization  19% [===                 ] (1.08s left)\n",
      "Lemmatization  29% [=====               ] (0.933s left)\n",
      "Lemmatization  39% [=======             ] (0.81s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatization  49% [=========           ] (0.67s left)\n",
      "Lemmatization  59% [===========         ] (0.54s left)\n",
      "Lemmatization  69% [=============       ] (0.403s left)\n",
      "Lemmatization  79% [===============     ] (0.27s left)\n",
      "Lemmatization  89% [=================   ] (0.135s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatization  99% [=================== ] (0s left)\n",
      "[ [ police, :, ..., ,, oh ], [ how, drug, ..., independent, voice ], ..., [ see, jacob, ..., |, ew.com ], [ elon, musk, ..., know, it ] ]\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pbar = ProgressBar(len(titlesToken), logger=logger, message=\"Lemmatization\")\n",
    "for i in range(len(titlesToken)):\n",
    "    if titlesToken[i] == None:\n",
    "        # print(titlesToken[i])\n",
    "        continue\n",
    "    else:\n",
    "        for u in range(len(titlesToken[i])):\n",
    "            titlesToken[i][u] = lemmatizer.lemmatize(titlesToken[i][u])\n",
    "    pbar.tic()\n",
    "bp(titlesToken, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\n",
      "  http://247wallst.com/healthcare-business/2017/12/30/30-big-biotech-events-coming-in-2018/3/: [ 30, big, ..., wall, st. ],\n",
      "  http://247wallst.com/special-report/2017/12/08/americas-25-dying-industries-2: [ america, 's, ..., wall, st. ],\n",
      "  http://247wallst.com/special-report/2018/01/11/25-best-fitness-tech-gadgets/: [ 25, best, ..., wall, st. ],\n",
      "  http://a.msn.com/00/en-us/AAtmQOZ?ocid=st: [ america, 's, ..., live, in ],\n",
      "  http://a.msn.com/00/en-us/AAuyJLc?ocid=st: [ what, living, ..., motley, fool ],\n",
      "  ...,\n",
      "  https://xtinaluvspink.wordpress.com/2016/01/17/vegan-chickpea-barley-soup-for-two/: [ vegan, chickpea, ..., healthy, ! ],\n",
      "  https://yaledailynews.com/blog/2018/01/06/yale-psychiatrist-met-with-congressmen-about-trumps-mental-health/: [ yale, psychiatrist, ..., mental, health ],\n",
      "  https://zdubbzattmom.wordpress.com/2018/01/03/ex-libris-the-eyes-of-madness-presents-2017s-bakers-dozen-zakks-favorite-reads-of-the-year/: [ ex, libris, ..., of, madness ],\n",
      "  https://zdubbzattmom.wordpress.com/2018/01/03/ex-libris-the-eyes-of-madness-presents-2017s-bakers-dozen-zakks-favorite-reads-of-the-year/r: [ rating, and, ..., of, madness ],\n",
      "  https://zouxzoux.wordpress.com/2018/01/11/interview-with-julie-kane-poet-co-editor-of-nasty-women-poets-poetry-writing-women/#more-6733: [ interview, with, ..., –, zouxzoux ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# the (url:title) dict\n",
    "urlTitles= dict()\n",
    "for i in range(len(titlesToken)):\n",
    "    urlTitles[newsList[i]] = titlesToken[i]\n",
    "bp(urlTitles, 2, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ !, #, ..., •, … ]\n"
     ]
    }
   ],
   "source": [
    "# loading the vocab of title words\n",
    "titleVocab = []\n",
    "with open('/home/yuting/PycharmProjects/data/title_vocab.txt','r') as file:\n",
    "    for line in file.readlines():\n",
    "        titleVocab.append(line.strip('\\n'))\n",
    "\n",
    "bp(titleVocab,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainUsers.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build user history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "build user history   3% [                    ]\n",
      "build user history  10% [==                  ] (1.53s left)\n",
      "build user history  20% [====                ] (1.359s left)\n",
      "build user history  30% [======              ] (1.19s left)\n",
      "build user history  40% [========            ] (1.02s left)\n",
      "build user history  50% [==========          ] (0.85s left)\n",
      "build user history  60% [============        ] (0.686s left)\n",
      "build user history  70% [==============      ] (0.514s left)\n",
      "build user history  80% [================    ] (0.342s left)\n",
      "build user history  90% [==================  ] (0.171s left)\n",
      "build user history 100% [====================] (total duration: 1.71s, mean duration: 0.057s)\n"
     ]
    }
   ],
   "source": [
    "# for multiple users\n",
    "trainUserQuery = dict()\n",
    "pbar = ProgressBar(len(trainUsers.keys()), logger=logger, message=\"build user history\")\n",
    "\n",
    "for usr in trainUsers.keys():\n",
    "    queryFinal = []\n",
    "    \n",
    "    # for each user\n",
    "    # choose 15 urls\n",
    "    if len(list(trainUsers[usr])) < 15:\n",
    "        query = list(trainUsers[usr])\n",
    "    else:\n",
    "        query = random.sample(list(trainUsers[usr]),15)\n",
    "        \n",
    "    # put 15 titles together, using extend\n",
    "    temp = []\n",
    "    for url in query:\n",
    "        if urlTitles[url] == None:\n",
    "            continue\n",
    "        else:\n",
    "            temp.extend(urlTitles[url])\n",
    "            \n",
    "    # the following two steps are making the query titles more simplified and efficient          \n",
    "    # filter what is not in the titlevocab\n",
    "    tempCompact = []\n",
    "    for i in range(len(temp)):\n",
    "        if temp[i] in titleVocab:\n",
    "            tempCompact.append(temp[i])\n",
    "    \n",
    "    # remove duplicated & detokenize\n",
    "    queryCompact = detokenize(list(set(tempCompact)))\n",
    "    \n",
    "    queryFinal.append(queryCompact)\n",
    "    trainUserQuery[usr] = queryFinal\n",
    "    \n",
    "    pbar.tic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSimilarity(p,h,y):\n",
    "    \n",
    "    tf.reset_default_graph() \n",
    "    model = Graph()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # restore the trained model\n",
    "    with tf.Session()as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver.restore(sess, '/home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt')\n",
    "        loss, acc, logit = sess.run([model.loss, model.acc, model.logits],\n",
    "                             feed_dict={model.p: p,\n",
    "                                        model.h: h,\n",
    "                                        model.y: y,\n",
    "                                        model.keep_prob: 1})\n",
    "\n",
    "        #print('loss: ', loss, ' acc:', acc)\n",
    "        return logit\n",
    "#logit = computeSimilarity(p,h,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing   3% [                    ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  10% [==                  ] (23m 28.5s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  20% [====                ] (20m 52.4s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  30% [======              ] (18m 14.613s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  40% [========            ] (15m 38.37s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  50% [==========          ] (13m 2s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  60% [============        ] (10m 25.439s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  70% [==============      ] (7m 49.002s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  80% [================    ] (5m 12.615s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing  90% [==================  ] (2m 36.317s left)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n",
      "INFO:tensorflow:Restoring parameters from /home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks/dssm/output/dssm_99.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing 100% [====================] (total duration: 26m 3.319s, mean duration: 52.11s)\n"
     ]
    }
   ],
   "source": [
    "# rankings of the candidates with logits output result\n",
    "pbar = ProgressBar(len(trainUsers.keys()), logger=logger, message=\"testing\")\n",
    "rankings = dict()\n",
    "for usr in trainUsers.keys():\n",
    "    usrRankings = []\n",
    "    \n",
    "    # for each user\n",
    "    # get historical data as query(p)\n",
    "    query = trainUserQuery[usr] * 1000\n",
    "    \n",
    "    for candidates in evalData['candidates'][usr]:\n",
    "        candidates = list(candidates)\n",
    "        # get 1000 h as a list(h)\n",
    "        h_temp = []\n",
    "        for url in candidates:\n",
    "            h_temp.append(detokenize(urlTitles[url]))\n",
    "        # get p,h\n",
    "        p,h = char_index(query,h_temp)\n",
    "        y = np.zeros((1000))                    # whatever initialization\n",
    "        \n",
    "        scoresCandidates = []\n",
    "        logit = computeSimilarity(p,h,y)            # get the 1000 scores list\n",
    "        scoresCandidates = zip(candidates,list(logit[:,1]))\n",
    "        ranking = sortBy(scoresCandidates, index=1, desc=True)\n",
    "        ranking = [e[0] for e in ranking]\n",
    "        usrRankings.append(ranking)\n",
    "    rankings[usr] = usrRankings\n",
    "    \n",
    "    pbar.tic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkRankings(rankings,evalData['candidates'],maxUsers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "addRanking(modelName, rankings, config, logger=logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
