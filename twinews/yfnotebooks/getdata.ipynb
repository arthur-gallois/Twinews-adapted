{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from databasetools.mongo import *\n",
    "from newstools.goodarticle.utils import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.news import parser as newsParser\n",
    "from machinelearning.iterator import *\n",
    "from twinews.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pymongo\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twinews users (version 1.0) initialised.\n"
     ]
    }
   ],
   "source": [
    "ConnUsers = getUsersCollection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twinews news (version 1.0) initialised.\n"
     ]
    }
   ],
   "source": [
    "ConnNews = getNewsCollection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def readMongo(con,myquery):\n",
    "    output = []\n",
    "    try:\n",
    "        #myquery = {} # 查询条件\n",
    "        for item in con.find(myquery):\n",
    "            del item['_id']\n",
    "            output.append(item)\n",
    "        print ('data reading finished')\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJson(con, myquery,table_name):\n",
    "    dataDir = '/home/yuting/PycharmProjects/data'\n",
    "    data = readMongo(con,myquery)\n",
    "    with open(dataDir + \"/\" + table_name + '.json', 'w', encoding=\"UTF-8\") as jf:\n",
    "        jf.write(json.dumps(data, indent=2))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading finished\n"
     ]
    }
   ],
   "source": [
    "getJson(ConnNews,{},'news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading finished\n"
     ]
    }
   ],
   "source": [
    "getJson(ConnUsers,{{},{\"user_id\":1},{\"news\":1}},'users_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tictoc starts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/dssm_yf.log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get training data urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "userHistory = {}\n",
    "negativeNewList = []\n",
    "\n",
    "# read user with more than 30 news\n",
    "for row in ConnUsers.find().limit(100):\n",
    "    if len(row[\"news\"]) >= 30:\n",
    "        userHistory[row[\"user_id\"]] = row[\"news\"]\n",
    "# get news for negative sampling     \n",
    "for row in ConnNews.find().limit(1000):\n",
    "    negativeNewList.append(row[\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataUrls = [] # composed of user_id, 20 history as query, 10 history as positive doc, 10 from neg as negative doc\n",
    "\n",
    "for k in userHistory.keys():\n",
    "    query20 = random.sample(userHistory[k],20)\n",
    "    doc10 = random.sample(list(set(userHistory[k]).difference(set(query20))),10)\n",
    "    dataUrls.append([k,query20,doc10])\n",
    "    \n",
    "assert len(negativeNewList)//len(userHistory.keys()) >= 10   #make sure we have enough negative sample\n",
    "\n",
    "for k in range(len(dataUrls)):\n",
    "    try:\n",
    "        negTemp = []\n",
    "        #if len(negativeNewList) > 10:\n",
    "        negTemp = random.sample(list(set(negativeNewList).difference(set(userHistory[dataUrls[k][0]]))),10)\n",
    "        dataUrls[k].append(negTemp)\n",
    "        negativeNewList = list(set(negativeNewList)^set(negTemp))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/yuting/PycharmProjects/data/dssm_test', 'w', newline='') as csvfile:\n",
    "    writer  = csv.writer(csvfile)\n",
    "    for row in dataUrls:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sampling and labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryList = []\n",
    "posList = []\n",
    "negList = []\n",
    "\n",
    "for i in range(len(dataUrls)):\n",
    "#     queryList.append(dataUrls[i][1])\n",
    "#     posList.append(dataUrls[i][2])\n",
    "#     negList.append(dataUrls[i][3])\n",
    "    for j in range(len(dataUrls[i][1])):\n",
    "        queryList.append(dataUrls[i][1][j])\n",
    "    for k in range(len(dataUrls[i][2])):\n",
    "        posList.append(dataUrls[i][2][k])\n",
    "    for m in range(len(dataUrls[i][3])):\n",
    "        negList.append(dataUrls[i][3][m])\n",
    "urlsList = queryList + posList + negList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twinews news (version 1.0) initialised.\n",
      "  0% [                    ]\n",
      " 10% [==                  ] (1m 48.989s left)\n",
      " 20% [====                ] (1m 35.479s left)\n",
      " 30% [======              ] (1m 25.96s left)\n",
      " 40% [========            ] (1m 14.264s left)\n",
      " 50% [==========          ] (1m 1.27s left)\n",
      " 60% [============        ] (48.98s left)\n",
      " 70% [==============      ] (37.118s left)\n",
      " 80% [================    ] (24.327s left)\n",
      " 90% [==================  ] (11.874s left)\n",
      "100% [====================] (total duration: 1m 56.5s, mean duration: 0.044s)\n",
      "[\n",
      "  [ [ Media, playback, ..., Manhattan, . ], [ A, -, ..., arrested, . ], ..., [ Please, include, ..., journalist, . ], [ You, can, ..., ways, : ] ],\n",
      "  [ [ Now, ,, ..., long, . ], [ After, more, ..., life, . ], ..., [ \", But, ..., schedule, . ], [ Not, when, ..., calling, . ] ],\n",
      "  ...,\n",
      "  [ [ TAMPA, ,, ..., morning, . ], [ The, Hillsborough, ..., Times, . ], ..., [ Email, __email__, ..., feed, . ], [ Chart, :, ..., school, year ] ],\n",
      "  [ [ (, WSAV, ..., week, . ], [ Tuesday, morning, ..., Walk, . ], ..., [ \", My, ..., said, . ], [ He, also, ..., sick, . ] ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentences = getNewsSentences(urlsList, logger=logger)\n",
    "bp(sentences, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ [ Media, playback, ..., ways, : ], [ Now, ,, ..., calling, . ], ..., [ TAMPA, ,, ..., school, year ], [ (, WSAV, ..., sick, . ] ]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = flattenLists(sentences[i])\n",
    "docs = sentences\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lower casing   0% [                    ]\n",
      "Lower casing  10% [==                  ] (0.809s left)\n",
      "Lower casing  20% [====                ] (0.639s left)\n",
      "Lower casing  30% [======              ] (0.536s left)\n",
      "Lower casing  40% [========            ] (0.464s left)\n",
      "Lower casing  50% [==========          ] (0.38s left)\n",
      "Lower casing  60% [============        ] (0.306s left)\n",
      "Lower casing  70% [==============      ] (0.227s left)\n",
      "Lower casing  80% [================    ] (0.144s left)\n",
      "Lower casing  90% [==================  ] (0.067s left)\n",
      "Lower casing 100% [====================] (total duration: 0.64s, mean duration: 0s)\n",
      "[ [ media, playback, ..., ways, : ], [ now, ,, ..., calling, . ], ..., [ tampa, ,, ..., school, year ], [ (, wsav, ..., sick, . ] ]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    for i in pb(list(range(len(docs))), logger=logger, message=\"Lower casing\"):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = docs[i][u].lower()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatization   0% [                    ]\n",
      "Lemmatization  10% [==                  ] (17.55s left)\n",
      "Lemmatization  20% [====                ] (11.08s left)\n",
      "Lemmatization  30% [======              ] (8.26s left)\n",
      "Lemmatization  40% [========            ] (6.645s left)\n",
      "Lemmatization  50% [==========          ] (5.16s left)\n",
      "Lemmatization  60% [============        ] (4s left)\n",
      "Lemmatization  70% [==============      ] (2.91s left)\n",
      "Lemmatization  80% [================    ] (1.825s left)\n",
      "Lemmatization  90% [==================  ] (0.848s left)\n",
      "Lemmatization 100% [====================] (total duration: 7.96s, mean duration: 0.003s)\n",
      "[ [ medium, playback, ..., way, : ], [ now, ,, ..., calling, . ], ..., [ tampa, ,, ..., school, year ], [ (, wsav, ..., sick, . ] ]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pbar = ProgressBar(len(docs), logger=logger, message=\"Lemmatization\")\n",
    "    for i in range(len(docs)):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = lemmatizer.lemmatize(docs[i][u])\n",
    "        pbar.tic()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Voc removed because of minDF (21854 elements):\n",
      "{ #eaglesparade, 0, 0comments, 100bn, 100gm, 103rd, 1040x, 105mt, 109th, 10am, ..., zwillich, zwonitzer, zynga, zyprexa, zyxel, 😁, 😂, 😉, 😒, 😢 }\n",
      "Voc removed because of maxDF (300 elements):\n",
      "{ \", ', (, ), ,, -, ., :, ;, ?, ..., woman, work, working, world, would, year, yet, york, you, your }\n",
      "44.09% of voc will be removed.\n",
      "[ [ medium, playback, ..., contact, following ], [ spirited, race, ..., ranch, calling ], ..., [ tampa, fla., ..., salary, county ], [ wsav, hilton, ..., drop, sick ] ]\n"
     ]
    }
   ],
   "source": [
    "docs = filterCorpus(docs, minDF=1/2000, maxDF=300,\n",
    "                    removeEmptyDocs=False, allowEmptyDocs=False, logger=logger)\n",
    "for doc in docs: assert len(doc) > 0\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [['sdf','dfwer','dfs'],['derw','ewrc5']]\n",
    "a[1] = ' '.join(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sdf', 'dfwer', 'dfs'], 'derw ewrc5']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toStr(sentencesList):\n",
    "    for i in range(len(sentencesList)):\n",
    "        sentencesList[i] = ' '.join(sentencesList[i])\n",
    "    return sentencesList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\n",
      "  medium playback unsupported device medium caption terror attack event unfolded eight killed seriousl,\n",
      "  spirited race succeed underway talk turned mr. brown chapter resilience imprint identified mr. brown,\n",
      "  ...,\n",
      "  tampa fla. hundred jefferson student walked class protested teacher salary increase wednesday mornin,\n",
      "  wsav hilton head robotics team officially renowned team won international robotics competition mit t\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "corpus = toStr(docs)\n",
    "for corp in corpus: assert len(corp) > 0\n",
    "bp(corpus,logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ' '.join(corpus)\n",
    "with open('/home/yuting/PycharmProjects/data/dssm_test_corpus.txt', 'a') as f:\n",
    "    f.write(c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\n",
      "  http://a.msn.com/00/en-us/BBFZDuB?ocid=st: stock went wild ride friday falling sharply michael flynn directed talk russian abc reported flynn s,\n",
      "  http://a.msn.com/01/en-us/BBIVxj7?ocid=st: worker ice deport immigrant quit courtesy daisy robert quit helena mont. robert immediately reservat,\n",
      "  http://a.msn.com/0B/en-us/AAuMl9k?ocid=st: widow eagle guitarist frey sue nyc hospital wrongful death popular breakup song michael loccisano ge,\n",
      "  http://abc6.wjbf.com/2AEprPm: drew foundation host night drew copyright wjbf reserved video augusta ga. wjbf drew legacy baseball ,\n",
      "  http://abc6.wjbf.com/2CgdJM1: evans host 12th annual monterrey christmas tournament copyright wjbf reserved video evans ga. wjbf e,\n",
      "  ...,\n",
      "  https://www.yourtango.com/experts/shireen-yates/safe-holiday-eating-tips-while-gluten-free-diet: communicating dietary advance alleviate tension everyone involved holiday packed social gathering pl,\n",
      "  https://www.zerohedge.com/news/2017-01-02/currency-collapse-roman-empire: peak roman empire held span __float_1__ square mile rome conquered known empire built mile road amph,\n",
      "  https://www.zerohedge.com/news/2018-02-11/jpmorgan-publishes-bitcoin-bible: jamie dimon infamous outburst ceo bitcoin fraud threatened jpmorgan trader caught trading cryptocurr,\n",
      "  https://zerohedge.com/news/2017-12-28/uber-no-longer-worlds-most-valuable-startup: reported disappointing inevitable development valuable startup softbank investor offer buy share ube,\n",
      "  https://zwischenzugs.wordpress.com/2017/10/15/my-20-year-experience-of-software-development-methodologies/: sapiens collective fiction recently read sapiens brief history humankind basic thesis book human req\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "urlDocs = dict()\n",
    "for i in range(len(urlsList)):\n",
    "    urlDocs[urlsList[i]] = corpus[i]\n",
    "\n",
    "bp(urlDocs, 2, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [\"sentence1\",\"sentence2\",\"label\"]\n",
    "with open('/home/yuting/PycharmProjects/data/dssm_test_train.csv', 'w', newline='') as csvfile:\n",
    "    writer  = csv.writer(csvfile)\n",
    "    writer.writerow(h)\n",
    "    for i in range(len(dataUrls)):\n",
    "        query = \"\"\n",
    "        for j in range(len(dataUrls[i][1])):\n",
    "            query += urlDocs[dataUrls[i][1][j]]\n",
    "            for k in range(len(dataUrls[i][2])):\n",
    "                posSamp = []\n",
    "                pos = urlDocs[dataUrls[i][2][k]]\n",
    "                posSamp.append(query)\n",
    "                posSamp.append(pos)\n",
    "                posSamp.append(\"1\")\n",
    "                writer.writerow(posSamp)\n",
    "            for m in range(len(dataUrls[i][3])):\n",
    "                negSamp = []\n",
    "                neg = urlDocs[dataUrls[i][3][m]]\n",
    "                negSamp.append(query)\n",
    "                negSamp.append(neg)\n",
    "                negSamp.append(\"0\")\n",
    "                writer.writerow(negSamp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [\"sentence1\",\"sentence2\",\"label\"]\n",
    "with open('/home/yuting/PycharmProjects/data/dssm_test_train.csv','rb') as reader, open('/home/yuting/PycharmProjects/data/dssm_test_train_sub.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(h)\n",
    "    for index, line in enumerate(reader):\n",
    "        if (index-1) % 10 == 0:\n",
    "            writer.writerow(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = \"sentence1\"+','+\"sentence2\"+','+\"label\"+'\\n'\n",
    "with open('/home/yuting/PycharmProjects/data/dssm_test_dev.csv','r') as reader:\n",
    "    with open('/home/yuting/PycharmProjects/data/dssm_test_dev_sub.txt','w') as writer:\n",
    "        writer.write(h)\n",
    "        for index, line in enumerate(reader):\n",
    "            if (index-1)%10==0:\n",
    "                writer.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dialect',\n",
       " 'DictReader',\n",
       " 'DictWriter',\n",
       " 'Error',\n",
       " 'OrderedDict',\n",
       " 'QUOTE_ALL',\n",
       " 'QUOTE_MINIMAL',\n",
       " 'QUOTE_NONE',\n",
       " 'QUOTE_NONNUMERIC',\n",
       " 'Sniffer',\n",
       " 'StringIO',\n",
       " '_Dialect',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'excel',\n",
       " 'excel_tab',\n",
       " 'field_size_limit',\n",
       " 'get_dialect',\n",
       " 'list_dialects',\n",
       " 're',\n",
       " 'reader',\n",
       " 'register_dialect',\n",
       " 'unix_dialect',\n",
       " 'unregister_dialect',\n",
       " 'writer']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 4, 5]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(b).difference(set(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1 sentence2 label\n",
      "challenge team unveiling innovation tackle glass ceiling along apps wearable tech necklace team created eye catching art installation happened road tested idea silicon valley bbc influential inspirational challenging tackle biggest facing glass ceiling female illiteracy harassment space sexism sport coming real solution involved idea facebook instagram twitter 100women student facing hate crime charge admitting smearing bodily fluid black roommate possession apparent attempt force room brianna brochu charged misdemeanour criminal mischief breach peace police west hartford connecticut judge add felony bigotry charge greg woodward m brochu longer student campaign roommate chennel rowe apparently light via instagram bragged action published local described m rowe facebook video reportedly wrote finally yo girl rid roommate ! spitting coconut oil putting moldy clam dip lotion ... putting toothbrush sun shine finally goodbye jamaican barbie addressed student mr woodward m brochu action reprehensible incident deeply disturbing morning brianna brochu longer student hartford returning institution m brochu appeared hartford court wednesday morning comment brief appearance facebook video m rowe felt unwanted disrespected m brochu moving room described becoming sick suffered extreme throat pain alleged connected m brochu action looking paragraph paragraph stuff using toothbrush toothbrush sun shine 1\n"
     ]
    }
   ],
   "source": [
    "filename = open('/home/yuting/PycharmProjects/data/dssm_test_dev.csv')\n",
    "reader = csv.reader(filename)\n",
    "i = 0\n",
    "for row in reader:\n",
    "    corpus = ' '.join(row)\n",
    "    print(corpus)\n",
    "    i += 1\n",
    "    if i >= 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
