{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from databasetools.mongo import *\n",
    "from newstools.goodarticle.utils import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.news import parser as newsParser\n",
    "from machinelearning.iterator import *\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/yuting/PycharmProjects/Twinews/twinews/yfnotebooks', '/home/yuting/anaconda3/lib/python37.zip', '/home/yuting/anaconda3/lib/python3.7', '/home/yuting/anaconda3/lib/python3.7/lib-dynload', '', '/home/yuting/anaconda3/lib/python3.7/site-packages', '/home/yuting/PycharmProjects/Twinews', '/home/yuting/anaconda3/lib/python3.7/site-packages/shap-0.34.0-py3.7-linux-x86_64.egg', '/home/yuting/anaconda3/lib/python3.7/site-packages/IPython/extensions', '/home/yuting/.ipython', '/home/yuting/PycharmProjects/Twinews']\n"
     ]
    }
   ],
   "source": [
    "import sys ; sys.path.append('/home/yuting/PycharmProjects/Twinews')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from systemtools.printer import bp\n",
    "from twinews.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twinews users (version 1.0) initialised.\n"
     ]
    }
   ],
   "source": [
    "ConnUsers = getUsersCollection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twinews news (version 1.0) initialised.\n"
     ]
    }
   ],
   "source": [
    "ConnNews = getNewsCollection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# json dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def readMongo(con,myquery):\n",
    "    output = []\n",
    "    try:\n",
    "        #myquery = {} # 查询条件\n",
    "        for item in con.find(myquery):\n",
    "            del item['_id']\n",
    "            output.append(item)\n",
    "        print ('data reading finished')\n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getJson(con, myquery,table_name):\n",
    "    dataDir = '/home/yuting/PycharmProjects/data'\n",
    "    data = readMongo(con,myquery)\n",
    "    with open(dataDir + \"/\" + table_name + '.json', 'w', encoding=\"UTF-8\") as jf:\n",
    "        jf.write(json.dumps(data, indent=2))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading finished\n"
     ]
    }
   ],
   "source": [
    "getJson(ConnNews,{},'news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data reading finished\n"
     ]
    }
   ],
   "source": [
    "getJson(ConnUsers,{{},{\"user_id\":1},{\"news\":1}},'users_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tictoc starts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/dssm_yf.log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get training data urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "userHistory = {}\n",
    "negativeNewList = []\n",
    "\n",
    "# read user with more than 30 news\n",
    "for row in ConnUsers.find().limit(10):\n",
    "    if len(row[\"news\"]) >= 30:\n",
    "        userHistory[row[\"user_id\"]] = row[\"news\"]\n",
    "# get news for negative sampling     \n",
    "for row in ConnNews.find().limit(100):\n",
    "    negativeNewList.append(row[\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataUrls = [] # composed of user_id, 20 history as query, 10 history as positive doc, 10 from neg as negative doc\n",
    "\n",
    "for k in userHistory.keys():\n",
    "    query20 = random.sample(userHistory[k],20)\n",
    "    doc10 = random.sample(list(set(userHistory[k]).difference(set(query20))),10)\n",
    "    dataUrls.append([k,query20,doc10])\n",
    "    \n",
    "assert len(negativeNewList)//len(userHistory.keys()) >= 10   #make sure we have enough negative sample\n",
    "\n",
    "for k in range(len(dataUrls)):\n",
    "    try:\n",
    "        negTemp = []\n",
    "        #if len(negativeNewList) > 10:\n",
    "        negTemp = random.sample(list(set(negativeNewList).difference(set(userHistory[dataUrls[k][0]]))),10)\n",
    "        dataUrls[k].append(negTemp)\n",
    "        negativeNewList = list(set(negativeNewList)^set(negTemp))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/yuting/PycharmProjects/data/dssm_test', 'w', newline='') as csvfile:\n",
    "    writer  = csv.writer(csvfile)\n",
    "    for row in dataUrls:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sampling and labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryList = []\n",
    "posList = []\n",
    "negList = []\n",
    "\n",
    "for i in range(len(dataUrls)):\n",
    "#     queryList.append(dataUrls[i][1])\n",
    "#     posList.append(dataUrls[i][2])\n",
    "#     negList.append(dataUrls[i][3])\n",
    "    for j in range(len(dataUrls[i][1])):\n",
    "        queryList.append(dataUrls[i][1][j])\n",
    "    for k in range(len(dataUrls[i][2])):\n",
    "        posList.append(dataUrls[i][2][k])\n",
    "    for m in range(len(dataUrls[i][3])):\n",
    "        negList.append(dataUrls[i][3][m])\n",
    "urlsList = queryList + posList + negList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% [                    ]\n",
      " 10% [==                  ] (9.54s left)\n",
      " 20% [====                ] (8.48s left)\n",
      " 30% [======              ] (7.56s left)\n",
      " 40% [========            ] (6.375s left)\n",
      " 50% [==========          ] (5.43s left)\n",
      " 60% [============        ] (4.24s left)\n",
      " 70% [==============      ] (3.094s left)\n",
      " 80% [================    ] (2.035s left)\n",
      " 90% [==================  ] (0.993s left)\n",
      "100% [====================] (total duration: 9.67s, mean duration: 0.04s)\n",
      "[\n",
      "  'Why do so many of us, as female artists, have to go to war? 'No to opening the door to him at all h,\n",
      "  It was yells of pure joy for one school in Puerto Rico. Interested in Hurricanes? Add Hurricanes as ,\n",
      "  ...,\n",
      "  MADRID, March __int_2__ Xinhua) The Amancio Ortega Foundation announced Wednesday that it would dona,\n",
      "  HUNTINGTON (WOWK TV) printing is the future and it right here in our area. The Robert C Byrd Institu\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sentences = getNewsText(urlsList, logger=logger)\n",
    "bp(sentences, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ [ T, h, ..., m, . ], [ M, a, ..., t, . ], ..., [ B, L, ..., e, . ], [ (, W, ..., t, . ] ]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    sentences[i] = flattenLists(sentences[i])\n",
    "docs = sentences\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lower casing   0% [                    ]\n",
      "Lower casing  10% [==                  ] (0.45s left)\n",
      "Lower casing  20% [====                ] (0.439s left)\n",
      "Lower casing  30% [======              ] (0.326s left)\n",
      "Lower casing  40% [========            ] (0.285s left)\n",
      "Lower casing  50% [==========          ] (0.23s left)\n",
      "Lower casing  60% [============        ] (0.18s left)\n",
      "Lower casing  70% [==============      ] (0.137s left)\n",
      "Lower casing  80% [================    ] (0.084s left)\n",
      "Lower casing  90% [==================  ] (0.039s left)\n",
      "Lower casing 100% [====================] (total duration: 0.39s, mean duration: 0s)\n",
      "[ [ the, weinstein, ..., them, . ], [ massive, wildfires, ..., report, . ], ..., [ blacksburg, ,, ..., here, . ], [ (, wsav, ..., it, . ] ]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    for i in pb(list(range(len(docs))), logger=logger, message=\"Lower casing\"):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = docs[i][u].lower()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatization   0% [                    ]\n",
      "Lemmatization  10% [==                  ] (4.77s left)\n",
      "Lemmatization  20% [====                ] (4.16s left)\n",
      "Lemmatization  30% [======              ] (3.36s left)\n",
      "Lemmatization  40% [========            ] (2.909s left)\n",
      "Lemmatization  50% [==========          ] (2.35s left)\n",
      "Lemmatization  60% [============        ] (1.893s left)\n",
      "Lemmatization  70% [==============      ] (1.418s left)\n",
      "Lemmatization  80% [================    ] (0.894s left)\n",
      "Lemmatization  90% [==================  ] (0.421s left)\n",
      "Lemmatization 100% [====================] (total duration: 4s, mean duration: 0.002s)\n",
      "[ [ the, weinstein, ..., them, . ], [ massive, wildfire, ..., report, . ], ..., [ blacksburg, ,, ..., here, . ], [ (, wsav, ..., it, . ] ]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pbar = ProgressBar(len(docs), logger=logger, message=\"Lemmatization\")\n",
    "    for i in range(len(docs)):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = lemmatizer.lemmatize(docs[i][u])\n",
    "        pbar.tic()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Voc removed because of maxDF (300 elements):\n",
      "{ \", ', (, ), ,, -, ., :, ;, ?, ..., woman, work, working, world, would, year, yet, york, you, your }\n",
      "0.75% of voc will be removed.\n",
      "[ [ weinstein, co., ..., victim, feel ], [ massive, wildfire, ..., zee, contributed ], ..., [ blacksburg, va., ..., follow, link ], [ wsav, name, ..., definitely, worth ] ]\n"
     ]
    }
   ],
   "source": [
    "docs = filterCorpus(docs, minDF=1/2000, maxDF=300,\n",
    "                    removeEmptyDocs=False, allowEmptyDocs=False, logger=logger)\n",
    "for doc in docs: assert len(doc) > 0\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\n",
      "  http://a.msn.com/01/en-us/BBGSv8x?ocid=st: BEN MARGOT Ben Margot Associated Press __int_4__ Some of the key Uber meetings took place at the Fou,\n",
      "  http://abcn.ws/2B5RHZX: It was yells of pure joy for one school in Puerto Rico. Interested in Hurricanes? Add Hurricanes as ,\n",
      "  http://abcn.ws/2EO1Gp3: Police in Florida issued arrest warrants for nine men on Tuesday in connection with the November haz,\n",
      "  http://abcn.ws/2hAYPGp: Sen. Jeff Flake, a frequent sparring partner of President Donald Trump, continues to make enemies in,\n",
      "  http://abcn.ws/2yfx5Rl: Massive wildfires sweeping through California have killed at least __int_2__ people and damaged thou,\n",
      "  ...,\n",
      "  https://www.wkbw.com/news/new-app-will-allow-users-to-request-sexual-consent-before-dating: BUFFALO, N.Y. (WKBW) Want to request sexual consent before dating? There an app for that. It called ,\n",
      "  https://www.wkbw.com/sports/bills/2017-buffalo-bills-all-22-in-review-defensive-ends: (WKBW) For the Buffalo Bills and all but two franchises in the NFL, the reality of the offseason has,\n",
      "  https://www.wsj.com/articles/moores-law-kooky-candidates-lose-1513210820: In the interest of full disclosure: Since Nov. __int_2__ my face has appeared on nearly a score of e,\n",
      "  https://www.wsj.com/articles/the-lefts-rage-and-trumps-peril-1517530358: The State of the Union speech was good spirited, pointed, with a credible warmth for the heroes in t,\n",
      "  https://www.wsls.com/news/virginia/president-trump-s-announcement-of-new-program-could-speed-process-of-future-drone-services: BLACKSBURG, Va. - Companies like Alphabet and Amazon could be closer to testing drones for package d\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "urlDocs = dict()\n",
    "for i in range(len(urlsList)):\n",
    "    urlDocs[urlsList[i]] = sentences[i]\n",
    "\n",
    "bp(urlDocs, 2, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = [\"sentence1\",\"sentence2\",\"label\"]\n",
    "with open('/home/yuting/PycharmProjects/data/dssm_test_dev.csv', 'w', newline='') as csvfile:\n",
    "    writer  = csv.writer(csvfile)\n",
    "    writer.writerow(h)\n",
    "    for i in range(len(dataUrls)):\n",
    "        query = \"\"\n",
    "        for j in range(len(dataUrls[i][1])):\n",
    "            query += urlDocs[dataUrls[i][1][j]]\n",
    "            for k in range(len(dataUrls[i][2])):\n",
    "                posSamp = []\n",
    "                pos = urlDocs[dataUrls[i][2][k]]\n",
    "                posSamp.append(query)\n",
    "                posSamp.append(pos)\n",
    "                posSamp.append(\"1\")\n",
    "                writer.writerow(posSamp)\n",
    "            for m in range(len(dataUrls[i][3])):\n",
    "                negSamp = []\n",
    "                neg = urlDocs[dataUrls[i][3][m]]\n",
    "                negSamp.append(query)\n",
    "                negSamp.append(neg)\n",
    "                negSamp.append(\"0\")\n",
    "                writer.writerow(negSamp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 4, 5]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(b).difference(set(a)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2, 4, 5, 9}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(b)^set(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(a) ^ set(b))()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
