{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beyond accuracy evaluation (bacceval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    " * Diversity at 100 (div@100) : on prend seulement les 100 premiers elements du ranking et on calcul la diversité de ceux-ci en terme de topic, de TFIDF et de style. Donc la moyenne des pairwise distances $$diversity(R) = \\frac{\\sum_{i=1}^{|R|}\\sum_{j=i+1}^{|R|} dist(R_i, R_j)}{\\frac{{|R|}^2-{|R|}}{2}}$$\n",
    " With R a set of recommendation lists. On ne prend que les 100 premier car si on prennait les 1000, alors tous les modèles auraient la même diversité.\n",
    " On utilise les representation vectorielles de TFIDF, style et topic avec la cosine distance. Une quatrieme diversity se base sur la moyenne des distance de jaccard. Attention cette distance c'est pas [l'extension à n ensembles comme décrit sur wikipedia](https://fr.wikipedia.org/wiki/Indice_et_distance_de_Jaccard) (car l'intersection de bcp de document donnera simplement un ensemble vide ou composé de stop words...) mais la moyenne des pairwises distances comme dans cet [article](https://sci-hub.tw/https://ieeexplore.ieee.org/abstract/document/4812525) et [celui-ci](http://www.l3s.de/~siersdorfer/sources/2012/fp055-deng.pdf) (refined diversity jaccard) :\n",
    " $$JD(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}$$ where A and B are sets of words from the item A and item B. TODO dire si on supprime les stopwords.\n",
    " * Novelty at 100 (nov@100) : pareil mais entre l'historique utilisateur et R.\n",
    " $$novelty(R, H) = \\frac{\\sum_{i=1}^{|R|}\\sum_{j=1}^{|H|} dist(R_i, H_j)}{|R|.|H|}$$\n",
    " * Strict novelty at 100 (snov@100) : pareil mais on prend le min.\n",
    " $$strictnovelty(R, H) = \\frac{\\sum_{i=1}^{|R|} mindist(R_i, H)}{|R|}$$\n",
    " * Serendipity at 100 (ser@100) : the ratio of relevants items the evaluated model recommanded and the primitive model didn't recommand. With $R$ the recommendation set of the evaluated model, $P$ the recommendation set of the primitive model, $T$ the set of relevant items, and for cases where $T \\setminus P \\neq \\emptyset$, we define the serendipity as:\n",
    " $$serendipity(R, P, T) = \\frac{|R \\cap (T \\setminus P)|}{|T \\setminus P|}$$\n",
    " Cases where $T \\setminus P = \\emptyset$ are not relevant because the primitive model already predicted all relevant items. Thus no model can be serendipe. These cases are not taken into account in the average for all user (+ TODO donner le % des cas $T \\setminus P = \\emptyset$).\n",
    " Les modèles primitif sont le modèle TFIDF avec historyRef=1 et lowercase et lemmatization. L'autre est le modèle qui prend le set des mots sans stop words pour l'historique, et cherche la meilleur similarité jaccard dans les candidats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    " * Ne pas faire de normalisation sauf à la fin dans les tableaux de résultat quand on aura TOUTES les diversity, meme la diversité de random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pourquoi dbert-ft permet de maximiser diversité et accuracy ?\n",
    "\n",
    " * Il donne un espace vectoriel qui est plus loin, moins semantique, plus focalisé sur les sequence faiblemenet semantique \n",
    " * Le modèle est donc très complementaire puisqu'arrive à trouver des articles interessant pour l'utilistaur juste par le style mais qui diverge d'un point de vue topic...\n",
    " * on peut donner un bout d'explication en mentionnant le TFIDF focus (non spécificité) que j'ai mentionné au chapitre style\n",
    " * autre explication avec serendipity ?\n",
    " * Ou parce qu'il reconnait les sources ? TODO voir l'overlap de source interuser. Faire une baseline qui recommande la source majoritaire ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mongo monitoring\n",
    "\n",
    "    db.getCollection('scores').find({'metric': 'snov@100'}).count()\n",
    "    db.getCollection('scores').find({'metric': 'jacc-snov@100'}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Killer unique run:\n",
    "# oomstopper --no-tail bacceval ; killbill bacceval ; cd ~/twinews-logs ; jupython -o nohup-bacceval-$HOSTNAME-$(date +%Y-%m-%d.%M-%S).out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/evaluation/bacceval.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique run:\n",
    "# oomstopper --no-tail bacceval ; cd ~/twinews-logs ; jupython -o nohup-bacceval-$HOSTNAME-$(date +%Y-%m-%d.%M-%S).out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/evaluation/bacceval.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Killer triple run:\n",
    "# oomstopper --no-tail bacceval ; killbill bacceval ; cd ~/twinews-logs ; jupython --no-tail -o nohup-bacceval-$HOSTNAME-$(date +%Y-%m-%d.%M-%S).out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/evaluation/bacceval.ipynb ; sleep 30 ; jupython --no-tail -o nohup-bacceval-$HOSTNAME-$(date +%Y-%m-%d.%M-%S).out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/evaluation/bacceval.ipynb ; sleep 30 ; jupython --no-tail -o nohup-bacceval-$HOSTNAME-$(date +%Y-%m-%d.%M-%S).out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/evaluation/bacceval.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triple run:\n",
    "# oomstopper --no-tail bacceval ; cd ~/twinews-logs ; jupython --no-tail -o nohup-bacceval-$HOSTNAME-$(date +%Y-%m-%d.%M-%S).out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/evaluation/bacceval.ipynb ; sleep 30 ; jupython --no-tail -o nohup-bacceval-$HOSTNAME-$(date +%Y-%m-%d.%M-%S).out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/evaluation/bacceval.ipynb ; sleep 30 ; jupython --no-tail -o nohup-bacceval-$HOSTNAME-$(date +%Y-%m-%d.%M-%S).out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/evaluation/bacceval.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "isNotebook = '__file__' not in locals()\n",
    "TEST = isNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from databasetools.mongo import *\n",
    "from datastructuretools.cache import *\n",
    "from newstools.goodarticle.utils import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.news import parser as newsParser\n",
    "from machinelearning.iterator import *\n",
    "from twinews.utils import *\n",
    "from twinews.evaluation import metrics\n",
    "from twinews.evaluation.utils import *\n",
    "from twinews.models.genericutils import *\n",
    "from twinews.models.ranking import *\n",
    "import time\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tictoc starts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining logger:\n",
    "logger = Logger(tmpDir('logs') + \"/bacceval.log\") if isNotebook else Logger(\"bacceval-\" + getHostname() + \"-\" + getDateSec() + \".log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twinews news (version 1.0) initialised.\n"
     ]
    }
   ],
   "source": [
    "# Making the cache that is a dict-like object (url --> vector) keeping data until 2Go of free RAM:\n",
    "genericCaches = dict()\n",
    "newsCollection = getNewsCollection(logger=logger)\n",
    "def getter(key, logger=None, verbose=True):\n",
    "    global newsCollection\n",
    "    global genericCaches\n",
    "    global genericFields\n",
    "    if newsCollection is None:\n",
    "        newsCollection = getNewsCollection(logger=logger, verbose=verbose)\n",
    "    cacheKey, url = key\n",
    "    field = genericFields[cacheKey]\n",
    "    if cacheKey in genericCaches:\n",
    "        genericCache = genericCaches[cacheKey]\n",
    "    else:\n",
    "        genericCache = getGenericCache(cacheKey, logger=logger, verbose=verbose)\n",
    "        genericCaches[cacheKey] = genericCache\n",
    "    row = newsCollection.findOne({'url': url}, projection={field: True})\n",
    "    theHash = objectToHash(row[field])\n",
    "    return genericCache[theHash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define primitive models and cache keys:\n",
    "cacheKeys = {\"tfidf\", \"dbert-ft\", \"nmf\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the cache instance (don't forget to purge it at the end):\n",
    "cache = Cache(getter, logger=logger, name=\"cacheForGenericVectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twinews scores initialised.\n"
     ]
    }
   ],
   "source": [
    "# We get scores collection and the rankings GridFS:\n",
    "twinewsScores = getTwinewsScores(logger=logger)\n",
    "twinewsRankings = getTwinewsRankings(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cacheForceFeeding(model, maxItems=None, logger=None, verbose=True):\n",
    "    global cache\n",
    "    global newsCollection\n",
    "    i = 0\n",
    "    urls = list(newsCollection.distinct(\"url\"))\n",
    "    if maxItems is not None:\n",
    "        urls = urls[:maxItems]\n",
    "    for url in pb(urls, printRatio=0.01, message=\"Force-feeding the cache...\",\n",
    "                  logger=logger, verbose=verbose):\n",
    "        cache[(model, url)]\n",
    "        if i % 1000 == 0 and freeRAM() < 2:\n",
    "            logWarning(\"Stopping because no RAM left.\", logger)\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicDistPrint(url1, url2, dist, prob=1.0, logger=None, verbose=True):\n",
    "    if verbose:\n",
    "        if getRandomFloat() < prob and (dist >= 0.99 or dist < 0.84):\n",
    "                log(dist, logger)\n",
    "                # t1 = getNewsField(url1, 'detokText')\n",
    "                # t2 = getNewsField(url2, 'detokText')\n",
    "                t1Words = set(flattenLists(getNewsField(url1, 'sentences', verbose=False)))\n",
    "                t2Words = set(flattenLists(getNewsField(url2, 'sentences', verbose=False)))\n",
    "                inter = t1Words.intersection(t2Words)\n",
    "                log(\"-\" * 20, logger)\n",
    "                bp(t1Words, 4, logger)\n",
    "                log(\"-\" * 20, logger)\n",
    "                bp(t2Words, 4, logger)\n",
    "                log(\"-\" * 20, logger)\n",
    "                bp(inter, 5, logger)\n",
    "                log(len(inter), logger)\n",
    "                log(\"#\" * 20, logger)\n",
    "                log(\"#\" * 20, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfDiversityAt100(urls, logger=None, verbose=False):\n",
    "    return diversity(urls, 'tfidf', at=100, logger=logger, verbose=verbose)\n",
    "def styleDiversityAt100(urls, logger=None, verbose=False):\n",
    "    return diversity(urls, 'dbert-ft', at=100, logger=logger, verbose=verbose)\n",
    "def topicDiversityAt100(urls, logger=None, verbose=False):\n",
    "    return diversity(urls, 'nmf', at=100, logger=logger, verbose=verbose)\n",
    "def diversity(urls, model, at=100, distance=\"cosine\", logger=None, verbose=False):\n",
    "    global cache\n",
    "    assert isinstance(urls, list)\n",
    "    urls = urls[:at]\n",
    "    assert len(urls) == at\n",
    "    vectors = vstack([cache[(model, url)] for url in urls])\n",
    "    distances = getDistances(vectors, vectors, metric=distance, verbose=False)\n",
    "    pairwiseCount = 0\n",
    "    distSum = 0\n",
    "    for i in range(at):\n",
    "        for u in range(i+1, at):\n",
    "            dist = distances[i][u]\n",
    "            distSum += dist\n",
    "            pairwiseCount += 1\n",
    "            basicDistPrint(urls[i], urls[u], dist, verbose=TEST and verbose, logger=logger)\n",
    "    assert pairwiseCount == (at**2 - at) / 2 # \\frac{{|R|}^2-{|R|}}{2}\n",
    "    return distSum / pairwiseCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swJaccardRepr(url, *args, **kwargs):\n",
    "    return __jaccardRepr(url, 200, *args, **kwargs)\n",
    "def jaccardRepr(url, *args, **kwargs):\n",
    "    return __jaccardRepr(url, 0, *args, **kwargs)\n",
    "def __jaccardRepr\\\n",
    "(\n",
    "    url,\n",
    "    stopWordAmount,\n",
    "    lowercase=True,\n",
    "    logger=None, verbose=True,\n",
    "):\n",
    "    global newsCollection\n",
    "    global STOP_WORDS\n",
    "    assert '__int_1__' in STOP_WORDS\n",
    "    if stopWordAmount is None or stopWordAmount == 0:\n",
    "        sw = None\n",
    "    else:\n",
    "        sw = set(STOP_WORDS[:stopWordAmount])\n",
    "    sentences = getNewsField(url, 'sentences', verbose=False)\n",
    "    tokens = flattenLists(sentences)\n",
    "    if lowercase:\n",
    "        tokens = [e.lower() for e in tokens]\n",
    "    tokens = set(tokens)\n",
    "    if sw is not None and len(sw) > 0:\n",
    "        tokens = set([e for e in tokens if e not in sw])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccardDistance(url1, url2, cache):\n",
    "    assert isinstance(useSW, Cache)\n",
    "    words1 = cache[url1]\n",
    "    words2 = cache[url2]\n",
    "    return 1 - len(words1.intersection(words2)) / len((words1.union(words2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccardDistance2(url1, url2, useSW):\n",
    "    global jaccardCache\n",
    "    global swJaccardCache\n",
    "    assert isinstance(useSW, bool)\n",
    "    cache = swJaccardCache if useSW else jaccardCache\n",
    "    words1 = cache[url1]\n",
    "    words2 = cache[url2]\n",
    "    return 1 - len(words1.intersection(words2)) / len((words1.union(words2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swJaccardDiversityAt100(urls, logger=None, verbose=False):\n",
    "    return jaccardDiversity(urls, True, at=100, logger=logger, verbose=verbose)\n",
    "def jaccardDiversityAt100(urls, logger=None, verbose=False):\n",
    "    return jaccardDiversity(urls, False, at=100, logger=logger, verbose=verbose)\n",
    "def jaccardDiversity(urls, useSW, at=100, logger=None, verbose=False):\n",
    "    global swJaccardCache\n",
    "    global jaccardCache\n",
    "    assert isinstance(urls, list)\n",
    "    urls = urls[:at]\n",
    "    assert len(urls) == at\n",
    "    pairwiseCount = 0\n",
    "    distSum = 0\n",
    "    for i in range(at):\n",
    "        for u in range(i+1, at):\n",
    "            dist = jaccardDistance(urls[i], urls[u], swJaccardCache if useSW else jaccardCache)\n",
    "            distSum += dist\n",
    "            pairwiseCount += 1\n",
    "            basicDistPrint(urls[i], urls[u], dist, verbose=TEST and verbose, logger=logger)\n",
    "    assert pairwiseCount == (at**2 - at) / 2 # \\frac{{|R|}^2-{|R|}}{2}\n",
    "    return distSum / pairwiseCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "swJaccardCache = Cache(swJaccardRepr, logger=logger, name=\"swJaccardCache\")\n",
    "jaccardCache = Cache(jaccardRepr, logger=logger, name=\"jaccardCache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfNoveltyAt100(*args, logger=None, verbose=False):\n",
    "    return novelty(*args, 'tfidf', at=100, logger=logger, verbose=verbose)\n",
    "def styleNoveltyAt100(*args, logger=None, verbose=False):\n",
    "    return novelty(*args, 'dbert-ft', at=100, logger=logger, verbose=verbose)\n",
    "def topicNoveltyAt100(*args, logger=None, verbose=False):\n",
    "    return novelty(*args, 'nmf', at=100, logger=logger, verbose=verbose)\n",
    "def novelty(historyUrls, urls, model, at=100, distance=\"cosine\", logger=None, verbose=False):\n",
    "    global cache\n",
    "    assert isinstance(urls, list)\n",
    "    urls = urls[:at]\n",
    "    assert len(urls) == at\n",
    "    historyUrls = list(historyUrls)\n",
    "    assert len(historyUrls) > 0\n",
    "    historyVectors = vstack([cache[(model, url)] for url in historyUrls])\n",
    "    vectors = vstack([cache[(model, url)] for url in urls])\n",
    "    distances = getDistances(historyVectors, vectors, metric=distance, verbose=False)\n",
    "    pairwiseCount = 0\n",
    "    distSum = 0\n",
    "    for i in range(len(historyUrls)):\n",
    "        for u in range(len(urls)):\n",
    "            dist = distances[i][u]\n",
    "            distSum += dist\n",
    "            pairwiseCount += 1\n",
    "            basicDistPrint(historyUrls[i], urls[u], dist, verbose=TEST and verbose, logger=logger)\n",
    "    assert pairwiseCount == len(historyUrls) * len(urls)\n",
    "    return distSum / pairwiseCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swJaccardNoveltyAt100(*args, logger=None, verbose=False):\n",
    "    return jaccardNovelty(*args, True, at=100, logger=logger, verbose=verbose)\n",
    "def jaccardNoveltyAt100(*args, logger=None, verbose=False):\n",
    "    return jaccardNovelty(*args, False, at=100, logger=logger, verbose=verbose)\n",
    "def jaccardNovelty(historyUrls, urls, useSW, at=100, logger=None, verbose=False):\n",
    "    global swJaccardCache\n",
    "    global jaccardCache\n",
    "    assert isinstance(urls, list)\n",
    "    urls = urls[:at]\n",
    "    assert len(urls) == at\n",
    "    historyUrls = list(historyUrls)\n",
    "    assert len(historyUrls) > 0\n",
    "    pairwiseCount = 0\n",
    "    distSum = 0\n",
    "    for i in range(len(historyUrls)):\n",
    "        for u in range(len(urls)):\n",
    "            dist = jaccardDistance(historyUrls[i], urls[u], swJaccardCache if useSW else jaccardCache)\n",
    "            distSum += dist\n",
    "            pairwiseCount += 1\n",
    "            basicDistPrint(historyUrls[i], urls[u], dist, verbose=TEST and verbose, logger=logger)\n",
    "    assert pairwiseCount == len(historyUrls) * len(urls)\n",
    "    return distSum / pairwiseCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strict novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfStrictNoveltyAt100(*args, logger=None, verbose=False):\n",
    "    return strictNovelty(*args, 'tfidf', at=100, logger=logger, verbose=verbose)\n",
    "def styleStrictNoveltyAt100(*args, logger=None, verbose=False):\n",
    "    return strictNovelty(*args, 'dbert-ft', at=100, logger=logger, verbose=verbose)\n",
    "def topicStrictNoveltyAt100(*args, logger=None, verbose=False):\n",
    "    return strictNovelty(*args, 'nmf', at=100, logger=logger, verbose=verbose)\n",
    "def strictNovelty(historyUrls, urls, model, at=100, distance=\"cosine\", logger=None, verbose=False):\n",
    "    global cache\n",
    "    assert isinstance(urls, list)\n",
    "    urls = urls[:at]\n",
    "    assert len(urls) == at\n",
    "    historyUrls = list(historyUrls)\n",
    "    assert len(historyUrls) > 0\n",
    "    historyVectors = vstack([cache[(model, url)] for url in historyUrls])\n",
    "    vectors = vstack([cache[(model, url)] for url in urls])\n",
    "    distances = getDistances(historyVectors, vectors, metric=distance, verbose=False)\n",
    "    pairwiseCount = 0\n",
    "    distSum = 0\n",
    "    for u in range(len(urls)):\n",
    "        minDist = None\n",
    "        for i in range(len(historyUrls)):\n",
    "            dist = distances[i][u]\n",
    "            if dist > 0 and dist < 0.00001:\n",
    "                dist = 0.0\n",
    "            if minDist is None or dist < minDist:\n",
    "                minDist = dist\n",
    "            basicDistPrint(historyUrls[i], urls[u], dist, verbose=TEST and verbose, logger=logger)\n",
    "        pairwiseCount += 1\n",
    "        distSum += minDist\n",
    "    assert pairwiseCount == len(urls)\n",
    "    return distSum / pairwiseCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard strict novelty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swJaccardStrictNoveltyAt100(*args, logger=None, verbose=False):\n",
    "    return jaccardStrictNovelty(*args, True, at=100, logger=logger, verbose=verbose)\n",
    "def jaccardStrictNoveltyAt100(*args, logger=None, verbose=False):\n",
    "    return jaccardStrictNovelty(*args, False, at=100, logger=logger, verbose=verbose)\n",
    "def jaccardStrictNovelty(historyUrls, urls, useSW, at=100, logger=None, verbose=False):\n",
    "    global swJaccardCache\n",
    "    global jaccardCache\n",
    "    assert isinstance(urls, list)\n",
    "    urls = urls[:at]\n",
    "    assert len(urls) == at\n",
    "    historyUrls = list(historyUrls)\n",
    "    assert len(historyUrls) > 0\n",
    "    pairwiseCount = 0\n",
    "    distSum = 0\n",
    "    for u in range(len(urls)):\n",
    "        minDist = None\n",
    "        for i in range(len(historyUrls)):\n",
    "            dist = jaccardDistance(historyUrls[i], urls[u], swJaccardCache if useSW else jaccardCache)\n",
    "            if dist > 0 and dist < 0.00001:\n",
    "                dist = 0.0\n",
    "            if minDist is None or dist < minDist:\n",
    "                minDist = dist\n",
    "            basicDistPrint(historyUrls[i], urls[u], dist, verbose=TEST and verbose, logger=logger)\n",
    "        pairwiseCount += 1\n",
    "        distSum += minDist\n",
    "    assert pairwiseCount == len(urls)\n",
    "    return distSum / pairwiseCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serendipity\n",
    "\n",
    " * splitVersion 1, rankings having no serendipity scores for tfidf-ser@100: **24.0%** (meaning the primitive model predicted all relevant items, thus 0-division in the formula...)\n",
    " * splitVersion 1, for jacc-ser@100: **2.31%**\n",
    " * splitVersion 2, for tfidf-ser@100: **27.9%**\n",
    " * splitVersion 2, for jacc-ser@100: **2.88%**\n",
    " * For wtfidf-ser@100: **28.78%** (unknown splitVersion)\n",
    " * For style-ser@100: **22.15%** (unknown splitVersion)\n",
    " * For bm25-ser@100: **28.78%** (unknown splitVersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmodels = {1: {\"tfidf\": \"tfidf-4b89a\", \"wtfidf\": \"tfidf-7febb\", \"jaccard\": \"jaccard-1d3f1\", \"bm25\": \"bm25-933f7\", \"dbert-ft\": \"dbert-ft-7847a\"}, 2: {\"tfidf\": \"tfidf-71fb5\", \"wtfidf\": \"tfidf-7e79d\", \"jaccard\": \"jaccard-1499a\", \"bm25\": \"bm25-1eb2a\", \"dbert-ft\": \"dbert-ft-d1b5f\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfSerendipityAt100(rankings, userId, rankingIndex, splitVersion, logger=None, verbose=False):\n",
    "    return serendipity(rankings, userId, rankingIndex, splitVersion, 'tfidf', at=100, logger=logger, verbose=verbose)\n",
    "def wtfidfSerendipityAt100(rankings, userId, rankingIndex, splitVersion, logger=None, verbose=False):\n",
    "    return serendipity(rankings, userId, rankingIndex, splitVersion, 'wtfidf', at=100, logger=logger, verbose=verbose)\n",
    "def bm25SerendipityAt100(rankings, userId, rankingIndex, splitVersion, logger=None, verbose=False):\n",
    "    return serendipity(rankings, userId, rankingIndex, splitVersion, 'bm25', at=100, logger=logger, verbose=verbose)\n",
    "def styleSerendipityAt100(rankings, userId, rankingIndex, splitVersion, logger=None, verbose=False):\n",
    "    return serendipity(rankings, userId, rankingIndex, splitVersion, 'dbert-ft', at=100, logger=logger, verbose=verbose)\n",
    "def jaccardSerendipityAt100(rankings, userId, rankingIndex, splitVersion, logger=None, verbose=False):\n",
    "    return serendipity(rankings, userId, rankingIndex, splitVersion, 'jaccard', at=100, logger=logger, verbose=verbose)\n",
    "def serendipity(rankings, userId, rankingIndex, splitVersion, model, at=100, logger=None, verbose=False):\n",
    "    global pmodels\n",
    "    global pmodelsRankingsCache\n",
    "    global evalDataCache\n",
    "    # Getting T:\n",
    "    evalData = evalDataCache[splitVersion]\n",
    "    T = set(evalData['testUsers'][userId].keys())\n",
    "    # Getting R:\n",
    "    R = rankings[userId][rankingIndex]\n",
    "    if isinstance(R[0], tuple):\n",
    "        R = [e[0] for e in R]\n",
    "    R = set(R[:at])\n",
    "    assert len(R) == 100\n",
    "    # Getting P:\n",
    "    pmodel = pmodels[splitVersion][model]\n",
    "    prankings = pmodelsRankingsCache[pmodel]\n",
    "    P = prankings[userId][rankingIndex]\n",
    "    if isinstance(P[0], tuple):\n",
    "        P = [e[0] for e in P]\n",
    "    P = set(P[:at])\n",
    "    assert len(P) == 100\n",
    "    # Getting T minus P:\n",
    "    TminusP = set([e for e in T if e not in P])\n",
    "    # We check if this is relevant:\n",
    "    if len(TminusP) == 0:\n",
    "        return None\n",
    "    # We compute the ratio:\n",
    "    score = len(R.intersection(TminusP)) / len(TminusP)\n",
    "    # Printing stuff:\n",
    "    if TEST:\n",
    "        allItems = T.union(R).union(P)\n",
    "        idsMap = dict()\n",
    "        i = 0\n",
    "        for url in allItems:\n",
    "            idsMap[url] = i\n",
    "            i += 1\n",
    "        T = set([idsMap[e] for e in T])\n",
    "        R = set([idsMap[e] for e in R])\n",
    "        P = set([idsMap[e] for e in P])\n",
    "        TminusP = set([idsMap[e] for e in TminusP])\n",
    "        log(\"userId: \" + str(userId), logger)\n",
    "        log(\"splitVersion: \" + str(splitVersion), logger)\n",
    "        log(\"model: \" + str(model), logger)\n",
    "        log(\"-\" * 20, logger)\n",
    "        log(\"T: \" + str(T), logger)\n",
    "        log(\"-\" * 20, logger)\n",
    "        log(\"R: \" + str(R), logger)\n",
    "        log(\"-\" * 20, logger)\n",
    "        log(\"P: \" + str(P), logger)\n",
    "        log(\"-\" * 20, logger)\n",
    "        log(\"TminusP: \" + str(TminusP), logger)\n",
    "        log(\"-\" * 20, logger)\n",
    "        log(\"score: \" + str(score), logger)\n",
    "        log(\"#\" * 20, logger)\n",
    "        log(\"#\" * 20, logger)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRankings(key, logger=None, verbose=True, **kwargs):\n",
    "    log(\"Downloading rankings of the primitive model \" + key + \"...\", logger, verbose=verbose)\n",
    "    rk = twinewsRankings[key]\n",
    "    log(\"Done.\", logger, verbose=verbose)\n",
    "    return rk\n",
    "pmodelsRankingsCache = Cache(getRankings, logger=logger, name=\"pmodelsRankingsCache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We init an eval data cache:\n",
    "def evalDataGetter(splitVersion, logger=None, verbose=True):\n",
    "    log(\"Downloading eval data version \" + str(splitVersion) + \"...\", logger, verbose=verbose)\n",
    "    return getEvalData(splitVersion, logger=logger, verbose=verbose, maxExtraNews=0)\n",
    "evalDataCache = Cache(evalDataGetter, logger=logger, name=\"evalDataCache\", minFreeRAM=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc params:\n",
    "iterations = 1 if isNotebook else 10000000\n",
    "sleep = 0 if isNotebook else 30\n",
    "exceptionSleep = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To prevent reloading rankings at each test:\n",
    "testRankings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current metric functions:\n",
      "{ jacc-div@100, jacc-nov@100, jacc-snov@100, swjacc-div@100, swjacc-nov@100, swjacc-snov@100 }\n"
     ]
    }
   ],
   "source": [
    "# Metrics for local:\n",
    "metricFuncts = \\\n",
    "{\n",
    "    ##### Diversity #####\n",
    "    'div@100': tfidfDiversityAt100,\n",
    "    'style-div@100': styleDiversityAt100,\n",
    "    'topic-div@100': topicDiversityAt100,\n",
    "    ##### Jaccard diversity #####\n",
    "    'jacc-div@100': jaccardDiversityAt100,\n",
    "    'swjacc-div@100': swJaccardDiversityAt100,\n",
    "    ##### Novelty #####\n",
    "    'nov@100': tfidfNoveltyAt100,\n",
    "    'style-nov@100': styleNoveltyAt100,\n",
    "    'topic-nov@100': topicNoveltyAt100,\n",
    "    ##### Jaccard novelty #####\n",
    "    'jacc-nov@100': jaccardNoveltyAt100,\n",
    "    'swjacc-nov@100': swJaccardNoveltyAt100,\n",
    "    ##### Strict novelty #####\n",
    "    'snov@100': tfidfStrictNoveltyAt100,\n",
    "    'style-snov@100': styleStrictNoveltyAt100,\n",
    "    'topic-snov@100': topicStrictNoveltyAt100,\n",
    "    ##### Jaccard strict novelty #####\n",
    "    'jacc-snov@100': jaccardStrictNoveltyAt100,\n",
    "    'swjacc-snov@100': swJaccardStrictNoveltyAt100,\n",
    "    ##### Serendipity #####\n",
    "    'tfidf-ser@100': tfidfSerendipityAt100,\n",
    "    'wtfidf-ser@100': wtfidfSerendipityAt100,\n",
    "    'bm25-ser@100': bm25SerendipityAt100,\n",
    "    'style-ser@100': styleSerendipityAt100,\n",
    "    'jacc-ser@100': jaccardSerendipityAt100,\n",
    "}\n",
    "tipiNum = lambda: tipiNumber(toInteger=True)\n",
    "# cacheForceFeeding('tfidf', maxItems=300, logger=logger)\n",
    "if TEST:\n",
    "    metricFuncts = dictSelect(metricFuncts, {'jacc-div@100', 'swjacc-div@100', 'jacc-nov@100', 'swjacc-nov@100', 'jacc-snov@100', 'swjacc-snov@100'})\n",
    "# elif tipiNum() in {60, 61, 62, 63}:\n",
    "#     metricFuncts = dictSelect(metricFuncts, {'div@100', 'nov@100'})\n",
    "elif isHostname(\"titanv\"):\n",
    "    metricFuncts = dictSelect(metricFuncts, {'tfidf-ser@100', 'wtfidf-ser@100', 'jacc-ser@100', 'bm25-ser@100', 'style-ser@100'})\n",
    "else:\n",
    "    # metricFuncts = dictSelect(metricFuncts, {'jacc-div@100', 'swjacc-div@100', 'jacc-nov@100', 'swjacc-nov@100', 'jacc-snov@100', 'swjacc-snov@100'})\n",
    "    # metricFuncts = dictSelect(metricFuncts, {'div@100', 'nov@100', 'snov@100'})\n",
    "    # metricFuncts = dictSelect(metricFuncts, {'topic-div@100', 'topic-nov@100', 'topic-snov@100'})\n",
    "    metricFuncts = dictSelect(metricFuncts, {'style-div@100', 'style-nov@100', 'style-snov@100'})\n",
    "    # metricFuncts = dictSelect(metricFuncts, {'jacc-div@100', 'swjacc-div@100', 'jacc-nov@100', 'swjacc-nov@100', 'jacc-snov@100', 'swjacc-snov@100'}) # This one hurt a lot!\n",
    "log(\"Current metric functions:\\n\" + b(metricFuncts.keys(), 5), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing swjacc-nov@100 score of bm25-1eb2a...\n",
      "Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! Taking testRankings as rankings !!! \n",
      "Computing swjacc-nov@100 of bm25-1eb2a   1% [                    ]\n",
      "Computing swjacc-nov@100 of bm25-1eb2a  10% [==                  ] (9.54s left)\n",
      "Computing swjacc-nov@100 of bm25-1eb2a  20% [====                ] (7.76s left)\n",
      "Computing swjacc-nov@100 of bm25-1eb2a  30% [======              ] (6.463s left)\n",
      "Computing swjacc-nov@100 of bm25-1eb2a  40% [========            ] (5.385s left)\n",
      "Computing swjacc-nov@100 of bm25-1eb2a  50% [==========          ] (4.48s left)\n",
      "Computing swjacc-nov@100 of bm25-1eb2a  60% [============        ] (3.886s left)\n",
      "Computing swjacc-nov@100 of bm25-1eb2a  70% [==============      ] (3.282s left)\n",
      "Computing swjacc-nov@100 of bm25-1eb2a  80% [================    ] (2.519s left)\n",
      "Computing swjacc-nov@100 of bm25-1eb2a  90% [==================  ] (1.258s left)\n",
      "Computing swjacc-nov@100 of bm25-1eb2a 100% [====================] (total duration: 13.02s, mean duration: 0.13s)\n",
      "swjacc-nov@100 score of bm25-1eb2a: 0.96\n",
      "Computing swjacc-snov@100 score of bm25-1eb2a...\n",
      "Computing swjacc-snov@100 of bm25-1eb2a   1% [                    ]\n",
      "Computing swjacc-snov@100 of bm25-1eb2a  10% [==                  ] (10.709s left)\n",
      "Computing swjacc-snov@100 of bm25-1eb2a  20% [====                ] (8.76s left)\n",
      "Computing swjacc-snov@100 of bm25-1eb2a  30% [======              ] (7.093s left)\n",
      "Computing swjacc-snov@100 of bm25-1eb2a  40% [========            ] (5.639s left)\n",
      "Computing swjacc-snov@100 of bm25-1eb2a  50% [==========          ] (4.66s left)\n",
      "Computing swjacc-snov@100 of bm25-1eb2a  60% [============        ] (3.946s left)\n",
      "Computing swjacc-snov@100 of bm25-1eb2a  70% [==============      ] (3.257s left)\n",
      "Computing swjacc-snov@100 of bm25-1eb2a  80% [================    ] (2.427s left)\n",
      "Computing swjacc-snov@100 of bm25-1eb2a  90% [==================  ] (1.227s left)\n",
      "Computing swjacc-snov@100 of bm25-1eb2a 100% [====================] (total duration: 12.9s, mean duration: 0.129s)\n",
      "swjacc-snov@100 score of bm25-1eb2a: 0.926\n",
      "Computing jacc-div@100 score of bm25-1eb2a...\n",
      "Computing jacc-div@100 of bm25-1eb2a   1% [                    ]\n",
      "Computing jacc-div@100 of bm25-1eb2a  10% [==                  ] (36.899s left)\n",
      "Computing jacc-div@100 of bm25-1eb2a  20% [====                ] (29.839s left)\n",
      "Computing jacc-div@100 of bm25-1eb2a  30% [======              ] (24.896s left)\n",
      "Computing jacc-div@100 of bm25-1eb2a  40% [========            ] (20.684s left)\n",
      "Computing jacc-div@100 of bm25-1eb2a  50% [==========          ] (16.81s left)\n",
      "Computing jacc-div@100 of bm25-1eb2a  60% [============        ] (13.34s left)\n",
      "Computing jacc-div@100 of bm25-1eb2a  70% [==============      ] (9.998s left)\n",
      "Computing jacc-div@100 of bm25-1eb2a  80% [================    ] (6.699s left)\n",
      "Computing jacc-div@100 of bm25-1eb2a  90% [==================  ] (3.352s left)\n",
      "Computing jacc-div@100 of bm25-1eb2a 100% [====================] (total duration: 33.55s, mean duration: 0.335s)\n",
      "jacc-div@100 score of bm25-1eb2a: 0.874\n",
      "Computing jacc-nov@100 score of bm25-1eb2a...\n",
      "Computing jacc-nov@100 of bm25-1eb2a   1% [                    ]\n",
      "Computing jacc-nov@100 of bm25-1eb2a  10% [==                  ] (14.04s left)\n",
      "Computing jacc-nov@100 of bm25-1eb2a  20% [====                ] (11.68s left)\n",
      "Computing jacc-nov@100 of bm25-1eb2a  30% [======              ] (9.613s left)\n",
      "Computing jacc-nov@100 of bm25-1eb2a  40% [========            ] (7.935s left)\n",
      "Computing jacc-nov@100 of bm25-1eb2a  50% [==========          ] (6.67s left)\n",
      "Computing jacc-nov@100 of bm25-1eb2a  60% [============        ] (5.866s left)\n",
      "Computing jacc-nov@100 of bm25-1eb2a  70% [==============      ] (5.065s left)\n",
      "Computing jacc-nov@100 of bm25-1eb2a  80% [================    ] (3.922s left)\n",
      "Computing jacc-nov@100 of bm25-1eb2a  90% [==================  ] (1.954s left)\n",
      "Computing jacc-nov@100 of bm25-1eb2a 100% [====================] (total duration: 19.97s, mean duration: 0.199s)\n",
      "jacc-nov@100 score of bm25-1eb2a: 0.877\n",
      "Computing jacc-snov@100 score of bm25-1eb2a...\n",
      "Computing jacc-snov@100 of bm25-1eb2a   1% [                    ]\n",
      "Computing jacc-snov@100 of bm25-1eb2a  10% [==                  ] (12.689s left)\n",
      "Computing jacc-snov@100 of bm25-1eb2a  20% [====                ] (10.68s left)\n",
      "Computing jacc-snov@100 of bm25-1eb2a  30% [======              ] (8.89s left)\n",
      "Computing jacc-snov@100 of bm25-1eb2a  40% [========            ] (7.29s left)\n",
      "Computing jacc-snov@100 of bm25-1eb2a  50% [==========          ] (6.07s left)\n",
      "Computing jacc-snov@100 of bm25-1eb2a  60% [============        ] (5.266s left)\n",
      "Computing jacc-snov@100 of bm25-1eb2a  70% [==============      ] (4.487s left)\n",
      "Computing jacc-snov@100 of bm25-1eb2a  80% [================    ] (3.372s left)\n",
      "Computing jacc-snov@100 of bm25-1eb2a  90% [==================  ] (1.685s left)\n",
      "Computing jacc-snov@100 of bm25-1eb2a 100% [====================] (total duration: 17.38s, mean duration: 0.173s)\n",
      "jacc-snov@100 score of bm25-1eb2a: 0.836\n",
      "Computing swjacc-div@100 score of bm25-1eb2a...\n",
      "Computing swjacc-div@100 of bm25-1eb2a   1% [                    ]\n",
      "Computing swjacc-div@100 of bm25-1eb2a  10% [==                  ] (20.25s left)\n",
      "Computing swjacc-div@100 of bm25-1eb2a  20% [====                ] (16.92s left)\n",
      "Computing swjacc-div@100 of bm25-1eb2a  30% [======              ] (14.77s left)\n",
      "Computing swjacc-div@100 of bm25-1eb2a  40% [========            ] (12.629s left)\n",
      "Computing swjacc-div@100 of bm25-1eb2a  50% [==========          ] (10.42s left)\n",
      "Computing swjacc-div@100 of bm25-1eb2a  60% [============        ] (8.433s left)\n",
      "Computing swjacc-div@100 of bm25-1eb2a  70% [==============      ] (6.39s left)\n",
      "Computing swjacc-div@100 of bm25-1eb2a  80% [================    ] (4.324s left)\n",
      "Computing swjacc-div@100 of bm25-1eb2a  90% [==================  ] (2.181s left)\n",
      "Computing swjacc-div@100 of bm25-1eb2a 100% [====================] (total duration: 21.92s, mean duration: 0.219s)\n",
      "swjacc-div@100 score of bm25-1eb2a: 0.957\n",
      "--> tic: 1m 59.16s | message: bm25-1eb2a done.\n"
     ]
    }
   ],
   "source": [
    "# For a certain amount of iterations:\n",
    "for i in range(iterations):\n",
    "    # We get all\n",
    "    modelsKeys = shuffle(sorted(list(twinewsRankings.keys())), seed=0 if TEST else None)\n",
    "    if TEST:\n",
    "        modelsKeys = [e for e in modelsKeys if \"combin\" not in e]\n",
    "        modelsKeys = modelsKeys[:1]\n",
    "    # For all model instances:\n",
    "    tt.tic(display=False)\n",
    "    for modelKey in modelsKeys:\n",
    "        # We init the eval data to None:\n",
    "        evalData = None\n",
    "        rankings = None\n",
    "        # For all metrics:\n",
    "        for metricKey, metricFunct in metricFuncts.items():\n",
    "            # If we didn't added the score previously:\n",
    "            if TEST or (not twinewsScores.has({'id': modelKey, 'metric': metricKey})):\n",
    "                try:\n",
    "                    # We print infos:\n",
    "                    log(\"Computing \" + metricKey + \" score of \" + modelKey + \"...\", logger)\n",
    "                    # We get all data:\n",
    "                    meta = twinewsRankings.getMeta(modelKey)\n",
    "                    splitVersion = meta['splitVersion']\n",
    "                    maxUsers = meta['maxUsers']\n",
    "                    modelName = meta['model']\n",
    "                    # We get eval data:\n",
    "                    if evalData is None:\n",
    "                        evalData = evalDataCache[splitVersion]\n",
    "                    candidates = evalData['candidates']\n",
    "                    # We get rankings:\n",
    "                    if rankings is None:\n",
    "                        if TEST and testRankings is not None:\n",
    "                            logWarning(\"Taking testRankings as rankings !!! \" * 20, logger)\n",
    "                            rankings = testRankings\n",
    "                        else:\n",
    "                            localTT = TicToc(logger=logger)\n",
    "                            localTT.tic(\"Downloading rankings of \" + modelKey + \"...\")\n",
    "                            rankings = twinewsRankings[modelKey]\n",
    "                            if rankings is None or len(rankings) == 0:\n",
    "                                raise Exception(\"Rankings of \" + modelKey + \" doesn't exist anymore, you need to re-generate it.\")\n",
    "                            else:\n",
    "                                checkRankings(rankings, candidates, maxUsers=maxUsers)\n",
    "                            localTT.toc(modelKey + \" downloaded.\", logger)\n",
    "                            if TEST:\n",
    "                                testRankings = rankings\n",
    "                    # Init scores:\n",
    "                    scores = []\n",
    "                    # We get user ids:\n",
    "                    userIds = shuffle(sorted(list(rankings.keys())), seed=0)\n",
    "                    if TEST:\n",
    "                        userIds = userIds[:100]\n",
    "                    # Diversity:\n",
    "                    if 'div@' in metricKey:\n",
    "                        for userId in pb(userIds, logger=logger, message=\"Computing \" + metricKey + \" of \" + modelKey):\n",
    "                            for currentRankings in rankings[userId]:\n",
    "                                assert len(currentRankings) >= 100\n",
    "                                assert isinstance(currentRankings, list)\n",
    "                                assert isinstance(currentRankings[0], str) or isinstance(currentRankings[0], tuple)\n",
    "                                if isinstance(currentRankings[0], tuple):\n",
    "                                    currentUrls = [e[0] for e in currentRankings]\n",
    "                                else:\n",
    "                                    currentUrls = currentRankings\n",
    "                                score = metricFunct(currentUrls, logger=logger)\n",
    "                                scores.append(score)\n",
    "                        if not TEST:\n",
    "                            assert len(scores) >= len(rankings)\n",
    "                    # Novelty:\n",
    "                    elif 'nov@' in metricKey:\n",
    "                        for userId in pb(userIds, logger=logger, message=\"Computing \" + metricKey + \" of \" + modelKey):\n",
    "                            for currentRankings in rankings[userId]:\n",
    "                                assert len(currentRankings) >= 100\n",
    "                                assert isinstance(currentRankings, list)\n",
    "                                assert isinstance(currentRankings[0], str) or isinstance(currentRankings[0], tuple)\n",
    "                                if isinstance(currentRankings[0], tuple):\n",
    "                                    currentUrls = [e[0] for e in currentRankings]\n",
    "                                else:\n",
    "                                    currentUrls = currentRankings\n",
    "                                historyUrls = set(evalData['trainUsers'][userId].keys())\n",
    "                                score = metricFunct(historyUrls, currentUrls, logger=logger)\n",
    "                                scores.append(score)\n",
    "                        if not TEST:\n",
    "                            assert len(scores) >= len(rankings)\n",
    "                    # Serendipity:\n",
    "                    elif 'ser@' in metricKey:\n",
    "                        totalScoresToCompute = 0\n",
    "                        noneScores = 0\n",
    "                        for userId in pb(userIds, logger=logger, message=\"Computing \" + metricKey + \" of \" + modelKey):\n",
    "                            for rankingIndex in range(len(rankings[userId])):\n",
    "                                currentRankings = rankings[userId][rankingIndex]\n",
    "                                assert len(currentRankings) >= 100\n",
    "                                assert isinstance(currentRankings, list)\n",
    "                                assert isinstance(currentRankings[0], str) or isinstance(currentRankings[0], tuple)\n",
    "                                score = metricFunct(rankings, userId, rankingIndex, splitVersion, logger=logger)\n",
    "                                if score is None:\n",
    "                                    noneScores += 1\n",
    "                                else:\n",
    "                                    scores.append(score)\n",
    "                                totalScoresToCompute += 1\n",
    "                        log(\"Rankings having no serendipity scores for \" + metricKey + \": \" + str(truncateFloat(noneScores / totalScoresToCompute * 100, 2)) + \"%\", logger)\n",
    "                    else:\n",
    "                        logError(\"The metric key \" + metricKey + \" is unknown.\", logger)\n",
    "                    # We mean all scrores:\n",
    "                    if len(scores) == 0:\n",
    "                        score = 0.0\n",
    "                    else:\n",
    "                        score = float(np.mean(scores))\n",
    "                    # And finally we add the score in the db:\n",
    "                    if not TEST:\n",
    "                        addTwinewsScore(modelKey, metricKey, score, verbose=False)\n",
    "                    # We print result:\n",
    "                    log(metricKey + \" score of \" + modelKey + \": \" + str(truncateFloat(score, 3)), logger)\n",
    "                except AssertionError as error:\n",
    "                    logException(e, logger)\n",
    "                except Exception as e:\n",
    "                    if isNotebook:\n",
    "                        raise e\n",
    "                    else:\n",
    "                        logError(str(e), logger)\n",
    "                        time.sleep(exceptionSleep)\n",
    "        tt.tic(modelKey + \" done.\")\n",
    "    if sleep > 0:\n",
    "        log(\"Sleeping \" + str(sleep) + \" seconds for the iteration \" + str(i) + \" on \" + str(iterations) + \"...\", logger)\n",
    "        time.sleep(sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  100022528: \n",
      "  [\n",
      "    [\n",
      "      (\n",
      "        http://newsok.com/article/5579183?utm_source=NewsOK.com&utm_medium=Social&utm_campaign=ShareBar-Twit,\n",
      "        30564.78\n",
      "      ),\n",
      "      (\n",
      "        http://newsok.com/article/5578978?utm_source=NewsOK.com&utm_medium=Social&utm_campaign=ShareBar-Twit,\n",
      "        26743.67\n",
      "      ),\n",
      "      ...,\n",
      "      ( https://biochem.wisc.edu/faculty/ansari, 455.35 ),\n",
      "      (\n",
      "        https://www.rochestercitynewspaper.com/rochester/fresh-cut-little-lost-by-gold-koa/Content?oid=52424,\n",
      "        368.0\n",
      "      )\n",
      "    ]\n",
      "  ],\n",
      "  100024324: \n",
      "  [\n",
      "    [\n",
      "      (\n",
      "        https://www.theguardian.com/us-news/2017/dec/27/its-all-fentanyl-opioid-crisis-takes-shape-in-philad,\n",
      "        3825.66\n",
      "      ),\n",
      "      (\n",
      "        https://www.csmonitor.com/Commentary/the-monitors-view/2017/1229/States-make-headway-on-opioid-abuse,\n",
      "        3820.17\n",
      "      ),\n",
      "      ...,\n",
      "      (\n",
      "        http://www.rte.ie/archives/exhibitions/937-u2/291639-end-of-a-decade/,\n",
      "        180.81\n",
      "      ),\n",
      "      (\n",
      "        http://www.itv.com/goodmorningbritain/news/sean-spicer-says-oprah-winfrey-doesnt-have-the-political-,\n",
      "        110.37\n",
      "      )\n",
      "    ]\n",
      "  ],\n",
      "  100064338: \n",
      "  [\n",
      "    [\n",
      "      ( https://fb.me/ADgHbnmp, 12946.08 ),\n",
      "      (\n",
      "        https://www.buzzfeed.com/tasneemnashrulla/hollywood-women-anti-harassment-initiative?bftwnews&utm_te,\n",
      "        12670.2\n",
      "      ),\n",
      "      ...,\n",
      "      (\n",
      "        http://mashable.com/2018/01/08/apple-spectre-software-update-macos-ios.amp,\n",
      "        1498.67\n",
      "      ),\n",
      "      ( http://bit.ly/2ldUJpY, 1254.19 )\n",
      "    ]\n",
      "  ],\n",
      "  1000908834: \n",
      "  [\n",
      "    [\n",
      "      (\n",
      "        http://dispatch.com/news/20180102/drugs-gangs-fuel-record-columbus-homicide-numbers,\n",
      "        7185.67\n",
      "      ),\n",
      "      ( https://fb.me/19fiMzSio, 7039.3 ),\n",
      "      ...,\n",
      "      (\n",
      "        https://www.newscientist.com/article/2157887-arsonist-falcons-suggest-birds-discovered-fire-before-h,\n",
      "        1010.94\n",
      "      ),\n",
      "      ( https://buff.ly/2mni6hy, 866.39 )\n",
      "    ]\n",
      "  ],\n",
      "  1002207570: \n",
      "  [\n",
      "    [\n",
      "      ( http://www.bbc.com/news/health-42544267, 2647.92 ),\n",
      "      ( https://buff.ly/2Dywh9o, 2516.23 ),\n",
      "      ...,\n",
      "      (\n",
      "        http://www.nola.com/northshore/index.ssf/2018/01/st_tammany_government_collecti.html,\n",
      "        176.8\n",
      "      ),\n",
      "      (\n",
      "        http://www.valleynewslive.com/content/misc/467916923.html,\n",
      "        166.82\n",
      "      )\n",
      "    ]\n",
      "  ],\n",
      "  ...,\n",
      "  996054186: \n",
      "  [\n",
      "    [\n",
      "      (\n",
      "        https://www.reuters.com/article/us-usa-puertorico-storm-death/puerto-rico-sets-90-day-target-for-rev,\n",
      "        22220.53\n",
      "      ),\n",
      "      (\n",
      "        https://www.washingtonpost.com/lifestyle/songs-about-generators-and-spam-new-puerto-rican-music-refl,\n",
      "        20839.92\n",
      "      ),\n",
      "      ...,\n",
      "      (\n",
      "        https://www.desmoinesregister.com/story/news/education/2018/01/13/iowans-public-education-fights-sch,\n",
      "        1693.52\n",
      "      ),\n",
      "      ( http://ow.ly/Eb9f50fZlkq, 1535.51 )\n",
      "    ]\n",
      "  ],\n",
      "  996228090: \n",
      "  [\n",
      "    [\n",
      "      (\n",
      "        http://www.heraldnet.com/life/the-years-best-film-an-18-hour-tv-series/,\n",
      "        5511.97\n",
      "      ),\n",
      "      (\n",
      "        http://www.seattleweekly.com/film/the-second-paddington-is-filled-with-slapstick-and-spirit/,\n",
      "        3805.36\n",
      "      ),\n",
      "      ...,\n",
      "      (\n",
      "        http://fox59.com/2018/01/10/new-layoffs-at-carrier-factory-year-after-trump-deal/amp/?__twitter_impr,\n",
      "        269.95\n",
      "      ),\n",
      "      (\n",
      "        https://academic.oup.com/sf/article/96/2/507/4797137?searchresult=1,\n",
      "        118.36\n",
      "      )\n",
      "    ]\n",
      "  ],\n",
      "  9963562: \n",
      "  [\n",
      "    [\n",
      "      ( http://bit.ly/2CMfuzZ, 3172.03 ),\n",
      "      (\n",
      "        https://medium.com/@bieaweh/30notunder30killing-it-ccbf9404e02c,\n",
      "        2998.91\n",
      "      ),\n",
      "      ...,\n",
      "      (\n",
      "        https://www.outlookindia.com/newsscroll/prof-gopal-guru-appointed-editor-of-epw/1222840,\n",
      "        371.27\n",
      "      ),\n",
      "      (\n",
      "        http://thehill.com/blogs/blog-briefing-room/news/366968-democrats-more-fearful-about-2018-than-repub,\n",
      "        361.63\n",
      "      )\n",
      "    ]\n",
      "  ],\n",
      "  9972002: \n",
      "  [\n",
      "    [\n",
      "      ( https://buff.ly/2CTE6Xn, 3229.38 ),\n",
      "      (\n",
      "        https://www.nytimes.com/2018/01/11/technology/facebook-news-feed.html,\n",
      "        2594.2\n",
      "      ),\n",
      "      ...,\n",
      "      ( http://bit.ly/2DfqkBT, 251.84 ),\n",
      "      (\n",
      "        https://www.theguardian.com/global-development/gallery/2018/jan/03/congo-river-survival-mothers-babi,\n",
      "        196.95\n",
      "      )\n",
      "    ]\n",
      "  ],\n",
      "  99895304: \n",
      "  [\n",
      "    [\n",
      "      ( http://ift.tt/2CVEZTz, 3387.91 ),\n",
      "      ( http://bit.ly/2mcxtbE, 3252.73 ),\n",
      "      ...,\n",
      "      (\n",
      "        http://boston.cbslocal.com/video/channel/314-live/,\n",
      "        111.1\n",
      "      ),\n",
      "      ( https://wp.me/p4XUzx-eG, 65.79 )\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook and TEST:\n",
    "    urls = shuffle(list(newsCollection.distinct(\"url\")))[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook and TEST:\n",
    "    log(tfidfDiversityAt100(urls), logger)\n",
    "    log(styleDiversityAt100(urls), logger)\n",
    "    log(topicDiversityAt100(urls), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook and TEST:\n",
    "    urls = shuffle(list(newsCollection.distinct(\"url\")))[:100]\n",
    "    jaccardDistance(urls[0], urls[1], swJaccardCache)\n",
    "    jaccardDiversity(urls, False, verbose=True, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook and TEST:\n",
    "    def parallelJaccardDiversity(urls, useSW, at=100, logger=None, verbose=False):\n",
    "        \"\"\"\n",
    "            Much slower...\n",
    "        \"\"\"\n",
    "        global swJaccardCache\n",
    "        global jaccardCache\n",
    "        assert isinstance(urls, list)\n",
    "        urls = urls[:at]\n",
    "        assert len(urls) == at\n",
    "        # Getting data:\n",
    "        cache = swJaccardCache if useSW else jaccardCache\n",
    "        data = dict()\n",
    "        for url in urls:\n",
    "            data[url] = cache[url]\n",
    "        # Getting pairs to compute:\n",
    "        pairs = []\n",
    "        for i in range(at):\n",
    "            for u in range(i+1, at):\n",
    "                pairs.append((i, u))\n",
    "        pairsChunks = split(pairs, cpuCount())\n",
    "        # Defining the gen fucnt:\n",
    "        def genFunct(pairs, urls, data, *args, **kwargs):\n",
    "            for i, u in pairs:\n",
    "                yield ((i, u), jaccardDistance(urls[i], urls[u], data))\n",
    "        # Defining the MLIterator:\n",
    "        mli = MLIterator(pairsChunks, genFunct, genArgs=(urls, data), verbose=False, parallelProcesses=cpuCount(), maxParallelProcesses=cpuCount())\n",
    "        # Iterating all pairs yielded by the mli:\n",
    "        pairwiseCount = 0\n",
    "        distSum = 0\n",
    "        for ((i, u), dist) in mli:\n",
    "            distSum += dist\n",
    "            pairwiseCount += 1\n",
    "        # Checking the size:\n",
    "        assert pairwiseCount == (at**2 - at) / 2 # \\frac{{|R|}^2-{|R|}}{2}\n",
    "        # Returning the result:\n",
    "        return distSum / pairwiseCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook and TEST:\n",
    "    urls = shuffle(list(newsCollection.distinct(\"url\")))[:100]\n",
    "    tt.tic(display=False)\n",
    "    for i in range(100):\n",
    "        print(jaccardDiversity(urls, False, verbose=False, logger=logger))\n",
    "    tt.tic()\n",
    "    tt.tic(display=False)\n",
    "    for i in range(100):\n",
    "        print(parallelJaccardDiversity(urls, False, verbose=False, logger=logger))\n",
    "    tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook and TEST:\n",
    "    urls = shuffle(list(newsCollection.distinct(\"url\")))[:100]\n",
    "    at = 50\n",
    "    historyUrls1 = urls[:10]\n",
    "    urls1 = urls[:50]\n",
    "    historyUrls2 = urls[50:60]\n",
    "    urls2 = urls[:50]\n",
    "    historyUrls3 = urls[:60]\n",
    "    urls3 = urls[:50]\n",
    "    historyUrls4 = urls[:60]\n",
    "    urls4 = urls[:49] + [urls[62]]\n",
    "    print(jaccardStrictNovelty(historyUrls1, urls1, True, at=at, logger=logger, verbose=True))\n",
    "    print(jaccardStrictNovelty(historyUrls2, urls2, True, at=at, logger=logger))\n",
    "    print(jaccardStrictNovelty(historyUrls3, urls3, True, at=at, logger=logger))\n",
    "    print(jaccardStrictNovelty(historyUrls4, urls4, True, at=at, logger=logger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.purge()\n",
    "swJaccardCache.purge()\n",
    "jaccardCache.purge()\n",
    "pmodelsRankingsCache.purge()\n",
    "evalDataCache.purge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
