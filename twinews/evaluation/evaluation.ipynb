{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tipi 86\n",
    "# cd ~/twinews-logs ; jupython -o nohup-evaluation.out --venv st-venv ~/Workspace/Python/Datasets/Twinews/twinews/evaluation/evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from databasetools.mongo import *\n",
    "from newstools.goodarticle.utils import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.news import parser as newsParser\n",
    "from machinelearning.iterator import *\n",
    "from twinews.utils import *\n",
    "from twinews.evaluation import metrics\n",
    "from twinews.evaluation.utils import *\n",
    "import time\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tictoc starts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/evaluation.log\") if isNotebook else Logger(\"evaluation.log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1 if isNotebook else 10000000\n",
    "sleep = 0 if isNotebook else 10\n",
    "exceptionSleep = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricFuncts = \\\n",
    "{\n",
    "    'ndcg': metrics.ndcg,\n",
    "    'ndcg@10': metrics.ndcgAt10,\n",
    "    'ndcg@100': metrics.ndcgAt100,\n",
    "    'mrr': metrics.mrr,\n",
    "    'p@10': metrics.pAt10,\n",
    "    'p@100': metrics.pAt100,\n",
    "    'map': metrics.map,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twinews scores initialised.\n",
      "Computing ndcg score of lda-72dfd...\n",
      "Downloading eval data version 2...\n",
      "--> tic: 32.9s | message: Eval data loaded\n",
      "--> toc total duration: 36.36s | message: Got Twinews evaluation data\n",
      "Downloading rankings of lda-72dfd...\n",
      "Done.\n",
      "ndcg score of lda-72dfd: 0.17\n",
      "Computing ndcg@10 score of lda-72dfd...\n",
      "ndcg@10 score of lda-72dfd: 0.0\n",
      "Computing ndcg@100 score of lda-72dfd...\n",
      "ndcg@100 score of lda-72dfd: 0.02\n",
      "Computing mrr score of lda-72dfd...\n",
      "mrr score of lda-72dfd: 0.0\n",
      "Computing p@10 score of lda-72dfd...\n",
      "p@10 score of lda-72dfd: 0.0\n",
      "Computing p@100 score of lda-72dfd...\n",
      "p@100 score of lda-72dfd: 0.0\n",
      "Computing map score of lda-72dfd...\n",
      "map score of lda-72dfd: 0.0\n",
      "Computing ndcg score of lda-71511...\n",
      "Downloading eval data version 2...\n",
      "--> tic: 33.15s | message: Eval data loaded\n",
      "--> toc total duration: 36.88s | message: Got Twinews evaluation data\n",
      "Downloading rankings of lda-71511...\n",
      "Done.\n",
      "ndcg score of lda-71511: 0.23\n",
      "Computing ndcg@10 score of lda-71511...\n",
      "ndcg@10 score of lda-71511: 0.01\n",
      "Computing ndcg@100 score of lda-71511...\n",
      "ndcg@100 score of lda-71511: 0.04\n",
      "Computing mrr score of lda-71511...\n",
      "mrr score of lda-71511: 0.04\n",
      "Computing p@10 score of lda-71511...\n",
      "p@10 score of lda-71511: 0.01\n",
      "Computing p@100 score of lda-71511...\n",
      "p@100 score of lda-71511: 0.0\n",
      "Computing map score of lda-71511...\n",
      "map score of lda-71511: 0.01\n",
      "Computing ndcg score of lda-d3b82...\n",
      "Downloading eval data version 2...\n",
      "--> tic: 32.07s | message: Eval data loaded\n",
      "--> toc total duration: 34.62s | message: Got Twinews evaluation data\n",
      "Downloading rankings of lda-d3b82...\n",
      "Done.\n",
      "ndcg score of lda-d3b82: 0.35\n",
      "Computing ndcg@10 score of lda-d3b82...\n",
      "ndcg@10 score of lda-d3b82: 0.12\n",
      "Computing ndcg@100 score of lda-d3b82...\n",
      "ndcg@100 score of lda-d3b82: 0.23\n",
      "Computing mrr score of lda-d3b82...\n",
      "mrr score of lda-d3b82: 0.23\n",
      "Computing p@10 score of lda-d3b82...\n",
      "p@10 score of lda-d3b82: 0.08\n",
      "Computing p@100 score of lda-d3b82...\n",
      "p@100 score of lda-d3b82: 0.03\n",
      "Computing map score of lda-d3b82...\n",
      "map score of lda-d3b82: 0.1\n",
      "Computing ndcg score of lda-94782...\n",
      "Downloading eval data version 2...\n",
      "--> tic: 34.14s | message: Eval data loaded\n",
      "--> toc total duration: 37.88s | message: Got Twinews evaluation data\n",
      "Downloading rankings of lda-94782...\n",
      "Done.\n",
      "ndcg score of lda-94782: 0.33\n",
      "Computing ndcg@10 score of lda-94782...\n",
      "ndcg@10 score of lda-94782: 0.1\n",
      "Computing ndcg@100 score of lda-94782...\n",
      "ndcg@100 score of lda-94782: 0.21\n",
      "Computing mrr score of lda-94782...\n",
      "mrr score of lda-94782: 0.19\n",
      "Computing p@10 score of lda-94782...\n",
      "p@10 score of lda-94782: 0.06\n",
      "Computing p@100 score of lda-94782...\n",
      "p@100 score of lda-94782: 0.03\n",
      "Computing map score of lda-94782...\n",
      "map score of lda-94782: 0.08\n",
      "Computing ndcg score of lda-de387...\n",
      "Downloading eval data version 2...\n",
      "--> tic: 33.46s | message: Eval data loaded\n",
      "--> toc total duration: 37.72s | message: Got Twinews evaluation data\n",
      "Downloading rankings of lda-de387...\n",
      "Done.\n",
      "ndcg score of lda-de387: 0.3\n",
      "Computing ndcg@10 score of lda-de387...\n",
      "ndcg@10 score of lda-de387: 0.07\n",
      "Computing ndcg@100 score of lda-de387...\n",
      "ndcg@100 score of lda-de387: 0.16\n",
      "Computing mrr score of lda-de387...\n",
      "mrr score of lda-de387: 0.16\n",
      "Computing p@10 score of lda-de387...\n",
      "p@10 score of lda-de387: 0.05\n",
      "Computing p@100 score of lda-de387...\n",
      "p@100 score of lda-de387: 0.02\n",
      "Computing map score of lda-de387...\n",
      "map score of lda-de387: 0.06\n"
     ]
    }
   ],
   "source": [
    "# We get scores collection et the rankings GridFS:\n",
    "twinewsScores = getTwinewsScores(logger=logger)\n",
    "twinewsRankings = getTwinewsRankings(logger=logger)\n",
    "# For a certain amount of iterations:\n",
    "for i in range(iterations):\n",
    "    # We get all\n",
    "    modelsKeys = twinewsRankings.keys()\n",
    "    # For all model instances:\n",
    "    for modelKey in modelsKeys:\n",
    "        # We init the eval data to None:\n",
    "        evalData = None\n",
    "        rankings = None\n",
    "        # For all metrics:\n",
    "        for metricKey, metricFunct in metricFuncts.items():\n",
    "            # If we didn't added the score previously:\n",
    "            if not twinewsScores.has({'id': modelKey, 'metric': metricKey}):\n",
    "                try:\n",
    "                    # We print infos:\n",
    "                    log(\"Computing \" + metricKey + \" score of \" + modelKey + \"...\", logger)\n",
    "                    # We get all data:\n",
    "                    meta = twinewsRankings.getMeta(modelKey)\n",
    "                    splitVersion = meta['splitVersion']\n",
    "                    maxUsers = meta['maxUsers']\n",
    "                    modelName = meta['model']\n",
    "                    # We get eval data:\n",
    "                    if evalData is None:\n",
    "                        log(\"Downloading eval data version \" + str(splitVersion) + \"...\", logger)\n",
    "                        evalData = getEvalData(splitVersion, logger=logger, maxExtraNews=0)\n",
    "                    candidates = evalData['candidates']\n",
    "                    # We get rankings:\n",
    "                    if rankings is None:\n",
    "                        log(\"Downloading rankings of \" + modelKey + \"...\", logger)\n",
    "                        rankings = twinewsRankings[modelKey]\n",
    "                        # We check if rankings are coherent with candidates (for the right split version):\n",
    "                        gotACheck = False\n",
    "                        rankingsKeys = set(rankings.keys())\n",
    "                        candidatesKeys = set(candidates.keys())\n",
    "                        if maxUsers is None:\n",
    "                            assert len(rankingsKeys) == len(candidatesKeys)\n",
    "                        assert len(rankingsKeys.union(candidatesKeys)) == len(candidatesKeys)\n",
    "                        for userId in rankings:\n",
    "                            assert len(rankings[userId]) == len(candidates[userId])\n",
    "                            for i, ranking in enumerate(rankings[userId]):\n",
    "                                assert isinstance(ranking, list)\n",
    "                                rankingSet = set(ranking)\n",
    "                                currentCandidates = candidates[userId][i]\n",
    "                                assert len(rankingSet) == len(currentCandidates)\n",
    "                                assert isinstance(currentCandidates, set)\n",
    "                                assert len(rankingSet.union(currentCandidates)) == len(rankingSet)\n",
    "                                gotACheck = True\n",
    "                        assert gotACheck\n",
    "                        log(\"Done.\", logger)\n",
    "                    # We convert all in a list of rel vectors:\n",
    "                    rels = []\n",
    "                    for userId in rankings:\n",
    "                        for ranking in rankings[userId]:\n",
    "                            rel = rankingToRelevanceVector(ranking, set(evalData['testUsers'][userId].keys()))\n",
    "                            rels.append(rel)\n",
    "                    # We compute all scores:\n",
    "                    scores = []\n",
    "                    for rel in rels:\n",
    "                        scores.append(metricFunct(rel))\n",
    "                    # We mean all scrores:\n",
    "                    score = np.mean(scores)\n",
    "                    # And finally we add the score in the db:\n",
    "                    addTwinewsScore(modelKey, metricKey, score, verbose=False)\n",
    "                    # We print result:\n",
    "                    log(metricKey + \" score of \" + modelKey + \": \" + str(truncateFloat(score, 2)), logger)\n",
    "                except Exception as e:\n",
    "                    if isNotebook:\n",
    "                        raise e\n",
    "                    else:\n",
    "                        logException(e, logger)\n",
    "                        time.sleep(exceptionSleep)\n",
    "    if sleep > 0:\n",
    "        log(\"Sleeping \" + str(sleep) + \" seconds for the iteration \" + str(i) + \" on \" + str(iterations) + \"...\", logger)\n",
    "    time.sleep(sleep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
