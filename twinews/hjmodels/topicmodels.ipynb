{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd twinews-logs ; jupython -o nohup-topicmodels-$HOSTNAME.out --venv st-venv ~/notebooks/twinews/hjmodels/topicmodels.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# https://towardsdatascience.com/lets-build-an-article-recommender-using-lda-f22d71b7143e\n",
    "# https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = isNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from databasetools.mongo import *\n",
    "from newstools.goodarticle.utils import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.news import parser as newsParser\n",
    "from machinelearning.iterator import *\n",
    "from twinews.utils import *\n",
    "from twinews.ranking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptools.topicmodeling import *\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "import gensim\n",
    "from math import log2\n",
    "from math import sqrt\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tictoc starts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/lda.log\") if isNotebook else Logger(\"lda-\" + getHostname() + \".log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \\\n",
    "{\n",
    "    'splitVersion': 2,\n",
    "    \n",
    "    'maxUsers': 2 if TEST else None, # Sub-sampling\n",
    "    'maxDocuments': 30 if TEST else 10000,\n",
    "    'useExtraNews': False if TEST else False, # None = unlimited, 0 = no extra news\n",
    "    'minDF': 1 / 500 if TEST else 1 / 2000, # Remove words that have a document frequency ratio lower than 1 / 500\n",
    "    'maxDF': 300, # Remove top 300 voc elements\n",
    "    \n",
    "    'nbTopics': 30 if TEST else 30, # 30, 100\n",
    "    'lowercase': False if TEST else True,\n",
    "    'doLemmatization': False if TEST else True,\n",
    "    # <https://www.quora.com/How-do-you-combine-LDA-and-tf-idf>\n",
    "    # <https://www.quora.com/Why-is-the-performance-improved-by-using-TFIDF-instead-of-bag-of-words-in-LDA-clustering>\n",
    "    'useTFIDF': True,\n",
    "    \n",
    "    'maxIter': 2 if TEST else 30, # 30 for lda, 200 for nmf\n",
    "    \n",
    "    'nmfInit': 'nndsvd', # None, 'nndsvd'\n",
    "    'nmfL1Ratio': 0, # 0.0, 0.5, 1.0\n",
    "    'nmfAlpha': 0.1, # 0.0, 0.1\n",
    "    \n",
    "    'ldaLearningMethod': 'online',\n",
    "    'ldaLearningOffset': 1.0, # 1.0, 10.0\n",
    "    'ldaLearningDecay': 0.7, # 0.5, 0.7, 0.9, 1.0\n",
    "    \n",
    "    'implementation': 'gensim-lda', # gensim-lda, sklearn-lda, sklearn-nmf\n",
    "    'distance': 'cosine', # 'cosine', 'euclidean', 'kl', 'js'\n",
    "    # The historyRef param is very important, it allow to choose, for a particular candidate,\n",
    "    # how many train history items will be used to calculate the similarity with\n",
    "    # the user history.\n",
    "    # Float are ratio on train history\n",
    "    # Integers are absolute number of train item in the history\n",
    "    # For example:\n",
    "    #Â  * 1.0 will allow to mean similarities of a candidate with all train history items\n",
    "    #  * 1 will allow to use only the most similar train item for the similarity of\n",
    "    #Â    the candidate with the history of the user\n",
    "    #  * 0.5 will allow to use the half of history for each candidates\n",
    "    #Â  * 3 to use 3 most similar items with the current candidate...\n",
    "    'historyRef': 0.3, #Â 1, 1.0, 0.5, 0.3, 3, 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'nmf' in config['implementation']:\n",
    "    modelName = \"nmf\"\n",
    "    newConfig = dict()\n",
    "    for key, value in config.items():\n",
    "        if not key.startswith('lda'):\n",
    "            newConfig[key] = value\n",
    "    config = newConfig\n",
    "    del config['useTFIDF']\n",
    "elif 'lda' in config['implementation']:\n",
    "    modelName = \"lda\"\n",
    "    newConfig = dict()\n",
    "    for key, value in config.items():\n",
    "        if not key.startswith('nmf'):\n",
    "            newConfig[key] = value\n",
    "    config = newConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isNotebook:\n",
    "    assert not rankingExists(modelName, config, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 3.74s | message: Eval data loaded\n",
      "twinews news (version 1.0) initialised.\n",
      "--> tic: 8.22s | message: Extra news downloaded\n",
      "--> toc total duration: 11.97s | message: Got Twinews evaluation data\n",
      "{ candidates, extraNews, meta, testNews, testUsers, trainNews, trainUsers }\n",
      "{ 'created': 2020.03.24-14.28.06, 'endDate': 2018-01-15, 'id': 2, 'ranksLength': 1000, 'splitDate': 2017-12-25, 'startDate': 2017-10-01, 'testMaxNewsPerUser': 97, 'testMeanNewsPerUser': 7.22, 'testMinNewsPerUser': 2, 'testNewsCount': 71781, 'totalNewsAvailable': 570210, 'trainMaxNewsPerUser': 379, 'trainMeanNewsPerUser': 26.48, 'trainMinNewsPerUser': 8, 'trainNewsCount': 237150, 'usersCount': 15905 }\n"
     ]
    }
   ],
   "source": [
    "# Getting users and news\n",
    "evalData = getEvalData(config['splitVersion'], maxExtraNews=config['maxDocuments'],\n",
    "                       maxUsers=config['maxUsers'], logger=logger)\n",
    "(trainUsers, testUsers, trainNews, testNews, candidates, extraNews) = \\\n",
    "(evalData['trainUsers'], evalData['testUsers'], evalData['trainNews'],\n",
    " evalData['testNews'], evalData['candidates'], evalData['extraNews'])\n",
    "bp(evalData.keys(), 5, logger)\n",
    "log(b(evalData['meta'], 5), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraNewsList = shuffle(list(extraNews), seed=0)\n",
    "trainNewsList = shuffle(list(trainNews), seed=0)\n",
    "testNewsList = shuffle(list(testNews), seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 urls for urlsForModel:\n",
      "[\n",
      "  https://buff.ly/2CVPqCO,\n",
      "  https://www.theguardian.com/football/2017/dec/21/paulinho-barcelona-clasico-tottenham-china,\n",
      "  ...,\n",
      "  https://lnkd.in/d8xkM9M,\n",
      "  http://ow.ly/BztX30hsqIZ\n",
      "]\n",
      "2017 urls for urlsToVectorize:\n",
      "[\n",
      "  https://buff.ly/2CVPqCO,\n",
      "  https://www.theguardian.com/football/2017/dec/21/paulinho-barcelona-clasico-tottenham-china,\n",
      "  ...,\n",
      "  https://fb.me/2zj0SDL7r,\n",
      "  http://on.wsj.com/2DrnLdR\n",
      "]\n",
      "2017 urls for urlsToInfere:\n",
      "[\n",
      "  https://buff.ly/2CVPqCO,\n",
      "  https://www.theguardian.com/football/2017/dec/21/paulinho-barcelona-clasico-tottenham-china,\n",
      "  ...,\n",
      "  https://fb.me/2zj0SDL7r,\n",
      "  http://on.wsj.com/2DrnLdR\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# We get urls for the LDA model:\n",
    "if config['useExtraNews']:\n",
    "    urlsForModel = extraNewsList + trainNewsList + testNewsList\n",
    "else:\n",
    "    urlsForModel = trainNewsList + testNewsList + extraNewsList\n",
    "urlsForModel = urlsForModel[:config['maxDocuments']]\n",
    "# We get urls to vectorize for the training and the inference:\n",
    "urlsForModelSet = set(urlsForModel)\n",
    "urlsToVectorize = copy.deepcopy(urlsForModel)\n",
    "for url in trainNewsList + testNewsList:\n",
    "    if url not in urlsForModelSet:\n",
    "        urlsToVectorize.append(url)\n",
    "# We get url to infere for the scoring:\n",
    "urlsToInfere = trainNewsList + testNewsList\n",
    "# Print all:\n",
    "log(str(len(urlsForModel)) + \" urls for urlsForModel:\\n\" + b(urlsForModel), logger=logger)\n",
    "log(str(len(urlsToVectorize)) + \" urls for urlsToVectorize:\\n\" + b(urlsToVectorize), logger=logger)\n",
    "log(str(len(urlsToInfere)) + \" urls for urlsToInfere:\\n\" + b(urlsToInfere), logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twinews news (version 1.0) initialised.\n",
      "  0% [                    ]\n",
      "  9% [=                   ] (5.149s left)\n",
      " 19% [===                 ] (4.62s left)\n",
      " 29% [=====               ] (3.986s left)\n",
      " 39% [=======             ] (3.605s left)\n",
      " 49% [=========           ] (2.96s left)\n",
      " 59% [===========         ] (2.346s left)\n",
      " 69% [=============       ] (1.725s left)\n",
      " 79% [===============     ] (1.152s left)\n",
      " 89% [=================   ] (0.581s left)\n",
      " 99% [=================== ] (0.019s left)\n",
      "100% [====================] (total duration: 5.62s, mean duration: 0.002s)\n",
      "[\n",
      "  [ [ Leaders, of, ..., location, . ], [ But, if, ..., host, ? ], ..., [ Gates, said, ..., Denver, . ], [ \", If, ..., said, . ] ],\n",
      "  [ [ The, Brazil, ..., more, . ], [ He, was, ..., play, . ], ..., [ That, football, . ], [ It, was, ..., am, . ] ],\n",
      "  ...,\n",
      "  [ [ BRUNERSBURG, The, ..., attendance, . ], [ \", I, ..., Brunersburg, . ], ..., [ \", Thousands, ..., said, . ], [ Another, stated, ..., locally, . ] ],\n",
      "  [ [ District, attorneys, ..., unanimity, . ], [ Above, ,, ..., skyline, . ], ..., [ Mr., Marquis, ..., judge, . ], [ Write, to, ..., at, __email__ ] ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# We get sentences:\n",
    "sentences = getNewsSentences(urlsToVectorize, logger=logger)\n",
    "bp(sentences, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ [ Leaders, of, ..., said, . ], [ The, Brazil, ..., am, . ], ..., [ BRUNERSBURG, The, ..., locally, . ], [ District, attorneys, ..., at, __email__ ] ]\n"
     ]
    }
   ],
   "source": [
    "# We flatten sentences:\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = flattenLists(sentences[i])\n",
    "docs = sentences\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ [ Leaders, of, ..., said, . ], [ The, Brazil, ..., am, . ], ..., [ BRUNERSBURG, The, ..., locally, . ], [ District, attorneys, ..., at, __email__ ] ]\n"
     ]
    }
   ],
   "source": [
    "# Lower case:\n",
    "if config['lowercase']:\n",
    "    for i in pb(list(range(len(docs))), logger=logger, message=\"Lower casing\"):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = docs[i][u].lower()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ [ Leaders, of, ..., said, . ], [ The, Brazil, ..., am, . ], ..., [ BRUNERSBURG, The, ..., locally, . ], [ District, attorneys, ..., at, __email__ ] ]\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization:\n",
    "if config['doLemmatization']:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pbar = ProgressBar(len(docs), logger=logger, message=\"Lemmatization\")\n",
    "    for i in range(len(docs)):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = lemmatizer.lemmatize(docs[i][u])\n",
    "        pbar.tic()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Voc removed because of minDF (44267 elements):\n",
      "{ 1000th, 100s, 100th, 100x, 103rd, 104th, 108th, 10Weather, 10X, 10x, ..., zooms, zoonotic, zoos, ðŸ’—, ðŸ˜, ðŸ˜„, ðŸ˜‡, ðŸ˜’, ðŸ˜¢, ðŸ™Œ }\n",
      "Voc removed because of maxDF (300 elements):\n",
      "{ \", ', (, ), ,, -, ., :, ;, ?, ..., with, without, work, working, world, would, year, years, you, your }\n",
      "73.9% of voc will be removed.\n",
      "[ [ Leaders, Olympic, ..., side, Gates ], [ Brazil, midfielder, ..., chance, am ], ..., [ County, Society, ..., gas, locally ], [ District, attorneys, ..., Jacob, __email__ ] ]\n"
     ]
    }
   ],
   "source": [
    "# Filtering the corpus:\n",
    "docs = filterCorpus(docs, minDF=config['minDF'], maxDF=config['maxDF'],\n",
    "                    removeEmptyDocs=False, allowEmptyDocs=False, logger=logger)\n",
    "for doc in docs: assert len(doc) > 0\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 18.5s | message: Data preprocessed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18.5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tic(\"Data preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infering topic vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['implementation'] == 'gensim-lda':\n",
    "    dictionary = gensim.corpora.Dictionary(docs)\n",
    "    # dictionary.filter_extremes(no_below=config['min_df'])\n",
    "    bow = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    if config['useTFIDF']:\n",
    "        tfidf = gensim.models.TfidfModel(bow)\n",
    "        bow = tfidf[bow]\n",
    "    assert len(bow) == len(urlsToVectorize)\n",
    "    bowForModel = bow[:len(urlsForModel)]\n",
    "    assert len(bowForModel) == config['maxDocuments']\n",
    "    i = 0\n",
    "    for url in urlsToVectorize:\n",
    "        if url == trainNewsList[0]:\n",
    "            break\n",
    "        i += 1\n",
    "    assert i == len(extraNews) or i == 0\n",
    "    bowForInference = bow[i:i + len(trainNews) + len(testNews)]\n",
    "    assert len(bowForInference) == len(trainNews) + len(testNews)\n",
    "    lda_model = gensim.models.LdaMulticore\\\n",
    "    (\n",
    "        bowForModel,\n",
    "        num_topics=config['nbTopics'],\n",
    "        id2word=dictionary,\n",
    "        iterations=config['maxIter'],\n",
    "        decay=config['ldaLearningDecay'],\n",
    "        offset=config['ldaLearningOffset'],\n",
    "        workers=cpuCount(),\n",
    "        passes=3,\n",
    "    )\n",
    "    inferedVectors = []\n",
    "    for current in bowForInference:\n",
    "        topicProbDistrib = lda_model[current]\n",
    "        currentVector = [0.0] * config['nbTopics']\n",
    "        for t, v in topicProbDistrib:\n",
    "            currentVector[t] = v\n",
    "        inferedVectors.append(np.array(currentVector))\n",
    "    assert len(inferedVectors) == len(trainNews) + len(testNews)\n",
    "    assert len(inferedVectors[0]) == config['nbTopics']\n",
    "    topics = []\n",
    "    for i in range(lda_model.num_topics):\n",
    "        current = dict()\n",
    "        for x in lda_model.get_topic_terms(i, topn=100):\n",
    "            current[dictionary[x[0]]] = x[1]\n",
    "        topics.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['implementation'] == 'sklearn-lda' or config['implementation'] == 'sklearn-nmf':\n",
    "    if config['implementation'] == 'sklearn-nmf' or config['useTFIDF']:\n",
    "        vectorizer = TfidfVectorizer\\\n",
    "        (\n",
    "            sublinear_tf=True,\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            # lowercase=True, # Doesn't work because we erased preprocessor\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer\\\n",
    "        (\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            # lowercase=True, # Doesn't work because we erased preprocessor\n",
    "        )\n",
    "    vectors = vectorizer.fit_transform(docs)\n",
    "    assert vectors.shape[0] == len(urlsToVectorize)\n",
    "    vectorsForModel = vectors[:len(urlsForModel)]\n",
    "    assert vectorsForModel.shape[0] == config['maxDocuments']\n",
    "    i = 0\n",
    "    for url in urlsToVectorize:\n",
    "        if url == trainNewsList[0]:\n",
    "            break\n",
    "        i += 1\n",
    "    assert i == len(extraNews) or i == 0\n",
    "    vectorsForInference = vectors[i:i + len(trainNews) + len(testNews)]\n",
    "    assert vectorsForInference.shape[0] == len(trainNews) + len(testNews)\n",
    "    if config['implementation'] == 'sklearn-lda':\n",
    "        model = LatentDirichletAllocation\\\n",
    "        (\n",
    "            n_components=config['nbTopics'],\n",
    "            learning_method=config['ldaLearningMethod'],\n",
    "            learning_offset=config['ldaLearningOffset'],\n",
    "            learning_decay=config['ldaLearningDecay'],\n",
    "            random_state=0,\n",
    "            n_jobs=cpuCount(),\n",
    "            max_iter=config['maxIter'],\n",
    "        )\n",
    "    else:\n",
    "        model = NMF\\\n",
    "        (\n",
    "            n_components=config['nbTopics'],\n",
    "            random_state=0,\n",
    "            alpha=config['nmfAlpha'],\n",
    "            l1_ratio=config['nmfL1Ratio'],\n",
    "            init=config['nmfInit'],\n",
    "            max_iter=config['maxIter'],\n",
    "        )\n",
    "    model.fit(vectorsForModel)\n",
    "    inferedVectors = model.transform(vectorsForInference)\n",
    "    assert inferedVectors.shape[0] == len(trainNews) + len(testNews)\n",
    "    assert inferedVectors[0].shape[0] == config['nbTopics']\n",
    "    topics = []\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        wordProb = []\n",
    "        for i in range(len(topic)):\n",
    "            prob = topic[i]\n",
    "            word = feature_names[i]\n",
    "            wordProb.append((word, prob))\n",
    "        wordProb = sortBy(wordProb, desc=True, index=1)[:100]\n",
    "        current = dict()\n",
    "        for word, prob in wordProb:\n",
    "            current[word] = prob\n",
    "        topics.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 9.37s | message: Model fitted and topic vectors infered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.37"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tic(\"Model fitted and topic vectors infered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTopics(topics, maxWords=10, logger=None):\n",
    "    for i in range(len(topics)):\n",
    "        log(str(i) + \": \" + str(\" \".join(list(topics[i].keys())[:10])), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTopicsOf(vector, topics, logger=None):\n",
    "    topicsRepr = \"\"\n",
    "    topTopics = sortBy([(i, score) for i, score in enumerate(vector) if score > 0.1], desc=True, index=1)[:3]\n",
    "    log(\"Top topics number are: \" + str(\" \".join([str(e[0]) for e in topTopics])), logger)\n",
    "    currentTopics = [topics[e[0]] for e in topTopics]\n",
    "    printTopics(currentTopics, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: Denver Madrid Olympics Barcelona Cup FC Ham players league Colorado\n",
      "1: Ham Colorado match West Cup Hickenlooper Barcelona Messi Denver city\n",
      "2: FC Dallas Messi Hunt Olympics Denver Barcelona games Rio Soccer\n",
      "3: Messi Denver Barcelona Colorado Range city drainage housing Cup ball\n",
      "4: Denver Colorado Olympics games Winter Hickenlooper city Goldberg Olympic Games\n",
      "5: Dallas FC Panthers Hunt Denver Olympics Cup Fame club Stadium\n",
      "6: Barcelona referee federation soccer games referees Denver video Messi play\n",
      "7: Denver Cup Colorado No FIFA paramedic city Hickenlooper World Jones\n",
      "8: Cup players World games Amazon game Korea Barcelona league play\n",
      "9: jersey flag Spain colors yellow Cup Republican blue 1930s red\n",
      "10: Denver Beck Barcelona Madrid Colorado Olympics Ms. Winter Dr. Olympic\n",
      "11: Ham Denver Amazon match Colorado Panthers West defeat supporters Cup\n",
      "12: Denver games Amazon Cup Dallas housing league Olympics players Colorado\n",
      "13: Barcelona Front Denver Ham Colorado Range club Cup play team\n",
      "14: Dallas players Cup FC Beck Denver Colorado club league team\n",
      "15: Denver Colorado Games Olympic housing Olympics Winter Beck players exploratory\n",
      "16: Amazon bids taxes Messi Barcelona finalists Denver Winter play bidding\n",
      "17: Denver city Colorado Panthers housing coaching Range defensive flag Front\n",
      "18: Colorado Ham Olympics Cup Denver club games players jersey team\n",
      "19: Denver Olympics Olympic housing Brazil Barcelona Colorado Games Madrid Rio\n",
      "20: Denver Cup Rio Brazil city Olympics Olympic housing Stadium Beck\n",
      "21: Cup Denver Soccer Fame Panthers Frisco Olympics Games players ball\n",
      "22: Denver Cup Games Olympic Colorado Stadium Olympics games Soccer league\n",
      "23: Amazon Denver finalists Cup Ham bids taxes play Olympics Olympic\n",
      "24: Barcelona Messi Cup salary Corker Goldberg loophole Denver World widening\n",
      "25: Colorado Denver players Cup Barcelona Messi Goldberg games Games Olympics\n",
      "26: Ham West tax League Barcelona club Liverpool Cup games Corker\n",
      "27: Barcelona Ham Colorado Cup West Messi players Hickenlooper Italy match\n",
      "28: Denver Amazon Colorado Front taxes Range Barcelona city finalists bids\n",
      "29: Dallas Hunt FC players games Denver Beck referee team Colorado\n"
     ]
    }
   ],
   "source": [
    "printTopics(topics, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isNotebook:\n",
    "    try:\n",
    "        dirPath = nosaveDir() + \"/sklearn-models\"\n",
    "        mkdir(dirPath)\n",
    "        configHash = objectToHash(config)[:5]\n",
    "        serialize(model, dirPath + \"/model-\" + configHash + \".pickle\")\n",
    "        toJsonFile(config, dirPath + \"/config-\" + configHash + \".json\")\n",
    "    except Exception as e:\n",
    "        logException(e, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a dict url --> topic vector and a dict url --> text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert len(urlsToInfere) == len(inferedVectors)\n",
    "urlsVectors = dict()\n",
    "for i in range(len(urlsToInfere)):\n",
    "    urlsVectors[urlsToInfere[i]] = inferedVectors[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    urlsTexts = dict()\n",
    "    allTexts = getNewsText(urlsToInfere, logger=logger)\n",
    "    for i in range(len(urlsToInfere)):\n",
    "        urlsTexts[urlsToInfere[i]] = allTexts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isNotebook:\n",
    "    userId = list(trainUsers.keys())[11]\n",
    "    xvectors = []\n",
    "    xurls = []\n",
    "    for url in trainUsers[userId]:\n",
    "        xvectors.append(urlsVectors[url])\n",
    "        xurls.append(url)\n",
    "    xvectors = np.array(xvectors)\n",
    "    yvectors = []\n",
    "    yurls = []\n",
    "    for url in candidates[userId][0]:\n",
    "        yvectors.append(urlsVectors[url])\n",
    "        yurls.append(url)\n",
    "    yvectors = np.array(yvectors)\n",
    "    distances = getDistances(xvectors, yvectors, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing some docs with topics:\n",
    "if isNotebook:\n",
    "    for i in range(10):\n",
    "        urls = random.choice([xurls, yurls])\n",
    "        url = random.choice(urls)\n",
    "        text = urlsTexts[url]\n",
    "        vector = urlsVectors[url]\n",
    "        log(url, logger)\n",
    "        printTopicsOf(vector, topics, logger=logger)\n",
    "        log(text, logger)\n",
    "        log(\"\\n\" * 2, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing some similar docs:\n",
    "if isNotebook:\n",
    "    for i in range(distances.shape[0]):\n",
    "        if i > 100:\n",
    "            break\n",
    "        # Get train:\n",
    "        trainUrl = xurls[i]\n",
    "        trainText = urlsTexts[trainUrl]\n",
    "        trainVector = urlsVectors[trainUrl]\n",
    "        log(trainUrl, logger)\n",
    "        printTopicsOf(trainVector, topics, logger=logger)\n",
    "        log(trainText[:2000], logger)\n",
    "        log(\"\\n\", logger)\n",
    "        # Get distances:\n",
    "        currentDistances = []\n",
    "        for u in range(len(yurls)):\n",
    "            currentDistances.append((yurls[u], urlsTexts[yurls[u]], distances[i][u]))\n",
    "        topSim = sortBy(currentDistances, index=2, desc=False)[:3]\n",
    "        topDissim = sortBy(currentDistances, index=2, desc=True)[:3]\n",
    "        # Print similars:\n",
    "        log(\"MOST SIMILARS\", logger)\n",
    "        log(\"\\n\", logger)\n",
    "        for url, text, dist in topSim:\n",
    "            log(dist, logger)\n",
    "            log(url, logger)\n",
    "            printTopicsOf(urlsVectors[url], topics, logger=logger)\n",
    "            log(text[:2000], logger)\n",
    "            log(\"\\n\", logger)\n",
    "        # Print dissimilars:\n",
    "        log(\"MOST DISSIMILARS\", logger)\n",
    "        log(\"\\n\", logger)\n",
    "        for url, text, dist in topDissim:\n",
    "            log(dist, logger)\n",
    "            log(url, logger)\n",
    "            printTopicsOf(urlsVectors[url], topics, logger=logger)\n",
    "            log(text[:2000], logger)\n",
    "            log(\"\\n\", logger)\n",
    "        log(\"\\n\", logger)\n",
    "        log(\"\\n\" * 2 + '-' * 20 + \"\\n\" * 2, logger)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False and isNotebook:\n",
    "    config['historyRef'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyRef = config['historyRef']\n",
    "assert (isinstance(historyRef, int) and historyRef >= 1) or (isinstance(historyRef, float) and historyRef > 0.0 and historyRef <= 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ranking  50% [==========          ] (0.05s left)\n",
      "Generating ranking 100% [====================] (total duration: 0.09s, mean duration: 0.045s)\n"
     ]
    }
   ],
   "source": [
    "rankings = usersRankingsByHistoryDistance\\\n",
    "(\n",
    "    trainUsers,\n",
    "    candidates,\n",
    "    historyRef,\n",
    "    urlsVectors,\n",
    "    distanceMetric=config['distance'],\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '2617447752': \n",
      "  [\n",
      "    [\n",
      "      https://www.thenation.com/article/persistent-precarity/,\n",
      "      https://buff.ly/2Few0du,\n",
      "      https://pamboyd.wordpress.com/2018/01/02/while-thinking-over-2017/,\n",
      "      http://www.baltimoresun.com/news/maryland/baltimore-county/bs-md-co-pretrial-pilot-20171222-story.html,\n",
      "      http://ow.ly/tTw730hHaYI,\n",
      "      http://bit.ly/2CZ2vPw,\n",
      "      http://www.aljazeera.com/news/2018/01/har-gobind-khorana-google-honours-today-180108134719847.html,\n",
      "      https://www.nytimes.com/2018/01/11/technology/facebook-news-feed.html,\n",
      "      https://atlanta.eater.com/2017/12/28/16826246/atlanta-food-editors-name-their-biggest-dining-grievances-of-2017,\n",
      "      http://windsorstar.com/news/local-news/university-of-windsor-professor-and-team-get-millions-to-study-great-lakes/,\n",
      "      ...,\n",
      "      https://www.washingtonpost.com/local/can-you-prepay-your-real-estate-taxes-before-the-tax-bill-takes-effect-find-out-here/2017/12/26/5539b14c-ea7c-11e7-8a6a-80acf0774e64_story.html?utm_term=.d3b4b6c86a6e,\n",
      "      http://www.bbc.co.uk/sport/football/42683488,\n",
      "      https://www.washingtonpost.com/news/energy-environment/wp/2018/01/08/the-nations-rivers-and-streams-are-getting-dangerously-saltier/?tid=ss_tw&utm_term=.0256b9768971,\n",
      "      https://sports.yahoo.com/lamelo-liangelo-ball-lithuanian-jerseys-sale-u-s-early-january-231329812.html,\n",
      "      http://www.courant.com/news/connecticut/hc-news-towns-prepay-2018-taxes-20171227-story.html,\n",
      "      http://www.nj.com/news/index.ssf/2017/12/only_in_new_jersey_2017.html,\n",
      "      https://www.usatoday.com/story/money/2018/01/10/stores-back-four-ways-they-outfoxed-amazon/1016451001/,\n",
      "      http://nyti.ms/2pPhipJ,\n",
      "      http://www.crainsdetroit.com/article/20180107/news/649416/document-in-detroits-amazon-hq2-bid-reveals-details-of-talent-marshall,\n",
      "      https://www.seattletimes.com/business/amazon/seattle-artist-gives-amazons-fleet-of-treasure-trucks-their-distinctive-local-looks/?utm_source=twitter&utm_medium=social&utm_campaign=article_left_1.1\n",
      "    ]\n",
      "  ],\n",
      "  '790664132': \n",
      "  [\n",
      "    [\n",
      "      http://wapo.st/2lUs7BE?tid=ss_tw&utm_term=.fdf1094d6713,\n",
      "      http://www.arabnews.com/node/1214806/middle-east,\n",
      "      http://bit.ly/2CBzgy1,\n",
      "      http://bit.ly/2ECowQC,\n",
      "      http://ow.ly/YR8x50g4Ji7,\n",
      "      http://read.bi/2CR6E4P,\n",
      "      http://www.wzzm13.com/news/local/michigan/the-great-lakes-are-already-22-covered-in-ice-on-this-day-last-year-it-was-3/505171002,\n",
      "      http://ow.ly/Asot30hIo96,\n",
      "      http://www.cbc.ca/1.4476407,\n",
      "      http://www.chicagotribune.com/entertainment/ct-ent-milt-rosenberg-dead-0111-story.html,\n",
      "      ...,\n",
      "      http://www.bbc.co.uk/news/uk-england-london-42534127,\n",
      "      https://nyti.ms/2EQRVHq,\n",
      "      http://www.democratandchronicle.com/story/news/politics/albany/2017/12/26/prepaying-your-property-taxes/981506001/,\n",
      "      https://www.bloomberg.com/news/articles/2018-01-03/amazon-gets-the-headlines-but-lidl-is-still-the-grocer-to-watch?mc_cid=fad8becce2&mc_eid=e70e651839,\n",
      "      http://www.newsobserver.com/news/politics-government/politics-columns-blogs/under-the-dome/article191973099.html,\n",
      "      https://medium.com/whatever-source-derived/assessing-the-irss-guidance-on-prepaid-property-taxes-c81c24dcd075,\n",
      "      https://www.cbsnews.com/news/donald-trump-national-championship-2018/,\n",
      "      https://usat.ly/2EzTwAM,\n",
      "      http://www.cbc.ca/news/canada/nova-scotia/amazon-bid-new-location-canada-halifax-1.4361723,\n",
      "      https://www.chicagoreader.com/chicago/lake-michigan-bacteria/Content?oid=37062058\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "bp(rankings, logger, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '2617447752': \n",
      "  [\n",
      "    [\n",
      "      https://www.thenation.com/article/persistent-precarity/,\n",
      "      https://buff.ly/2Few0du,\n",
      "      https://pamboyd.wordpress.com/2018/01/02/while-thinking-over-2017/,\n",
      "      http://www.baltimoresun.com/news/maryland/baltimore-county/bs-md-co-pretrial-pilot-20171222-story.html,\n",
      "      http://ow.ly/tTw730hHaYI,\n",
      "      http://bit.ly/2CZ2vPw,\n",
      "      http://www.aljazeera.com/news/2018/01/har-gobind-khorana-google-honours-today-180108134719847.html,\n",
      "      https://www.nytimes.com/2018/01/11/technology/facebook-news-feed.html,\n",
      "      https://atlanta.eater.com/2017/12/28/16826246/atlanta-food-editors-name-their-biggest-dining-grievances-of-2017,\n",
      "      http://windsorstar.com/news/local-news/university-of-windsor-professor-and-team-get-millions-to-study-great-lakes/,\n",
      "      ...,\n",
      "      https://www.washingtonpost.com/local/can-you-prepay-your-real-estate-taxes-before-the-tax-bill-takes-effect-find-out-here/2017/12/26/5539b14c-ea7c-11e7-8a6a-80acf0774e64_story.html?utm_term=.d3b4b6c86a6e,\n",
      "      http://www.bbc.co.uk/sport/football/42683488,\n",
      "      https://www.washingtonpost.com/news/energy-environment/wp/2018/01/08/the-nations-rivers-and-streams-are-getting-dangerously-saltier/?tid=ss_tw&utm_term=.0256b9768971,\n",
      "      https://sports.yahoo.com/lamelo-liangelo-ball-lithuanian-jerseys-sale-u-s-early-january-231329812.html,\n",
      "      http://www.courant.com/news/connecticut/hc-news-towns-prepay-2018-taxes-20171227-story.html,\n",
      "      http://www.nj.com/news/index.ssf/2017/12/only_in_new_jersey_2017.html,\n",
      "      https://www.usatoday.com/story/money/2018/01/10/stores-back-four-ways-they-outfoxed-amazon/1016451001/,\n",
      "      http://nyti.ms/2pPhipJ,\n",
      "      http://www.crainsdetroit.com/article/20180107/news/649416/document-in-detroits-amazon-hq2-bid-reveals-details-of-talent-marshall,\n",
      "      https://www.seattletimes.com/business/amazon/seattle-artist-gives-amazons-fleet-of-treasure-trucks-their-distinctive-local-looks/?utm_source=twitter&utm_medium=social&utm_campaign=article_left_1.1\n",
      "    ]\n",
      "  ],\n",
      "  '790664132': \n",
      "  [\n",
      "    [\n",
      "      http://wapo.st/2lUs7BE?tid=ss_tw&utm_term=.fdf1094d6713,\n",
      "      http://www.arabnews.com/node/1214806/middle-east,\n",
      "      http://bit.ly/2CBzgy1,\n",
      "      http://bit.ly/2ECowQC,\n",
      "      http://ow.ly/YR8x50g4Ji7,\n",
      "      http://read.bi/2CR6E4P,\n",
      "      http://www.wzzm13.com/news/local/michigan/the-great-lakes-are-already-22-covered-in-ice-on-this-day-last-year-it-was-3/505171002,\n",
      "      http://ow.ly/Asot30hIo96,\n",
      "      http://www.cbc.ca/1.4476407,\n",
      "      http://www.chicagotribune.com/entertainment/ct-ent-milt-rosenberg-dead-0111-story.html,\n",
      "      ...,\n",
      "      http://www.bbc.co.uk/news/uk-england-london-42534127,\n",
      "      https://nyti.ms/2EQRVHq,\n",
      "      http://www.democratandchronicle.com/story/news/politics/albany/2017/12/26/prepaying-your-property-taxes/981506001/,\n",
      "      https://www.bloomberg.com/news/articles/2018-01-03/amazon-gets-the-headlines-but-lidl-is-still-the-grocer-to-watch?mc_cid=fad8becce2&mc_eid=e70e651839,\n",
      "      http://www.newsobserver.com/news/politics-government/politics-columns-blogs/under-the-dome/article191973099.html,\n",
      "      https://medium.com/whatever-source-derived/assessing-the-irss-guidance-on-prepaid-property-taxes-c81c24dcd075,\n",
      "      https://www.cbsnews.com/news/donald-trump-national-championship-2018/,\n",
      "      https://usat.ly/2EzTwAM,\n",
      "      http://www.cbc.ca/news/canada/nova-scotia/amazon-bid-new-location-canada-halifax-1.4361723,\n",
      "      https://www.chicagoreader.com/chicago/lake-michigan-bacteria/Content?oid=37062058\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "bp(rankings, logger, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Rankings done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding rankings to the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addRanking(modelName, rankings, config, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.tic(\"Rankings stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalDuration = tt.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notif(modelName + '-' + objectToHash(config)[:5] + \" done in \" + secondsToHumanReadableDuration(totalDuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
