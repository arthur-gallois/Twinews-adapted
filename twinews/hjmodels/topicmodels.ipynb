{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd twinews-logs ; jupython -o nohup-topicmodels-$HOSTNAME.out --venv st-venv ~/notebooks/twinews/hjmodels/topicmodels.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# https://towardsdatascience.com/lets-build-an-article-recommender-using-lda-f22d71b7143e\n",
    "# https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os ; os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "isNotebook = '__file__' not in locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = isNotebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systemtools.hayj import *\n",
    "from systemtools.location import *\n",
    "from systemtools.basics import *\n",
    "from systemtools.file import *\n",
    "from systemtools.printer import *\n",
    "from databasetools.mongo import *\n",
    "from newstools.goodarticle.utils import *\n",
    "from nlptools.preprocessing import *\n",
    "from nlptools.news import parser as newsParser\n",
    "from machinelearning.iterator import *\n",
    "from twinews.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlptools.topicmodeling import *\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "import gensim\n",
    "from math import log2\n",
    "from math import sqrt\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tictoc starts...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = Logger(tmpDir('logs') + \"/lda.log\") if isNotebook else Logger(\"lda-\" + getHostname() + \".log\")\n",
    "tt = TicToc(logger=logger)\n",
    "tt.tic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \\\n",
    "{\n",
    "    'splitVersion': 2,\n",
    "    \n",
    "    'maxUsers': 2 if TEST else None, # Sub-sampling\n",
    "    'maxDocuments': 30 if TEST else 10000,\n",
    "    'useExtraNews': False if TEST else False, # None = unlimited, 0 = no extra news\n",
    "    'minDF': 1 / 500 if TEST else 1 / 2000, # Remove words that have a document frequency ratio lower than 1 / 500\n",
    "    'maxDF': 300, # Remove top 300 voc elements\n",
    "    \n",
    "    'nbTopics': 30 if TEST else 30, # 30, 100\n",
    "    'lowercase': False if TEST else True,\n",
    "    'doLemmatization': False if TEST else True,\n",
    "    # <https://www.quora.com/How-do-you-combine-LDA-and-tf-idf>\n",
    "    # <https://www.quora.com/Why-is-the-performance-improved-by-using-TFIDF-instead-of-bag-of-words-in-LDA-clustering>\n",
    "    'useTFIDF': True,\n",
    "    \n",
    "    'maxIter': 2 if TEST else 30, # 30 for lda, 200 for nmf\n",
    "    \n",
    "    'nmfInit': 'nndsvd', # None, 'nndsvd'\n",
    "    'nmfL1Ratio': 0, # 0.0, 0.5, 1.0\n",
    "    'nmfAlpha': 0.1, # 0.0, 0.1\n",
    "    \n",
    "    'ldaLearningMethod': 'online',\n",
    "    'ldaLearningOffset': 1.0, # 1.0, 10.0\n",
    "    'ldaLearningDecay': 0.7, # 0.5, 0.7, 0.9, 1.0\n",
    "    \n",
    "    'implementation': 'gensim-lda', # gensim-lda, sklearn-lda, sklearn-nmf\n",
    "    'distance': 'cosine', # 'cosine', 'euclidean', 'kl', 'js'\n",
    "    # The historyRef param is very important, it allow to choose, for a particular candidate,\n",
    "    # how many train history items will be used to calculate the similarity with\n",
    "    # the user history.\n",
    "    # Float are ratio on train history\n",
    "    # Integers are absolute number of train item in the history\n",
    "    # For example:\n",
    "    #  * 1.0 will allow to mean similarities of a candidate with all train history items\n",
    "    #  * 1 will allow to use only the most similar train item for the similarity of\n",
    "    #    the candidate with the history of the user\n",
    "    #  * 0.5 will allow to use the half of history for each candidates\n",
    "    #  * 3 to use 3 most similar items with the current candidate...\n",
    "    'historyRef': 0.3, # 1, 1.0, 0.5, 0.3, 3, 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'nmf' in config['implementation']:\n",
    "    modelName = \"nmf\"\n",
    "    newConfig = dict()\n",
    "    for key, value in config.items():\n",
    "        if not key.startswith('lda'):\n",
    "            newConfig[key] = value\n",
    "    config = newConfig\n",
    "    del config['useTFIDF']\n",
    "elif 'lda' in config['implementation']:\n",
    "    modelName = \"lda\"\n",
    "    newConfig = dict()\n",
    "    for key, value in config.items():\n",
    "        if not key.startswith('nmf'):\n",
    "            newConfig[key] = value\n",
    "    config = newConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isNotebook:\n",
    "    assert not rankingExists(modelName, config, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 3.78s | message: Eval data loaded\n",
      "twinews news (version 1.0) initialised.\n",
      "--> tic: 6.35s | message: Extra news downloaded\n",
      "--> toc total duration: 10.13s | message: Got Twinews evaluation data\n",
      "{ candidates, extraNews, meta, testNews, testUsers, trainNews, trainUsers }\n",
      "{ 'created': 2020.03.24-14.28.06, 'endDate': 2018-01-15, 'id': 2, 'ranksLength': 1000, 'splitDate': 2017-12-25, 'startDate': 2017-10-01, 'testMaxNewsPerUser': 97, 'testMeanNewsPerUser': 7.22, 'testMinNewsPerUser': 2, 'testNewsCount': 71781, 'totalNewsAvailable': 570210, 'trainMaxNewsPerUser': 379, 'trainMeanNewsPerUser': 26.48, 'trainMinNewsPerUser': 8, 'trainNewsCount': 237150, 'usersCount': 15905 }\n"
     ]
    }
   ],
   "source": [
    "# Getting users and news\n",
    "evalData = getEvalData(config['splitVersion'], maxExtraNews=config['maxDocuments'],\n",
    "                       maxUsers=config['maxUsers'], logger=logger)\n",
    "(trainUsers, testUsers, trainNews, testNews, candidates, extraNews) = \\\n",
    "(evalData['trainUsers'], evalData['testUsers'], evalData['trainNews'],\n",
    " evalData['testNews'], evalData['candidates'], evalData['extraNews'])\n",
    "bp(evalData.keys(), 5, logger)\n",
    "log(b(evalData['meta'], 5), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraNewsList = shuffle(list(extraNews), seed=0)\n",
    "trainNewsList = shuffle(list(trainNews), seed=0)\n",
    "testNewsList = shuffle(list(testNews), seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 urls for urlsForModel:\n",
      "[\n",
      "  https://www.cfr.org/backgrounder/economics-hosting-olympic-games,\n",
      "  http://www.nytimes.com/2017/10/21/us/trump-epa-chemicals-regulations.html,\n",
      "  ...,\n",
      "  http://www.washingtonexaminer.com/article/2644573,\n",
      "  https://buff.ly/2E1b34w\n",
      "]\n",
      "2017 urls for urlsToVectorize:\n",
      "[\n",
      "  https://www.cfr.org/backgrounder/economics-hosting-olympic-games,\n",
      "  http://www.nytimes.com/2017/10/21/us/trump-epa-chemicals-regulations.html,\n",
      "  ...,\n",
      "  https://www.thewrap.com/michael-wolff-west-wing-how-fire-and-fury-was-reported/#.WlFF_XIcoYQ.twitter,\n",
      "  http://bit.ly/2F1Xtz7\n",
      "]\n",
      "2017 urls for urlsToInfere:\n",
      "[\n",
      "  https://www.cfr.org/backgrounder/economics-hosting-olympic-games,\n",
      "  http://www.nytimes.com/2017/10/21/us/trump-epa-chemicals-regulations.html,\n",
      "  ...,\n",
      "  https://www.thewrap.com/michael-wolff-west-wing-how-fire-and-fury-was-reported/#.WlFF_XIcoYQ.twitter,\n",
      "  http://bit.ly/2F1Xtz7\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# We get urls for the LDA model:\n",
    "if config['useExtraNews']:\n",
    "    urlsForModel = extraNewsList + trainNewsList + testNewsList\n",
    "else:\n",
    "    urlsForModel = trainNewsList + testNewsList + extraNewsList\n",
    "urlsForModel = urlsForModel[:config['maxDocuments']]\n",
    "# We get urls to vectorize for the training and the inference:\n",
    "urlsForModelSet = set(urlsForModel)\n",
    "urlsToVectorize = copy.deepcopy(urlsForModel)\n",
    "for url in trainNewsList + testNewsList:\n",
    "    if url not in urlsForModelSet:\n",
    "        urlsToVectorize.append(url)\n",
    "# We get url to infere for the scoring:\n",
    "urlsToInfere = trainNewsList + testNewsList\n",
    "# Print all:\n",
    "log(str(len(urlsForModel)) + \" urls for urlsForModel:\\n\" + b(urlsForModel), logger=logger)\n",
    "log(str(len(urlsToVectorize)) + \" urls for urlsToVectorize:\\n\" + b(urlsToVectorize), logger=logger)\n",
    "log(str(len(urlsToInfere)) + \" urls for urlsToInfere:\\n\" + b(urlsToInfere), logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twinews news (version 1.0) initialised.\n",
      "  0% [                    ]\n",
      "  9% [=                   ] (5.872s left)\n",
      " 19% [===                 ] (4.941s left)\n",
      " 29% [=====               ] (4.22s left)\n",
      " 39% [=======             ] (3.666s left)\n",
      " 49% [=========           ] (2.98s left)\n",
      " 59% [===========         ] (2.346s left)\n",
      " 69% [=============       ] (1.738s left)\n",
      " 79% [===============     ] (1.157s left)\n",
      " 89% [=================   ] (0.585s left)\n",
      " 99% [=================== ] (0.019s left)\n",
      "100% [====================] (total duration: 5.66s, mean duration: 0.002s)\n",
      "[\n",
      "  [ [ The, Olympics, ..., in, __int_4__ ], [ In, the, ..., shouldered, . ], ..., [ For, author, ..., infrastructure, . ], [ Barring, that, ..., festivities, . ] ],\n",
      "  [ [ It, is, ..., the, E.P.A. ], [ regulates, some, ..., products, . ], ..., [ Last, year, ..., group, . ], [ His, fellow, ..., Beck, . ] ],\n",
      "  ...,\n",
      "  [ [ I, still, ..., notes, . ], [ Not, one, ..., administration, . ], ..., [ The, crown, ..., Six, . ], [ In, one, ..., out, : ] ],\n",
      "  [ [ I, did, ..., wind, . ], [ We, have, ..., friends, ... ], ..., [ Then, there, ..., own, ... ], [ There, were, ..., plants, ... ] ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# We get sentences:\n",
    "sentences = getNewsSentences(urlsToVectorize, logger=logger)\n",
    "bp(sentences, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ [ The, Olympics, ..., festivities, . ], [ It, is, ..., Beck, . ], ..., [ I, still, ..., out, : ], [ I, did, ..., plants, ... ] ]\n"
     ]
    }
   ],
   "source": [
    "# We flatten sentences:\n",
    "for i in range(len(sentences)):\n",
    "    sentences[i] = flattenLists(sentences[i])\n",
    "docs = sentences\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ [ The, Olympics, ..., festivities, . ], [ It, is, ..., Beck, . ], ..., [ I, still, ..., out, : ], [ I, did, ..., plants, ... ] ]\n"
     ]
    }
   ],
   "source": [
    "# Lower case:\n",
    "if config['lowercase']:\n",
    "    for i in pb(list(range(len(docs))), logger=logger, message=\"Lower casing\"):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = docs[i][u].lower()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ [ The, Olympics, ..., festivities, . ], [ It, is, ..., Beck, . ], ..., [ I, still, ..., out, : ], [ I, did, ..., plants, ... ] ]\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization:\n",
    "if config['doLemmatization']:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pbar = ProgressBar(len(docs), logger=logger, message=\"Lemmatization\")\n",
    "    for i in range(len(docs)):\n",
    "        for u in range(len(docs[i])):\n",
    "            docs[i][u] = lemmatizer.lemmatize(docs[i][u])\n",
    "        pbar.tic()\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Voc removed because of minDF (44267 elements):\n",
      "{ 1000th, 100s, 100th, 100x, 103rd, 104th, 108th, 10Weather, 10X, 10x, ..., zooms, zoonotic, zoos, 💗, 😁, 😄, 😇, 😒, 😢, 🙌 }\n",
      "Voc removed because of maxDF (300 elements):\n",
      "{ \", ', (, ), ,, -, ., :, ;, ?, ..., with, without, work, working, world, would, year, years, you, your }\n",
      "73.9% of voc will be removed.\n",
      "[ [ Olympics, evolved, ..., Olympic, festivities ], [ cause, reaching, ..., Dr., Beck ], ..., [ trying, wrap, ..., Wolff, points ], [ Christmas, Year, ..., plants, ... ] ]\n"
     ]
    }
   ],
   "source": [
    "# Filtering the corpus:\n",
    "docs = filterCorpus(docs, minDF=config['minDF'], maxDF=config['maxDF'],\n",
    "                    removeEmptyDocs=False, allowEmptyDocs=False, logger=logger)\n",
    "for doc in docs: assert len(doc) > 0\n",
    "bp(docs, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 16.73s | message: Data preprocessed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16.73"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tic(\"Data preprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infering topic vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['implementation'] == 'gensim-lda':\n",
    "    dictionary = gensim.corpora.Dictionary(docs)\n",
    "    # dictionary.filter_extremes(no_below=config['min_df'])\n",
    "    bow = [dictionary.doc2bow(doc) for doc in docs]\n",
    "    if config['useTFIDF']:\n",
    "        tfidf = gensim.models.TfidfModel(bow)\n",
    "        bow = tfidf[bow]\n",
    "    assert len(bow) == len(urlsToVectorize)\n",
    "    bowForModel = bow[:len(urlsForModel)]\n",
    "    assert len(bowForModel) == config['maxDocuments']\n",
    "    i = 0\n",
    "    for url in urlsToVectorize:\n",
    "        if url == trainNewsList[0]:\n",
    "            break\n",
    "        i += 1\n",
    "    assert i == len(extraNews) or i == 0\n",
    "    bowForInference = bow[i:i + len(trainNews) + len(testNews)]\n",
    "    assert len(bowForInference) == len(trainNews) + len(testNews)\n",
    "    lda_model = gensim.models.LdaMulticore\\\n",
    "    (\n",
    "        bowForModel,\n",
    "        num_topics=config['nbTopics'],\n",
    "        id2word=dictionary,\n",
    "        iterations=config['maxIter'],\n",
    "        decay=config['ldaLearningDecay'],\n",
    "        offset=config['ldaLearningOffset'],\n",
    "        workers=cpuCount(),\n",
    "        passes=3,\n",
    "    )\n",
    "    inferedVectors = []\n",
    "    for current in bowForInference:\n",
    "        topicProbDistrib = lda_model[current]\n",
    "        currentVector = [0.0] * config['nbTopics']\n",
    "        for t, v in topicProbDistrib:\n",
    "            currentVector[t] = v\n",
    "        inferedVectors.append(np.array(currentVector))\n",
    "    assert len(inferedVectors) == len(trainNews) + len(testNews)\n",
    "    assert len(inferedVectors[0]) == config['nbTopics']\n",
    "    topics = []\n",
    "    for i in range(lda_model.num_topics):\n",
    "        current = dict()\n",
    "        for x in lda_model.get_topic_terms(i, topn=100):\n",
    "            current[dictionary[x[0]]] = x[1]\n",
    "        topics.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['implementation'] == 'sklearn-lda' or config['implementation'] == 'sklearn-nmf':\n",
    "    if config['implementation'] == 'sklearn-nmf' or config['useTFIDF']:\n",
    "        vectorizer = TfidfVectorizer\\\n",
    "        (\n",
    "            sublinear_tf=True,\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            # lowercase=True, # Doesn't work because we erased preprocessor\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer\\\n",
    "        (\n",
    "            tokenizer=lambda x: x,\n",
    "            preprocessor=lambda x: x,\n",
    "            # lowercase=True, # Doesn't work because we erased preprocessor\n",
    "        )\n",
    "    vectors = vectorizer.fit_transform(docs)\n",
    "    assert vectors.shape[0] == len(urlsToVectorize)\n",
    "    vectorsForModel = vectors[:len(urlsForModel)]\n",
    "    assert vectorsForModel.shape[0] == config['maxDocuments']\n",
    "    i = 0\n",
    "    for url in urlsToVectorize:\n",
    "        if url == trainNewsList[0]:\n",
    "            break\n",
    "        i += 1\n",
    "    assert i == len(extraNews) or i == 0\n",
    "    vectorsForInference = vectors[i:i + len(trainNews) + len(testNews)]\n",
    "    assert vectorsForInference.shape[0] == len(trainNews) + len(testNews)\n",
    "    if config['implementation'] == 'sklearn-lda':\n",
    "        model = LatentDirichletAllocation\\\n",
    "        (\n",
    "            n_components=config['nbTopics'],\n",
    "            learning_method=config['ldaLearningMethod'],\n",
    "            learning_offset=config['ldaLearningOffset'],\n",
    "            learning_decay=config['ldaLearningDecay'],\n",
    "            random_state=0,\n",
    "            n_jobs=cpuCount(),\n",
    "            max_iter=config['maxIter'],\n",
    "        )\n",
    "    else:\n",
    "        model = NMF\\\n",
    "        (\n",
    "            n_components=config['nbTopics'],\n",
    "            random_state=0,\n",
    "            alpha=config['nmfAlpha'],\n",
    "            l1_ratio=config['nmfL1Ratio'],\n",
    "            init=config['nmfInit'],\n",
    "            max_iter=config['maxIter'],\n",
    "        )\n",
    "    model.fit(vectorsForModel)\n",
    "    inferedVectors = model.transform(vectorsForInference)\n",
    "    assert inferedVectors.shape[0] == len(trainNews) + len(testNews)\n",
    "    assert inferedVectors[0].shape[0] == config['nbTopics']\n",
    "    topics = []\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        wordProb = []\n",
    "        for i in range(len(topic)):\n",
    "            prob = topic[i]\n",
    "            word = feature_names[i]\n",
    "            wordProb.append((word, prob))\n",
    "        wordProb = sortBy(wordProb, desc=True, index=1)[:100]\n",
    "        current = dict()\n",
    "        for word, prob in wordProb:\n",
    "            current[word] = prob\n",
    "        topics.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 9.95s | message: Model fitted and topic vectors infered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.95"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tic(\"Model fitted and topic vectors infered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTopics(topics, maxWords=10, logger=None):\n",
    "    for i in range(len(topics)):\n",
    "        log(str(i) + \": \" + str(\" \".join(list(topics[i].keys())[:10])), logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTopicsOf(vector, topics, logger=None):\n",
    "    topicsRepr = \"\"\n",
    "    topTopics = sortBy([(i, score) for i, score in enumerate(vector) if score > 0.1], desc=True, index=1)[:3]\n",
    "    log(\"Top topics number are: \" + str(\" \".join([str(e[0]) for e in topTopics])), logger)\n",
    "    currentTopics = [topics[e[0]] for e in topTopics]\n",
    "    printTopics(currentTopics, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0: Cup Barcelona Denver Messi games match Olympics Ham Madrid Colorado\n",
      "1: Amazon bids finalists taxes bidding Beck auction Cup players game\n",
      "2: Denver Barcelona Madrid Cup Messi Olympics World Olympic tax Brazil\n",
      "3: Barcelona Messi Texas tax salary club Corker Colorado contract La\n",
      "4: Texas Cup World No referee team Denver FIFA players games\n",
      "5: jersey flag Cup colors Spain Denver housing red Brazil 1930s\n",
      "6: Ham match Colorado Cup West Denver housing games team referees\n",
      "7: Messi Denver Texas housing intelligence Dallas Barcelona team FC Beck\n",
      "8: Ham West Denver Cup Premier club Stadium team play intelligence\n",
      "9: Colorado Denver Barcelona Cup Madrid housing World Olympics FIFA games\n",
      "10: Messi Denver Ham city Olympics Colorado Beck Olympic Games players\n",
      "11: Colorado Dallas Cup Denver FC Hickenlooper Texas players city coal\n",
      "12: Denver Colorado Olympic Games Olympics Barcelona Brazil Winter Cup players\n",
      "13: jersey flag Denver Olympic Spain Cup Brazil colors Dallas Winter\n",
      "14: Barcelona Colorado Denver Olympics Amazon apology Madrid Messi play cry\n",
      "15: Fame Colorado FC Barcelona Hunt Frisco Stadium Hall Soccer Denver\n",
      "16: Ham West Cup Colorado Amazon Denver play club League Barcelona\n",
      "17: Denver Cup Ham Colorado club Olympics World match Barcelona Spain\n",
      "18: Denver Dallas Olympics Colorado federation games referee Olympic FC Ham\n",
      "19: Dallas FC tax Colorado Cup Denver players Corker intelligence loophole\n",
      "20: Cup ball Barcelona game players Spain housing World Denver football\n",
      "21: Dallas FC Hunt Frisco Fame Soccer Hall Toyota Stadium Photographer\n",
      "22: Denver city Goldberg drainage project Colorado widening Olympics housing ad\n",
      "23: Cup Colorado World Barcelona Denver No Texas team FIFA Italy\n",
      "24: tax Ham players Corker Denver loophole Cup Mr. games supporters\n",
      "25: Amazon Denver taxes bids city Cup Colorado Winter Olympics ball\n",
      "26: Denver Colorado Olympics Barcelona Messi Winter Hickenlooper Goldberg games drainage\n",
      "27: Denver Colorado Barcelona city housing Hickenlooper Beck Winter apology Hancock\n",
      "28: Olympics Denver Olympic games Barcelona Ham club housing Colorado Games\n",
      "29: Ham Fame Soccer games Olympics soccer Frisco referee club Cup\n"
     ]
    }
   ],
   "source": [
    "printTopics(topics, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isNotebook:\n",
    "    try:\n",
    "        dirPath = nosaveDir() + \"/sklearn-models\"\n",
    "        mkdir(dirPath)\n",
    "        configHash = objectToHash(config)[:5]\n",
    "        serialize(model, dirPath + \"/model-\" + configHash + \".pickle\")\n",
    "        toJsonFile(config, dirPath + \"/config-\" + configHash + \".json\")\n",
    "    except Exception as e:\n",
    "        logException(e, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a dict url --> topic vector and a dict url --> text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert len(urlsToInfere) == len(inferedVectors)\n",
    "urlsVectors = dict()\n",
    "for i in range(len(urlsToInfere)):\n",
    "    urlsVectors[urlsToInfere[i]] = inferedVectors[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% [                    ]\n",
      "  9% [=                   ] (5.782s left)\n",
      " 19% [===                 ] (4.74s left)\n",
      " 29% [=====               ] (4.103s left)\n",
      " 39% [=======             ] (3.439s left)\n",
      " 49% [=========           ] (2.839s left)\n",
      " 59% [===========         ] (2.246s left)\n",
      " 69% [=============       ] (1.677s left)\n",
      " 79% [===============     ] (1.119s left)\n",
      " 89% [=================   ] (0.566s left)\n",
      " 99% [=================== ] (0.019s left)\n",
      "100% [====================] (total duration: 5.49s, mean duration: 0.002s)\n"
     ]
    }
   ],
   "source": [
    "if isNotebook:\n",
    "    urlsTexts = dict()\n",
    "    allTexts = getNewsText(urlsToInfere, logger=logger)\n",
    "    for i in range(len(urlsToInfere)):\n",
    "        urlsTexts[urlsToInfere[i]] = allTexts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3a43d6f2fda1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misNotebook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0muserId\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainUsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mxvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mxurls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainUsers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muserId\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if isNotebook:\n",
    "    userId = list(trainUsers.keys())[11]\n",
    "    xvectors = []\n",
    "    xurls = []\n",
    "    for url in trainUsers[userId]:\n",
    "        xvectors.append(urlsVectors[url])\n",
    "        xurls.append(url)\n",
    "    xvectors = np.array(xvectors)\n",
    "    yvectors = []\n",
    "    yurls = []\n",
    "    for url in candidates[userId][0]:\n",
    "        yvectors.append(urlsVectors[url])\n",
    "        yurls.append(url)\n",
    "    yvectors = np.array(yvectors)\n",
    "    distances = getDistances(xvectors, yvectors, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing some docs with topics:\n",
    "if isNotebook:\n",
    "    for i in range(10):\n",
    "        urls = random.choice([xurls, yurls])\n",
    "        url = random.choice(urls)\n",
    "        text = urlsTexts[url]\n",
    "        vector = urlsVectors[url]\n",
    "        log(url, logger)\n",
    "        printTopicsOf(vector, topics, logger=logger)\n",
    "        log(text, logger)\n",
    "        log(\"\\n\" * 2, logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing some similar docs:\n",
    "if isNotebook:\n",
    "    for i in range(distances.shape[0]):\n",
    "        if i > 100:\n",
    "            break\n",
    "        # Get train:\n",
    "        trainUrl = xurls[i]\n",
    "        trainText = urlsTexts[trainUrl]\n",
    "        trainVector = urlsVectors[trainUrl]\n",
    "        log(trainUrl, logger)\n",
    "        printTopicsOf(trainVector, topics, logger=logger)\n",
    "        log(trainText[:2000], logger)\n",
    "        log(\"\\n\", logger)\n",
    "        # Get distances:\n",
    "        currentDistances = []\n",
    "        for u in range(len(yurls)):\n",
    "            currentDistances.append((yurls[u], urlsTexts[yurls[u]], distances[i][u]))\n",
    "        topSim = sortBy(currentDistances, index=2, desc=False)[:3]\n",
    "        topDissim = sortBy(currentDistances, index=2, desc=True)[:3]\n",
    "        # Print similars:\n",
    "        log(\"MOST SIMILARS\", logger)\n",
    "        log(\"\\n\", logger)\n",
    "        for url, text, dist in topSim:\n",
    "            log(dist, logger)\n",
    "            log(url, logger)\n",
    "            printTopicsOf(urlsVectors[url], topics, logger=logger)\n",
    "            log(text[:2000], logger)\n",
    "            log(\"\\n\", logger)\n",
    "        # Print dissimilars:\n",
    "        log(\"MOST DISSIMILARS\", logger)\n",
    "        log(\"\\n\", logger)\n",
    "        for url, text, dist in topDissim:\n",
    "            log(dist, logger)\n",
    "            log(url, logger)\n",
    "            printTopicsOf(urlsVectors[url], topics, logger=logger)\n",
    "            log(text[:2000], logger)\n",
    "            log(\"\\n\", logger)\n",
    "        log(\"\\n\", logger)\n",
    "        log(\"\\n\" * 2 + '-' * 20 + \"\\n\" * 2, logger)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False and isNotebook:\n",
    "    config['historyRef'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "historyRef = config['historyRef']\n",
    "assert (isinstance(historyRef, int) and historyRef >= 1) or (isinstance(historyRef, float) and historyRef > 0.0 and historyRef <= 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating ranking  50% [==========          ] (0.06s left)\n",
      "Generating ranking 100% [====================] (total duration: 0.09s, mean duration: 0.045s)\n"
     ]
    }
   ],
   "source": [
    "ranks = dict()\n",
    "for userId in pb(list(trainUsers.keys()), logger=logger, message=\"Generating ranking\", printRatio=0.01):\n",
    "    if isinstance(historyRef, float):\n",
    "        currentHistoryRef = int(historyRef * len(trainUsers[userId]))\n",
    "    else:\n",
    "        currentHistoryRef = historyRef\n",
    "    xvectors = []\n",
    "    xurls = []\n",
    "    for url in trainUsers[userId]:\n",
    "        xvectors.append(urlsVectors[url])\n",
    "        xurls.append(url)\n",
    "    xvectors = np.array(xvectors)\n",
    "    ranks[userId] = []\n",
    "    for currentCandidates in candidates[userId]:\n",
    "        yvectors = []\n",
    "        yurls = []\n",
    "        for url in currentCandidates:\n",
    "            yvectors.append(urlsVectors[url])\n",
    "            yurls.append(url)\n",
    "        yvectors = np.array(yvectors)\n",
    "        distances = getDistances(xvectors, yvectors, metric=config['distance'], logger=logger)\n",
    "        urlDistances = dict()\n",
    "        for testIndex in range(len(yurls)):\n",
    "            url = yurls[testIndex]\n",
    "            currentDists = distances[:, testIndex]\n",
    "            assert currentDists.shape[0] == len(xurls)\n",
    "            currentDists = sorted(list(currentDists), reverse=False)\n",
    "            currentDists = currentDists[:currentHistoryRef]\n",
    "            currentDist = np.mean(currentDists)\n",
    "            urlDistances[url] = currentDist\n",
    "        rank = [e[0] for e in sortBy(urlDistances, index=1, desc=False)]\n",
    "        ranks[userId].append(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '2617447752': \n",
      "  [\n",
      "    [\n",
      "      http://www.sun-sentinel.com/business/fl-bz-woodbridge-investors-seek-money-back-2017122920171229-story.html,\n",
      "      https://www.charlestoncitypaper.com/HolyCinema/archives/2018/01/02/southern-charm-newbie-relationshep-edition-the-uninvited-guest,\n",
      "      http://bit.ly/2mpnEbh,\n",
      "      https://voiceofoc.org/2018/01/county-supervisors-criticized-for-plan-to-push-hundreds-of-homeless-people-from-riverbed-to-cities/,\n",
      "      http://www.nbcnews.com/news/nbcblk/five-things-about-dana-gresham-senate-democrats-only-black-chief-n834351,\n",
      "      https://www.theglobeandmail.com/report-on-business/rob-commentary/canada-needs-to-retool-its-asian-trade-strategy/article37457620/,\n",
      "      https://www.timesofmalta.com/articles/view/20180114/arts-entertainment/high-drama-doomed-love.667882#.Wlt3U9gQKYI.twitter,\n",
      "      https://reconciliationsyllabus.wordpress.com/2018/01/07/indigenous-ways-of-being-and-knowing-a-try-an-exercise-in-family-law-and-sex-o-at-uviclaw/,\n",
      "      https://buff.ly/2FjFWTa,\n",
      "      http://www.foxnews.com/politics/2018/01/04/house-russia-probe-ending-as-it-began-mess.amp.html?__twitter_impression=true,\n",
      "      ...,\n",
      "      http://ow.ly/sDmJ30hz4uS,\n",
      "      http://nyti.ms/2pPhipJ,\n",
      "      http://www.courant.com/news/connecticut/hc-news-towns-prepay-2018-taxes-20171227-story.html,\n",
      "      https://www.washingtonpost.com/local/can-you-prepay-your-real-estate-taxes-before-the-tax-bill-takes-effect-find-out-here/2017/12/26/5539b14c-ea7c-11e7-8a6a-80acf0774e64_story.html?utm_term=.d3b4b6c86a6e,\n",
      "      https://nypost.com/2018/01/13/she-was-a-post-columnist-and-a-heroic-wwii-spy/?utm_campaign=iosapp&utm_source=twitter_app,\n",
      "      https://www.dallasnews.com/news/dallas-city-hall/2017/12/30/mobike-no-problem-fifth-bike-share-company-pulls-dallas-way,\n",
      "      http://bit.ly/2AkNlxU,\n",
      "      https://sportsday.dallasnews.com/soccer/soccer/2018/01/01/fc-dallas-buy-new-years-gift,\n",
      "      http://www.crainsdetroit.com/article/20180107/news/649416/document-in-detroits-amazon-hq2-bid-reveals-details-of-talent-marshall,\n",
      "      https://www.seattletimes.com/business/amazon/seattle-artist-gives-amazons-fleet-of-treasure-trucks-their-distinctive-local-looks/?utm_source=twitter&utm_medium=social&utm_campaign=article_left_1.1\n",
      "    ]\n",
      "  ],\n",
      "  '790664132': \n",
      "  [\n",
      "    [\n",
      "      https://motherboard.vice.com/en_us/article/8xvvya/study-finds-women-and-minorities-in-stem-are-discriminated-against-pew-research-center-tech-science,\n",
      "      https://www.reuters.com/article/us-leeco-debt/leeco-founder-defies-china-return-order-stays-in-u-s-for-car-fundraising-idUSKBN1ER0DS,\n",
      "      https://www.greeleytribune.com/news/local/rising-groundwater-in-the-past-decade-inundates-more-basements-each-year-costing-homeowners-from-gilcrest-to-sterling-thousands-of-dollars-each/,\n",
      "      http://bit.ly/2u4QGTv,\n",
      "      https://medium.com/p/who-were-the-activists-on-the-red-carpet-at-the-golden-globes-5682df68a0c,\n",
      "      http://www.latimes.com/entertainment/arts/miranda/la-et-cam-rodney-mcmillian-austin-20180110-story.html,\n",
      "      https://www.insurancejournal.com/news/southeast/2017/12/29/475635.htm,\n",
      "      https://www.bloomberg.com/news/articles/2018-01-11/la-nina-to-last-through-winter-just-don-t-tell-chilly-houston,\n",
      "      http://bit.ly/2F4U4j4,\n",
      "      http://nbcnews.to/2CmgTjP,\n",
      "      ...,\n",
      "      https://www.bloomberg.com/news/articles/2018-01-03/amazon-gets-the-headlines-but-lidl-is-still-the-grocer-to-watch?mc_cid=fad8becce2&mc_eid=e70e651839,\n",
      "      https://www.chicagoreader.com/chicago/lake-michigan-bacteria/Content?oid=37062058,\n",
      "      http://www.democratandchronicle.com/story/news/politics/albany/2017/12/26/prepaying-your-property-taxes/981506001/,\n",
      "      http://www.newsobserver.com/news/politics-government/politics-columns-blogs/under-the-dome/article191973099.html,\n",
      "      http://bit.ly/2D9jaga,\n",
      "      https://medium.com/whatever-source-derived/assessing-the-irss-guidance-on-prepaid-property-taxes-c81c24dcd075,\n",
      "      https://usat.ly/2EzTwAM,\n",
      "      http://www.cbc.ca/news/canada/nova-scotia/amazon-bid-new-location-canada-halifax-1.4361723,\n",
      "      http://www.thewhig.com/2018/01/04/opp-charge-smith-falls-officer/,\n",
      "      https://www.cbssports.com/soccer/news/wynalda-says-u-s-a-soccer-doesnt-have-its-act-together-and-he-wants-to-change-it/\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "bp(ranks, logger, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 27m 10.779s | message: Ranks done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1630.78"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tic(\"Ranks done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding ranks to the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "addRanking(modelName, ranks, config, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> tic: 1.33s | message: Ranks stored\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.33"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt.tic(\"Ranks stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--> toc total duration: 27m 39.029s\n"
     ]
    }
   ],
   "source": [
    "totalDuration = tt.toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "notif(modelName + '-' + objectToHash(config)[:5] + \" done in \" + secondsToHumanReadableDuration(totalDuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
